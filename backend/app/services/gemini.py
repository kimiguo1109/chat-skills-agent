"""
Google Gemini API æœåŠ¡å°è£…

æä¾›ç»Ÿä¸€çš„ LLM API è°ƒç”¨æ¥å£ï¼Œæ”¯æŒï¼š
- æ–‡æœ¬ç”Ÿæˆ
- JSON æ ¼å¼åŒ–è¾“å‡º
- é”™è¯¯å¤„ç†å’Œé‡è¯•
- Token é™åˆ¶
"""
import logging
import json
import time
from typing import Optional, Dict, Any, List
from google import genai
from google.genai import types

from ..config import settings

logger = logging.getLogger(__name__)


class GeminiClient:
    """Gemini API å®¢æˆ·ç«¯å°è£…ï¼ˆä½¿ç”¨æœ€æ–° SDKï¼‰"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯
        
        Args:
            api_key: Gemini API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä» settings è¯»å–
        """
        self.api_key = api_key or settings.GEMINI_API_KEY
        
        # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æœ€æ–° SDKï¼‰
        self.client = genai.Client(api_key=self.api_key)
        self.async_client = self.client.aio
        
        logger.info("âœ… Gemini client initialized with new SDK")
    
    async def generate_stream(
        self,
        prompt: str,
        model: str = "gemini-2.5-flash",
        max_tokens: int = 2000,
        temperature: float = 0.7,
        thinking_budget: Optional[int] = 1024,
        return_thinking: bool = True
    ):
        """
        æµå¼ç”Ÿæˆå†…å®¹ï¼ˆç”¨äºå®æ—¶å±•ç¤ºæ€è€ƒè¿‡ç¨‹ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            thinking_budget: æ€è€ƒé¢„ç®—
            return_thinking: æ˜¯å¦è¿”å›æ€è€ƒè¿‡ç¨‹
            
        Yields:
            Dict: åŒ…å« type (thinking/content) å’Œ text çš„å­—å…¸
        """
        config_kwargs = {
            "temperature": temperature,
            "max_output_tokens": max_tokens,
            "response_modalities": ["TEXT"],
        }
        
        # æ·»åŠ æ€è€ƒé…ç½®
        if thinking_budget is not None and thinking_budget > 0:
            config_kwargs["thinkingConfig"] = types.ThinkingConfig(
                thinkingBudget=thinking_budget,
                includeThoughts=return_thinking
            )
        
        config = types.GenerateContentConfig(**config_kwargs)
        
        try:
            logger.info(f"ğŸŒŠ Starting streaming generation: model={model}")
            
            # ä½¿ç”¨æµå¼ API
            stream = await self.async_client.models.generate_content_stream(
                model=model,
                contents=prompt,
                config=config
            )
            
            thinking_accumulated = []
            content_accumulated = []
            
            async for chunk in stream:
                if hasattr(chunk, 'candidates') and chunk.candidates:
                    candidate = chunk.candidates[0]
                    
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ€è€ƒéƒ¨åˆ†
                            if hasattr(part, 'thought'):
                                thought = part.thought
                                if isinstance(thought, str) and thought:
                                    thinking_accumulated.append(thought)
                                    yield {
                                        "type": "thinking",
                                        "text": thought,
                                        "accumulated": "".join(thinking_accumulated)
                                    }
                                elif thought is True and hasattr(part, 'text'):
                                    text = part.text
                                    if text:
                                        thinking_accumulated.append(text)
                                        yield {
                                            "type": "thinking",
                                            "text": text,
                                            "accumulated": "".join(thinking_accumulated)
                                        }
                            # å†…å®¹éƒ¨åˆ†
                            elif hasattr(part, 'text') and part.text:
                                text = part.text
                                content_accumulated.append(text)
                                yield {
                                    "type": "content",
                                    "text": text,
                                    "accumulated": "".join(content_accumulated)
                                }
            
            # å®Œæˆæ ‡è®°
            yield {
                "type": "done",
                "thinking": "".join(thinking_accumulated),
                "content": "".join(content_accumulated)
            }
            
            logger.info(f"âœ… Streaming generation complete")
            
        except Exception as e:
            logger.error(f"âŒ Streaming generation error: {e}")
            yield {
                "type": "error",
                "error": str(e)
            }
    
    async def generate(
        self,
        prompt: str,
        model: str = "gemini-2.5-flash",  # ğŸ†• ä½¿ç”¨ 2.5 Flash æ”¯æŒæ€è€ƒæ¨¡å‹
        response_format: str = "text",
        max_tokens: int = 2000,
        temperature: float = 0.7,
        max_retries: int = 3,
        thinking_budget: Optional[int] = 1024,  # ğŸ†• æ€è€ƒé¢„ç®—ï¼Œé»˜è®¤ 1024 tokens
        return_thinking: bool = True  # ğŸ†• æ˜¯å¦è¿”å›æ€è€ƒè¿‡ç¨‹
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬å†…å®¹ï¼ˆå¼‚æ­¥ï¼‰- ğŸ†• æ”¯æŒæ€è€ƒæ¨¡å‹
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ gemini-2.5-flash
            response_format: å“åº”æ ¼å¼ï¼Œ"text" æˆ– "json"
            max_tokens: æœ€å¤§ token æ•°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰ï¼Œè¶Šé«˜è¶Šéšæœº
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            thinking_budget: æ€è€ƒé¢„ç®—ï¼ˆtokensï¼‰ï¼Œ0 = æ— æ€è€ƒï¼Œ1024 = ä¸­ç­‰ï¼Œæœ€å¤§ 24576
            return_thinking: æ˜¯å¦è¿”å›æ€è€ƒè¿‡ç¨‹
        
        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹é”®ï¼š
                - "content": ç”Ÿæˆçš„æ–‡æœ¬æˆ– JSON å­—ç¬¦ä¸²
                - "thinking": æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                - "usage": Token ä½¿ç”¨ç»Ÿè®¡
        
        Raises:
            Exception: API è°ƒç”¨å¤±è´¥
        """
        # å¦‚æœè¯·æ±‚ JSON æ ¼å¼ï¼Œåœ¨ prompt ä¸­æ˜ç¡®è¯´æ˜
        if response_format == "json":
            prompt = self._enhance_json_prompt(prompt)
        
        # é…ç½®ç”Ÿæˆå‚æ•°
        config_kwargs = {
            "temperature": temperature,
            "max_output_tokens": max_tokens,
        }
        
        # ğŸ†• æ·»åŠ æ€è€ƒé…ç½®ï¼ˆGemini 2.5 Flashï¼‰
        if thinking_budget is not None and thinking_budget > 0:
            config_kwargs["thinkingConfig"] = types.ThinkingConfig(
                thinkingBudget=thinking_budget,  # æ³¨æ„ï¼šä½¿ç”¨ camelCase
                includeThoughts=return_thinking  # æ˜¯å¦è¿”å›æ€è€ƒè¿‡ç¨‹
            )
            logger.info(f"ğŸ§  Thinking mode enabled: budget={thinking_budget} tokens, includeThoughts={return_thinking}")
        
        config = types.GenerateContentConfig(**config_kwargs)
        
        # é‡è¯•é€»è¾‘
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ¤– Calling Gemini API: model={model}, tokens<={max_tokens}")
                start_time = time.time()
                
                # ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯è°ƒç”¨ API
                response = await self.async_client.models.generate_content(
                    model=model,
                    contents=prompt,
                    config=config
                )
                
                # æ£€æŸ¥å“åº”
                if not response.text:
                    raise ValueError("Empty response from Gemini API")
                
                result = response.text.strip()
                elapsed = time.time() - start_time
                
                # ğŸ†• æå–æ€è€ƒè¿‡ç¨‹
                thinking_process = None
                if return_thinking:
                    thinking_process = self._extract_thinking(response)
                
                # ============= Token ä½¿ç”¨ç»Ÿè®¡ =============
                usage_metadata = getattr(response, 'usage_metadata', None)
                usage_stats = {}
                
                if usage_metadata:
                    input_tokens = getattr(usage_metadata, 'prompt_token_count', 0)
                    output_tokens = getattr(usage_metadata, 'candidates_token_count', 0)
                    total_tokens = getattr(usage_metadata, 'total_token_count', 0)
                    thoughts_tokens = getattr(usage_metadata, 'thoughts_token_count', 0)  # ğŸ†• æ€è€ƒ tokens
                    
                    usage_stats = {
                        "input_tokens": input_tokens,
                        "output_tokens": output_tokens,
                        "thoughts_tokens": thoughts_tokens,
                        "total_tokens": total_tokens
                    }
                    
                    log_msg = (
                        f"ğŸ“Š Token Usage | Input: {input_tokens:,} | Output: {output_tokens:,}"
                    )
                    if thoughts_tokens > 0:
                        log_msg += f" | Thoughts: {thoughts_tokens:,} ğŸ§ "
                    log_msg += f" | Total: {total_tokens:,} | Time: {elapsed:.2f}s | Model: {model}"
                    
                    logger.info(log_msg)
                else:
                    logger.info(f"âœ… Gemini response received in {elapsed:.2f}s, length={len(result)}")
                
                # å¦‚æœæ˜¯ JSON æ ¼å¼ï¼Œå°è¯•è§£æéªŒè¯
                if response_format == "json":
                    result = self._extract_json(result)
                    try:
                        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
                        json.loads(result)
                        return result
                    except json.JSONDecodeError as json_err:
                        # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤
                        if attempt == max_retries - 1:
                            logger.warning(f"âš ï¸ JSON parsing failed, attempting to fix...")
                            try:
                                fixed_result = self._try_fix_json(result)
                                json.loads(fixed_result)
                                logger.info(f"âœ… JSON auto-fixed successfully")
                                return fixed_result
                            except:
                                logger.error(f"âŒ Failed to fix JSON")
                                raise ValueError(f"Invalid JSON response: {str(json_err)}")
                        else:
                            raise json_err
                
                # ğŸ†• è¿”å›å­—å…¸æ ¼å¼ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
                return {
                    "content": result,
                    "thinking": thinking_process,
                    "usage": usage_stats
                }
                
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSON parsing failed (attempt {attempt + 1}/{max_retries}): {e}")
                logger.debug(f"Raw result (last 200 chars): ...{result[-200:]}")
                if attempt == max_retries - 1:
                    logger.error("âŒ Failed to parse JSON after all retries")
                    raise ValueError(f"Invalid JSON response: {str(e)}")
                time.sleep(2 * (attempt + 1))  # æŒ‡æ•°é€€é¿
                
            except Exception as e:
                logger.error(f"âŒ Gemini API error (attempt {attempt + 1}/{max_retries}): {type(e).__name__}: {e}")
                if attempt == max_retries - 1:
                    raise
                
                # æŒ‡æ•°é€€é¿
                wait_time = 2 ** attempt
                logger.info(f"â³ Retrying in {wait_time}s...")
                time.sleep(wait_time)
        
        raise Exception("Failed to generate content after all retries")
    
    def _enhance_json_prompt(self, prompt: str) -> str:
        """
        å¢å¼º prompt ä»¥è·å¾— JSON æ ¼å¼è¾“å‡º
        
        Args:
            prompt: åŸå§‹ prompt
        
        Returns:
            str: å¢å¼ºåçš„ prompt
        """
        if "JSON" in prompt.upper() or "json" in prompt:
            # å·²ç»åŒ…å« JSON æŒ‡ç¤º
            return prompt
        
        return f"""{prompt}

IMPORTANT: You must respond with valid JSON only. Do not include any text before or after the JSON object.
Example format: {{"key": "value"}}

Your JSON response:"""
    
    def _try_fix_json(self, text: str) -> str:
        """
        å°è¯•ä¿®å¤å¸¸è§çš„ JSON é”™è¯¯
        """
        import re
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—
        text = text.strip()
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()
        
        # å°è¯•ç§»é™¤ JSON ä¸­çš„æ³¨é‡Šï¼ˆ// å’Œ /* */ï¼‰
        # ç§»é™¤å•è¡Œæ³¨é‡Š
        text = re.sub(r'//[^\n]*\n', '\n', text)
        # ç§»é™¤å¤šè¡Œæ³¨é‡Š
        text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
        
        # ç§»é™¤å°¾éšé€—å·ï¼ˆJSON ä¸­æœ€å¸¸è§çš„é”™è¯¯ï¼‰
        # 1. å¯¹è±¡ä¸­çš„å°¾éšé€—å·: , }
        text = re.sub(r',(\s*})', r'\1', text)
        # 2. æ•°ç»„ä¸­çš„å°¾éšé€—å·: , ]
        text = re.sub(r',(\s*\])', r'\1', text)
        
        # ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        # ä½†è¦å°å¿ƒä¸è¦æ”¹å˜å­—ç¬¦ä¸²å†…éƒ¨çš„å•å¼•å·
        # ç®€å•ç­–ç•¥ï¼šåªæ›¿æ¢é”®åçš„å•å¼•å·
        text = re.sub(r"'([^']*)'(\s*):", r'"\1"\2:', text)
        
        # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡æˆ–æ•°ç»„
        # ä»åå¾€å‰æ‰¾æœ€åä¸€ä¸ª } æˆ– ]
        last_brace = text.rfind('}')
        last_bracket = text.rfind(']')
        
        if last_brace > last_bracket:
            # å¯¹è±¡
            text = text[:last_brace + 1]
        elif last_bracket > last_brace:
            # æ•°ç»„
            text = text[:last_bracket + 1]
        
        return text
    
    def _extract_json(self, text: str) -> str:
        """
        ä»æ–‡æœ¬ä¸­æå– JSON å†…å®¹ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¤„ç†å¤šä½™å†…å®¹ï¼‰
        
        Args:
            text: å¯èƒ½åŒ…å« JSON çš„æ–‡æœ¬
        
        Returns:
            str: æå–çš„ JSON å­—ç¬¦ä¸²
        """
        text = text.strip()
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        
        text = text.strip()
        
        # å°è¯•æ‰¾åˆ°å®Œæ•´çš„ JSON å¯¹è±¡æˆ–æ•°ç»„
        # ä½¿ç”¨ç®€å•çš„æ‹¬å·åŒ¹é…æ¥æ‰¾åˆ°å®Œæ•´çš„ JSON
        
        # ä¼˜å…ˆæ£€æŸ¥å¯¹è±¡
        if "{" in text:
            start = text.find("{")
            depth = 0
            in_string = False
            escape_next = False
            
            for i in range(start, len(text)):
                char = text[i]
                
                # å¤„ç†å­—ç¬¦ä¸²ä¸­çš„å¼•å·
                if char == '"' and not escape_next:
                    in_string = not in_string
                elif char == '\\' and not escape_next:
                    escape_next = True
                    continue
                
                if not in_string:
                    if char == '{':
                        depth += 1
                    elif char == '}':
                        depth -= 1
                        if depth == 0:
                            # æ‰¾åˆ°å®Œæ•´çš„ JSON å¯¹è±¡
                            return text[start:i+1]
                
                escape_next = False
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹è±¡ï¼Œæ£€æŸ¥æ•°ç»„
        if "[" in text:
            start = text.find("[")
            depth = 0
            in_string = False
            escape_next = False
            
            for i in range(start, len(text)):
                char = text[i]
                
                if char == '"' and not escape_next:
                    in_string = not in_string
                elif char == '\\' and not escape_next:
                    escape_next = True
                    continue
                
                if not in_string:
                    if char == '[':
                        depth += 1
                    elif char == ']':
                        depth -= 1
                        if depth == 0:
                            # æ‰¾åˆ°å®Œæ•´çš„ JSON æ•°ç»„
                            return text[start:i+1]
                
                escape_next = False
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        return text
    
    async def generate_json(
        self,
        prompt: str,
        model: str = "gemini-2.0-flash-exp",
        max_tokens: int = 2000,
        temperature: float = 0.7,
        max_retries: int = 3
    ) -> str:
        """
        ç”Ÿæˆ JSON æ ¼å¼å†…å®¹ï¼ˆå¿«æ·æ–¹æ³•ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            str: JSON å­—ç¬¦ä¸²
        """
        return await self.generate(
            prompt=prompt,
            model=model,
            response_format="json",
            max_tokens=max_tokens,
            temperature=temperature,
            max_retries=max_retries
        )
    
    async def generate_batch(
        self,
        prompts: List[str],
        model: str = "gemini-2.0-flash-exp",
        **kwargs
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰
        
        Args:
            prompts: prompt åˆ—è¡¨
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            List[str]: ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        results = []
        for i, prompt in enumerate(prompts):
            logger.info(f"ğŸ“ Processing batch {i + 1}/{len(prompts)}")
            result = await self.generate(prompt, model=model, **kwargs)
            results.append(result)
        
        return results
    
    def get_model_info(self, model_name: str = "gemini-2.0-flash-exp") -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
        
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        try:
            # ä½¿ç”¨æ–° SDK çš„æ–¹å¼
            return {
                "name": model_name,
                "status": "available",
                "note": "Using new google.genai SDK"
            }
        except Exception as e:
            logger.error(f"âŒ Failed to get model info: {e}")
            return {"error": str(e)}
    
    def _extract_thinking(self, response) -> Optional[str]:
        """
        ä» Gemini å“åº”ä¸­æå–æ€è€ƒè¿‡ç¨‹
        
        Args:
            response: Gemini API å“åº”å¯¹è±¡
        
        Returns:
            Optional[str]: æ€è€ƒè¿‡ç¨‹æ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        try:
            # å°è¯•ä»å“åº”ä¸­æå– candidates
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                
                # æ£€æŸ¥ content.parts
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        # æŸ¥æ‰¾ thought å±æ€§ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å¸ƒå°”ï¼‰
                        if hasattr(part, 'thought'):
                            thought = part.thought
                            # æ£€æŸ¥æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹
                            if isinstance(thought, str) and thought:
                                logger.info(f"ğŸ§  Thinking process found: {len(thought)} chars")
                                return thought
                            # å¦‚æœæ˜¯å¸ƒå°”å€¼ Trueï¼ŒæŸ¥æ‰¾ text
                            elif thought is True and hasattr(part, 'text'):
                                text = part.text
                                if text:
                                    logger.info(f"ğŸ§  Thinking process found (via text): {len(text)} chars")
                                    return text
                        
                        # å¤‡é€‰æ–¹æ¡ˆï¼šæ£€æŸ¥ part çš„å…¶ä»–å±æ€§
                        if hasattr(part, 'text') and part.text:
                            # å¦‚æœ text åŒ…å«æ€è€ƒæ ‡è®°
                            text = part.text
                            if text.startswith("<thinking>") or text.startswith("æ€è€ƒè¿‡ç¨‹:"):
                                logger.info(f"ğŸ§  Thinking process found in text: {len(text)} chars")
                                return text
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å› None
            logger.debug("â„¹ï¸  No thinking process found in response")
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract thinking process: {e}")
            return None
    
    async def close(self):
        """å…³é—­å¼‚æ­¥å®¢æˆ·ç«¯"""
        try:
            if hasattr(self, 'async_client') and hasattr(self.async_client, 'aclose'):
                await self.async_client.aclose()
                logger.info("âœ… Async client closed")
            else:
                logger.info("â„¹ï¸  Async client does not require explicit close")
        except Exception as e:
            logger.warning(f"âš ï¸ Error closing async client: {e}")
