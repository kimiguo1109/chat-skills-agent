"""
Token ç»Ÿè®¡æœåŠ¡ - æŒä¹…åŒ–å­˜å‚¨ Token ä½¿ç”¨è®°å½•

åŠŸèƒ½ï¼š
- æŒ‰å¤©åˆ‡åˆ†å­˜å‚¨ JSON æ–‡ä»¶
- è®°å½•æ¯æ¬¡ API è°ƒç”¨çš„ token æ¶ˆè€—
- æ”¯æŒæ±‡æ€»ç»Ÿè®¡
"""

import json
import logging
from datetime import datetime, date
from pathlib import Path
from typing import Dict, Any, Optional, List
import threading
import asyncio

logger = logging.getLogger(__name__)


class TokenStatsService:
    """Token ç»Ÿè®¡æœåŠ¡"""
    
    def __init__(self, stats_dir: Optional[str] = None):
        """
        åˆå§‹åŒ– Token ç»Ÿè®¡æœåŠ¡
        
        Args:
            stats_dir: ç»Ÿè®¡æ–‡ä»¶å­˜å‚¨ç›®å½•ï¼Œé»˜è®¤ä¸º backend/token_stats/
        """
        if stats_dir:
            self.stats_dir = Path(stats_dir)
        else:
            # é»˜è®¤å­˜å‚¨åœ¨ backend/token_stats/
            self.stats_dir = Path(__file__).parent.parent.parent / "token_stats"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.stats_dir.mkdir(parents=True, exist_ok=True)
        
        # çº¿ç¨‹é”ï¼ˆç¡®ä¿å¹¶å‘å®‰å…¨ï¼‰
        self._lock = threading.Lock()
        
        logger.info(f"âœ… TokenStatsService initialized, stats_dir: {self.stats_dir}")
    
    def _get_today_file(self) -> Path:
        """è·å–ä»Šå¤©çš„ç»Ÿè®¡æ–‡ä»¶è·¯å¾„"""
        today = date.today().isoformat()  # 2025-11-27
        return self.stats_dir / f"token_stats_{today}.json"
    
    def _load_today_stats(self) -> Dict[str, Any]:
        """åŠ è½½ä»Šå¤©çš„ç»Ÿè®¡æ•°æ®"""
        file_path = self._get_today_file()
        
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                logger.error(f"âŒ Failed to load stats file: {e}")
                return self._create_empty_stats()
        else:
            return self._create_empty_stats()
    
    def _create_empty_stats(self) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„ç»Ÿè®¡ç»“æ„"""
        return {
            "date": date.today().isoformat(),
            "summary": {
                "total_requests": 0,
                "total_internal_tokens": 0,
                "intent_router_tokens": 0,
                "skill_execution_tokens": 0,
                "memory_operation_tokens": 0,
                "external_api_calls": 0,
                "llm_calls": 0,
                "thinking_model_calls": 0,  # ğŸ†• æ€è€ƒæ¨¡å‹è°ƒç”¨æ¬¡æ•°
                "total_generation_time": 0,  # ğŸ†• æ€»ç”Ÿæˆè€—æ—¶
                "models_used": {}  # ğŸ†• å„æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
            },
            "records": []
        }
    
    def _save_stats(self, stats: Dict[str, Any]):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶"""
        file_path = self._get_today_file()
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            logger.debug(f"ğŸ“ Saved stats to {file_path.name}")
        except IOError as e:
            logger.error(f"âŒ Failed to save stats file: {e}")
    
    def record_usage(
        self,
        user_id: str,
        session_id: str,
        message: str,
        intent: str,
        content_type: str,
        token_usage: Dict[str, Any],
        file_uris: Optional[List[str]] = None
    ):
        """
        è®°å½•ä¸€æ¬¡ API è°ƒç”¨çš„ token ä½¿ç”¨
        
        Args:
            user_id: ç”¨æˆ· ID
            session_id: ä¼šè¯ ID
            message: ç”¨æˆ·æ¶ˆæ¯
            intent: è¯†åˆ«çš„æ„å›¾
            content_type: å†…å®¹ç±»å‹
            token_usage: Token ä½¿ç”¨ç»Ÿè®¡
            file_uris: é™„ä»¶åˆ—è¡¨
        """
        with self._lock:
            stats = self._load_today_stats()
            
            # æ„å»ºè®°å½•
            record = {
                "timestamp": datetime.now().isoformat(),
                "user_id": user_id,
                "session_id": session_id,
                "message": message[:100] + ("..." if len(message) > 100 else ""),
                "intent": intent,
                "content_type": content_type,
                "has_files": bool(file_uris),
                "file_count": len(file_uris) if file_uris else 0,
                "token_usage": token_usage
            }
            
            # æ·»åŠ è®°å½•
            stats["records"].append(record)
            
            # æ›´æ–°æ±‡æ€»
            summary = stats["summary"]
            summary["total_requests"] += 1
            summary["total_internal_tokens"] += token_usage.get("total_internal_tokens", 0)
            summary["intent_router_tokens"] += token_usage.get("intent_router", {}).get("tokens", 0)
            
            skill_exec = token_usage.get("skill_execution", {})
            summary["skill_execution_tokens"] += skill_exec.get("total_tokens", 0)
            
            if skill_exec.get("source") == "external_api":
                summary["external_api_calls"] += 1
            elif skill_exec.get("source") == "llm":
                summary["llm_calls"] += 1
                
                # ğŸ†• ç»Ÿè®¡æ€è€ƒæ¨¡å‹è°ƒç”¨
                if skill_exec.get("thinking_mode"):
                    summary["thinking_model_calls"] += 1
                
                # ğŸ†• ç»Ÿè®¡ç”Ÿæˆè€—æ—¶
                summary["total_generation_time"] += skill_exec.get("generation_time", 0)
                
                # ğŸ†• ç»Ÿè®¡å„æ¨¡å‹ä½¿ç”¨æƒ…å†µ
                model_name = skill_exec.get("model", "unknown")
                if "models_used" not in summary:
                    summary["models_used"] = {}
                if model_name not in summary["models_used"]:
                    summary["models_used"][model_name] = {"calls": 0, "tokens": 0}
                summary["models_used"][model_name]["calls"] += 1
                summary["models_used"][model_name]["tokens"] += skill_exec.get("total_tokens", 0)
            
            memory_ops = token_usage.get("memory_operations", {})
            summary["memory_operation_tokens"] += (
                memory_ops.get("compression_tokens", 0) + 
                memory_ops.get("summary_tokens", 0)
            )
            
            # ä¿å­˜
            self._save_stats(stats)
            
            logger.info(
                f"ğŸ“Š Token usage recorded: user={user_id}, "
                f"intent={intent}, tokens={token_usage.get('total_internal_tokens', 0)}"
            )
    
    def get_today_summary(self) -> Dict[str, Any]:
        """è·å–ä»Šå¤©çš„æ±‡æ€»ç»Ÿè®¡"""
        stats = self._load_today_stats()
        return {
            "date": stats["date"],
            "summary": stats["summary"]
        }
    
    def get_today_records(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–ä»Šå¤©çš„è¯¦ç»†è®°å½•"""
        stats = self._load_today_stats()
        records = stats.get("records", [])
        # è¿”å›æœ€è¿‘çš„è®°å½•
        return records[-limit:] if len(records) > limit else records
    
    def get_stats_by_date(self, target_date: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„ç»Ÿè®¡æ•°æ®
        
        Args:
            target_date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ YYYY-MM-DD
        
        Returns:
            ç»Ÿè®¡æ•°æ®ï¼Œæˆ– Noneï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        """
        file_path = self.stats_dir / f"token_stats_{target_date}.json"
        
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                logger.error(f"âŒ Failed to load stats for {target_date}: {e}")
                return None
        else:
            return None
    
    def list_available_dates(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æœ‰ç»Ÿè®¡æ•°æ®çš„æ—¥æœŸ"""
        dates = []
        for file in self.stats_dir.glob("token_stats_*.json"):
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            date_str = file.stem.replace("token_stats_", "")
            dates.append(date_str)
        return sorted(dates, reverse=True)  # æœ€æ–°çš„åœ¨å‰


# å•ä¾‹æ¨¡å¼
_token_stats_service: Optional[TokenStatsService] = None


def get_token_stats_service() -> TokenStatsService:
    """è·å– TokenStatsService å•ä¾‹"""
    global _token_stats_service
    if _token_stats_service is None:
        _token_stats_service = TokenStatsService()
    return _token_stats_service

