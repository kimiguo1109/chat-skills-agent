"""
Kimi (Moonshot AI) API Service Wrapper
ä½¿ç”¨ Novita AI æä¾›çš„ Kimi API ä»£ç†

æ”¯æŒåŠŸèƒ½ï¼š
- éæµå¼ç”Ÿæˆ
- æµå¼ç”Ÿæˆ
- Reasoning æ¨¡å¼ï¼ˆç±»ä¼¼ Gemini çš„ thinkingï¼‰
- OpenAI SDK å…¼å®¹
"""
import logging
from typing import Dict, Any, Optional, AsyncGenerator
import json
from openai import OpenAI, AsyncOpenAI
from ..config import settings

logger = logging.getLogger(__name__)


class KimiClient:
    """Kimi (Moonshot AI) API Client"""
    
    def __init__(self):
        """åˆå§‹åŒ– Kimi Clientï¼ˆé€šè¿‡ Novita AIï¼‰"""
        self.api_key = settings.KIMI_API_KEY
        self.base_url = settings.KIMI_BASE_URL
        self.model = settings.KIMI_MODEL
        
        # åŒæ­¥å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        # å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆç”¨äºæµå¼ï¼‰
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        logger.info(f"âœ… Kimi client initialized via Novita AI")
        logger.info(f"ğŸ“ Base URL: {self.base_url}")
        logger.info(f"ğŸ¤– Model: {self.model}")
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        response_format: str = "text",
        temperature: float = 1.0,  # âš¡ æé«˜åˆ° 1.0 åŠ å¿«ç”Ÿæˆé€Ÿåº¦
        max_tokens: int = 4096,
        thinking_budget: Optional[int] = None,
        return_thinking: bool = True
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆéæµå¼ï¼‰
        
        Args:
            prompt: æç¤ºè¯ï¼ˆå·²æ ¼å¼åŒ–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
            response_format: å“åº”æ ¼å¼ ("text" or "json")
            temperature: æ¸©åº¦å‚æ•° [0, 1]ï¼ˆKimi èŒƒå›´ï¼Œ1.0 æœ€å¿«ï¼‰
            max_tokens: æœ€å¤§ token æ•°
            thinking_budget: Thinking é¢„ç®—ï¼ˆKimi é€šè¿‡ max_tokens æ§åˆ¶ï¼‰
            return_thinking: æ˜¯å¦è¿”å› thinking è¿‡ç¨‹
        
        Returns:
            Dict åŒ…å«: content, thinking, usage
        """
        model_to_use = model or self.model
        
        # âš¡âš¡âš¡ åº”ç”¨ thinking_budget æ§åˆ¶ï¼ˆä¼˜åŒ–ç‰ˆï¼ŒåŠ å¿«å“åº”ï¼‰
        if thinking_budget:
            if thinking_budget <= 32:
                # ğŸš€ æé€Ÿæ¨¡å¼ï¼šå¿«é€Ÿå“åº”ï¼Œç´§å‡‘è¾“å‡º
                content_budget = 2500
            elif thinking_budget <= 48:
                # âš¡ å¿«é€Ÿæ¨¡å¼ï¼šå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
                content_budget = 3000
            elif thinking_budget <= 64:
                # æ ‡å‡†æ¨¡å¼
                content_budget = 3500
            elif thinking_budget <= 96:
                # å¹³è¡¡æ¨¡å¼
                content_budget = 4000
            else:
                # æ·±åº¦æ¨¡å¼
                content_budget = 4500
            actual_max_tokens = thinking_budget + content_budget
            logger.info(f"âš¡ Token Budget: thinking={thinking_budget}, content={content_budget}, total={actual_max_tokens}")
        else:
            actual_max_tokens = max_tokens
        
        # âš¡âš¡âš¡ ä¸å†æ·»åŠ  system message - çº¦æŸå·²åœ¨ skill prompt ä¸­å®šä¹‰
        # Skill prompt å·²åŒ…å«ï¼š
        # - æ€ç»´é™åˆ¶ (Thinking): STOP THINKING. OUTPUT JSON DIRECTLY.
        # - æ•°é‡è¦æ±‚ã€æ ¼å¼è¦æ±‚ã€å†…å®¹ä¸€è‡´æ€§ç­‰
        # 
        # é¿å…é‡å¤çº¦æŸå¯¼è‡´ thinking è¿‡äºå¤æ‚
        messages = [{"role": "user", "content": prompt}]
        
        logger.info(f"ğŸš€ Generating: model={model_to_use}, temp={temperature}, max_tokens={actual_max_tokens}, thinking_budget={thinking_budget}")
        logger.info(f"â³ Waiting for LLM response... (expected ~15-30s, max 60s)")
        
        try:
            # Kimi API è°ƒç”¨ï¼ˆé€šè¿‡ Novita AIï¼‰
            import time
            start_time = time.time()
            last_log_time = start_time
            
            response = self.client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=actual_max_tokens,  # âš¡âš¡âš¡ ä½¿ç”¨å®é™…è®¡ç®—çš„ max_tokens
                stream=False
            )
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… LLM response received in {elapsed:.1f}s")
            
            choice = response.choices[0]
            content = choice.message.content or ""
            
            # æå– reasoning_contentï¼ˆKimi çš„ thinking æ¨¡å¼ï¼‰
            reasoning_content = getattr(choice.message, 'reasoning_content', None) or ""
            
            # æå– token ä½¿ç”¨ä¿¡æ¯
            usage = response.usage
            
            # ğŸ†• ä¼°ç®— thinking tokensï¼šreasoning_content çš„å­—ç¬¦æ•° / 2ï¼ˆä¸­æ–‡çº¦2å­—ç¬¦/tokenï¼‰
            estimated_thinking_tokens = len(reasoning_content) // 2 if reasoning_content else 0
            
            usage_stats = {
                "prompt_tokens": usage.prompt_tokens if usage else 0,
                "completion_tokens": usage.completion_tokens if usage else 0,
                "total_tokens": usage.total_tokens if usage else 0,
                "thinking_tokens": estimated_thinking_tokens,  # ğŸ†• ä» reasoning_content ä¼°ç®—
                "thinking_chars": len(reasoning_content) if reasoning_content else 0,
                "content_chars": len(content)
            }
            
            logger.info(f"âœ… Generation complete: {len(content)} chars, {usage_stats['total_tokens']} tokens")
            if reasoning_content:
                logger.info(f"ğŸ§  Reasoning: {len(reasoning_content)} chars (~{estimated_thinking_tokens} tokens)")
                logger.debug(f"ğŸ§  Reasoning content preview: {reasoning_content[:500]}")
            
            # JSON è§£æï¼ˆå¦‚æœéœ€è¦ï¼‰
            result = content
            if response_format == "json":
                try:
                    # ğŸ”¥ å¯¹äº thinking æ¨¡å‹ï¼ŒJSON å¯èƒ½åœ¨ reasoning_content ä¸­
                    json_source = content if content.strip() else reasoning_content
                    if not json_source.strip():
                        logger.warning("âš ï¸ Both content and reasoning_content are empty, cannot parse JSON")
                        result = content
                    else:
                        logger.debug(f"ğŸ” Attempting JSON parse from {'content' if content.strip() else 'reasoning_content'}: {json_source[:300]}")
                        # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
                        json_str = json_source
                        if "```json" in json_source:
                            json_str = json_source.split("```json")[1].split("```")[0].strip()
                        elif "```" in json_source:
                            json_str = json_source.split("```")[1].split("```")[0].strip()
                        
                        # ğŸ”§ ä¿®å¤ LaTeX è½¬ä¹‰é—®é¢˜ï¼ˆå¦‚ \vec, \fracï¼‰
                        json_str = self._fix_latex_escapes(json_str)
                        
                        result = json.loads(json_str)
                        logger.info(f"âœ… JSON parsed successfully from {'content' if content.strip() else 'reasoning_content'}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to parse JSON, returning raw content: {e}")
                    result = content if content.strip() else reasoning_content
            
            return {
                "content": result,
                "thinking": reasoning_content if return_thinking else "",
                "usage": usage_stats
            }
        
        except Exception as e:
            logger.error(f"âŒ Kimi generation error: {e}")
            raise
    
    def _fix_latex_escapes(self, json_str: str) -> str:
        """
        ä¿®å¤ JSON å­—ç¬¦ä¸²ä¸­ LaTeX å…¬å¼çš„è½¬ä¹‰é—®é¢˜
        
        LaTeX å…¬å¼ä¸­çš„åæ–œæ ï¼ˆå¦‚ \vec, \fracï¼‰åœ¨ JSON å­—ç¬¦ä¸²ä¸­éœ€è¦è½¬ä¹‰ä¸º \\
        
        Args:
            json_str: JSON å­—ç¬¦ä¸²
        
        Returns:
            ä¿®å¤åçš„ JSON å­—ç¬¦ä¸²
        """
        import re
        
        # åŒ¹é… JSON å­—ç¬¦ä¸²å€¼ï¼ˆ"..."ï¼‰
        def fix_string_with_latex(match):
            full_match = match.group(0)
            content = match.group(1)
            
            # å¦‚æœå†…å®¹ä¸­ä¸åŒ…å« $ï¼Œè¯´æ˜å¯èƒ½æ²¡æœ‰ LaTeXï¼Œç›´æ¥è¿”å›
            if '$' not in content:
                return full_match
            
            # ä¿®å¤ LaTeX å‘½ä»¤ï¼š\letter -> \\letter
            result = []
            i = 0
            while i < len(content):
                char = content[i]
                
                if char == '\\' and i + 1 < len(content):
                    next_char = content[i + 1]
                    # å¦‚æœä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å­—æ¯ï¼ˆLaTeX å‘½ä»¤ï¼‰ï¼Œéœ€è¦è½¬ä¹‰
                    if next_char.isalpha():
                        # æ£€æŸ¥å‰é¢æ˜¯å¦å·²ç»æ˜¯è½¬ä¹‰çš„åæ–œæ 
                        if i > 0 and content[i - 1] == '\\':
                            result.append(char)
                            result.append(next_char)
                        else:
                            # éœ€è¦è½¬ä¹‰ï¼šæ·»åŠ é¢å¤–çš„åæ–œæ 
                            result.append('\\\\')
                            result.append(next_char)
                        i += 2
                        continue
                    else:
                        result.append(char)
                        result.append(next_char)
                        i += 2
                        continue
                else:
                    result.append(char)
                
                i += 1
            
            fixed_content = ''.join(result)
            return f'"{fixed_content}"'
        
        # åŒ¹é… JSON å­—ç¬¦ä¸²å€¼ï¼ˆåŒ…æ‹¬è½¬ä¹‰çš„å¼•å·ï¼‰
        pattern = r'"((?:[^"\\]|\\.)*)"'
        fixed_json = re.sub(pattern, fix_string_with_latex, json_str)
        
        return fixed_json
    
    async def generate_stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 1.0,  # âš¡âš¡âš¡ å‚ç…§åœ¨çº¿ç‰ˆï¼š1.0æœ€å¤§åŒ–é€Ÿåº¦
        max_tokens: int = 131072,  # âš¡âš¡âš¡ å‚ç…§åœ¨çº¿ç‰ˆï¼š131072
        thinking_budget: Optional[int] = None,
        return_thinking: bool = True,
        buffer_size: int = 1  # âš¡âš¡âš¡âš¡ æé™ä¼˜åŒ–ï¼šæ¯ä¸ªå­—ç¬¦ç«‹å³å‘é€
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆæµå¼ + ä¼˜åŒ–ç¼“å†²ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°ï¼ˆæ€»è¾“å‡ºé™åˆ¶ï¼‰
            thinking_budget: Thinking é¢„ç®—ï¼ˆå¦‚æœè®¾ç½®ï¼Œä¼šè¦†ç›– max_tokensï¼‰
            return_thinking: æ˜¯å¦è¿”å› thinking
            buffer_size: ç¼“å†²åŒºå¤§å°ï¼ˆé»˜è®¤1å­—ç¬¦ï¼Œæé™æµå¼ï¼‰
        
        Yields:
            Dict: {"type": "thinking|content|done|error", ...}
        """
        model_to_use = model or self.model
        
        # âš¡âš¡âš¡ çœŸæ­£çš„ Token æ§åˆ¶ï¼šæ ¹æ® thinking_budget åŠ¨æ€è°ƒæ•´ max_tokens
        # Thinking æ¨¡å‹çš„è¾“å‡º = thinking_content + actual_content
        # 
        # ç­–ç•¥ï¼š
        # - thinking_budget: æ§åˆ¶æ¨ç†é•¿åº¦ï¼ˆ64-96 tokensï¼‰
        # - content_budget: ç¡®ä¿è¾“å‡ºè´¨é‡ï¼ˆ3500-4000 tokensï¼‰
        # - è‡ªç„¶çº¦æŸ + max_tokens é™åˆ¶ï¼Œä¸ä½¿ç”¨è¿‡åº¦åˆ»æ„çš„ system message
        # 
        # å®æµ‹æ•°æ®ï¼š
        # - Explain Skill: thinking ~150-200 tokens, content ~1500 tokens
        # - Quiz (3é¢˜): thinking ~100-150 tokens, content ~1200 tokens
        # - Flashcard (5å¼ ): thinking ~80-120 tokens, content ~800 tokens
        if thinking_budget:
            # æ ¹æ® thinking_budget æ™ºèƒ½åˆ†é… content budget
            # ğŸš€ ä¼˜åŒ–ç‰ˆï¼šé™ä½ budgetï¼ŒåŠ å¿«å“åº”é€Ÿåº¦
            if thinking_budget <= 32:
                # ğŸš€ æé€Ÿæ¨¡å¼ï¼šå¿«é€Ÿå“åº”
                content_budget = 2500
            elif thinking_budget <= 48:
                # âš¡ å¿«é€Ÿæ¨¡å¼ï¼šå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
                content_budget = 3000
            elif thinking_budget <= 64:
                # æ ‡å‡†æ¨¡å¼
                content_budget = 3500
            elif thinking_budget <= 96:
                # å¹³è¡¡æ¨¡å¼
                content_budget = 4000
            elif thinking_budget <= 128:
                # æ·±åº¦æ¨¡å¼
                content_budget = 4500
            else:
                # è¶…æ·±åº¦æ¨¡å¼
                content_budget = 5000
            
            actual_max_tokens = thinking_budget + content_budget
            logger.info(f"âš¡ Token Budget: thinking={thinking_budget}, content={content_budget}, total={actual_max_tokens}")
        else:
            actual_max_tokens = max_tokens
            logger.info(f"âš¡ Using default max_tokens={actual_max_tokens}")
        
        # âš¡âš¡âš¡ ä¸å†æ·»åŠ  system message - çº¦æŸå·²åœ¨ skill prompt ä¸­å®šä¹‰
        # Skill prompt å·²åŒ…å«ï¼š
        # - æ€ç»´é™åˆ¶ (Thinking): STOP THINKING. OUTPUT JSON DIRECTLY.
        # - æ•°é‡è¦æ±‚ã€æ ¼å¼è¦æ±‚ã€å†…å®¹ä¸€è‡´æ€§ç­‰
        # 
        # é¿å…é‡å¤çº¦æŸå¯¼è‡´ thinking è¿‡äºå¤æ‚
        messages = [{"role": "user", "content": prompt}]
        
        logger.info(f"ğŸŒŠ Starting streaming: model={model_to_use}, max_tokens={actual_max_tokens}, thinking_budget={thinking_budget}")
        logger.info(f"â³ Connecting to LLM stream... (thinking will appear first, then content)")
        
        # ç´¯åŠ å™¨
        content_accumulated = []
        reasoning_accumulated = []
        
        # ğŸ†• ç¼“å†²åŒºï¼ˆå‡å°‘ç¢ç‰‡åŒ–ï¼‰
        content_buffer = []
        reasoning_buffer = []
        
        # ğŸ†• è¿›åº¦è¿½è¸ª
        import time
        start_time = time.time()
        first_chunk_time = None
        thinking_complete_time = None
        
        try:
            # Kimi æµå¼ APIï¼ˆä½¿ç”¨OpenAIå…¼å®¹å‚æ•°ï¼‰
            # ğŸ†• å°è¯•å¯ç”¨ stream_options ä»¥è·å– usage ä¿¡æ¯
            stream = await self.async_client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=actual_max_tokens,  # âš¡âš¡âš¡ ä½¿ç”¨å®é™…è®¡ç®—çš„ max_tokens
                top_p=1.0,  # âš¡ æ§åˆ¶é‡‡æ ·èŒƒå›´
                presence_penalty=0.0,  # âš¡ æ— é‡å¤æƒ©ç½š
                frequency_penalty=0.0,  # âš¡ æ— é¢‘ç‡æƒ©ç½š
                stream=True,
                stream_options={"include_usage": True}  # ğŸ†• è¯·æ±‚è¿”å› usage ä¿¡æ¯
                # âš ï¸ æ³¨æ„ï¼štop_kä¸è¢«OpenAI APIæ”¯æŒï¼Œå·²ç§»é™¤
            )
            
            # ç”¨äºå­˜å‚¨æœ€ç»ˆçš„ usage ä¿¡æ¯
            final_usage = None
            
            async for chunk in stream:
                # ğŸ†• æ£€æŸ¥æ˜¯å¦æœ‰ usage ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunkï¼‰
                if hasattr(chunk, 'usage') and chunk.usage:
                    final_usage = {
                        "prompt_tokens": chunk.usage.prompt_tokens,
                        "completion_tokens": chunk.usage.completion_tokens,
                        "total_tokens": chunk.usage.total_tokens
                    }
                    logger.info(f"ğŸ“Š Received usage from API: {final_usage}")
                
                if not chunk.choices:
                    continue
                
                # ğŸ†• è®°å½•é¦–ä¸ª chunk åˆ°è¾¾æ—¶é—´
                if first_chunk_time is None:
                    first_chunk_time = time.time()
                    logger.info(f"âœ… First chunk received in {first_chunk_time - start_time:.1f}s")
                
                delta = chunk.choices[0].delta
                
                # æå– reasoning_contentï¼ˆKimi çš„ thinkingï¼‰
                reasoning_chunk = getattr(delta, 'reasoning_content', None)
                if reasoning_chunk and isinstance(reasoning_chunk, str):
                    reasoning_accumulated.append(reasoning_chunk)
                    
                    # ğŸ”¥ äºŒæ¬¡åˆ†å—ï¼šç¡®ä¿thinkingä¹Ÿæ˜¯æµå¼çš„
                    chunk_size = 20  # ğŸš€ å¢åŠ åˆ°20ä¸ªå­—ç¬¦ï¼Œæé«˜æµå¼é€Ÿåº¦
                    for i in range(0, len(reasoning_chunk), chunk_size):
                        mini_chunk = reasoning_chunk[i:i+chunk_size]
                        reasoning_buffer.append(mini_chunk)
                        
                        # ç«‹å³å‘é€
                        buffered_text = "".join(reasoning_buffer)
                        if len(buffered_text) >= buffer_size:
                            # logger.info(f"ğŸ§  Thinking stream: {len(buffered_text)} chars")
                            yield {
                                "type": "thinking",
                                "text": buffered_text,
                                "accumulated": "".join(reasoning_accumulated)
                            }
                            reasoning_buffer = []
                
                # æå– content
                content_chunk = delta.content
                if content_chunk and isinstance(content_chunk, str):
                    # ğŸ†• è®°å½• thinking å®Œæˆæ—¶é—´ï¼ˆç¬¬ä¸€æ¬¡æ”¶åˆ° contentï¼‰
                    if thinking_complete_time is None and len(reasoning_accumulated) > 0:
                        thinking_complete_time = time.time()
                        logger.info(f"ğŸ§  Thinking complete in {thinking_complete_time - start_time:.1f}s, content streaming started")
                    
                    content_accumulated.append(content_chunk)
                    
                    # ğŸ”¥ äºŒæ¬¡åˆ†å—ï¼šå¦‚æœAPIè¿”å›çš„chunkå¤ªå¤§ï¼Œæ‹†åˆ†æˆå°å—æµå¼å‘é€
                    # è¿™ç¡®ä¿äº†å³ä½¿APIä¸€æ¬¡è¿”å›å¤§å—å†…å®¹ï¼Œç”¨æˆ·ä¹Ÿèƒ½çœ‹åˆ°æµå¼æ•ˆæœ
                    chunk_size = 20  # ğŸš€ å¢åŠ åˆ°20ä¸ªå­—ç¬¦ï¼Œæé«˜æµå¼é€Ÿåº¦
                    for i in range(0, len(content_chunk), chunk_size):
                        mini_chunk = content_chunk[i:i+chunk_size]
                        content_buffer.append(mini_chunk)
                        
                        # ç«‹å³å‘é€ï¼ˆbuffer_size=1æ„å‘³ç€ä¸å†ç´¯ç§¯ï¼‰
                        buffered_text = "".join(content_buffer)
                        if len(buffered_text) >= buffer_size:
                            # logger.info(f"ğŸ“ Content stream: {len(buffered_text)} chars")
                            yield {
                                "type": "content",
                                "text": buffered_text,
                                "accumulated": "".join(content_accumulated)
                            }
                            content_buffer = []
            
            # ğŸ†• å‘é€å‰©ä½™ç¼“å†²åŒºå†…å®¹
            if reasoning_buffer:
                buffered_text = "".join(reasoning_buffer)
                # logger.info(f"ğŸ§  Reasoning final flush: {len(buffered_text)} chars")
                yield {
                    "type": "thinking",
                    "text": buffered_text,
                    "accumulated": "".join(reasoning_accumulated)
                }
            
            if content_buffer:
                buffered_text = "".join(content_buffer)
                # logger.info(f"ğŸ“ Content final flush: {len(buffered_text)} chars")
                yield {
                    "type": "content",
                    "text": buffered_text,
                    "accumulated": "".join(content_accumulated)
                }
            
            # å®Œæˆ
            full_thinking = "".join(reasoning_accumulated)
            full_content = "".join(content_accumulated)
            
            # è®¡ç®—æµå¼ç”Ÿæˆçš„æ—¶é—´
            total_time = time.time() - start_time
            
            logger.info(f"âœ… Streaming generation complete")
            logger.info(f"ğŸ“Š Final content: {len(full_content)} chars")
            logger.info(f"ğŸ§  Final reasoning: {len(full_thinking)} chars")
            
            # ğŸ†• Token ä½¿ç”¨ç»Ÿè®¡ï¼ˆä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„ç²¾ç¡®æ•°æ®ï¼‰
            if final_usage:
                # ä½¿ç”¨ API è¿”å›çš„ç²¾ç¡®æ•°æ®
                prompt_tokens = final_usage.get("prompt_tokens", 0)
                completion_tokens = final_usage.get("completion_tokens", 0)
                total_tokens = final_usage.get("total_tokens", 0)
                
                logger.info(f"ğŸ“Š Token Usage (Kimi Stream - EXACT)")
                logger.info(f"   â€¢ Input:  {prompt_tokens:,} tokens")
                logger.info(f"   â€¢ Output: {completion_tokens:,} tokens")
                logger.info(f"   â€¢ Total:  {total_tokens:,} tokens")
                logger.info(f"   â€¢ Time:   {total_time:.1f}s | Model: {model_to_use}")
                
                # ğŸ”¥ å‘é€ç²¾ç¡®çš„ usage ä¿¡æ¯ç»™ orchestrator
                yield {
                    "type": "usage",
                    "usage": {
                        "prompt_tokens": prompt_tokens,
                        "completion_tokens": completion_tokens,
                        "total_tokens": total_tokens,
                        "thinking_chars": len(full_thinking),
                        "content_chars": len(full_content),
                        "generation_time": total_time,
                        "model": model_to_use,
                        "source": "api"
                    }
                }
            else:
                # å›é€€åˆ°ä¼°ç®—ï¼ˆæµå¼APIå¯èƒ½ä¸è¿”å›usageï¼‰
                # ä¼°ç®—æ–¹æ³•ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
                # è¿™é‡Œä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼šå¹³å‡2å­—ç¬¦/token
                estimated_thinking_tokens = len(full_thinking) // 2
                estimated_content_tokens = len(full_content) // 2
                estimated_total_output = estimated_thinking_tokens + estimated_content_tokens
                
                logger.info(f"ğŸ“Š Token Usage (Kimi Stream - ESTIMATED)")
                logger.info(f"   â€¢ Thinking: ~{estimated_thinking_tokens:,} tokens")
                logger.info(f"   â€¢ Content:  ~{estimated_content_tokens:,} tokens")
                logger.info(f"   â€¢ Total:    ~{estimated_total_output:,} tokens")
                logger.info(f"   â€¢ Time:     {total_time:.1f}s | Model: {model_to_use}")
                
                # ğŸ”¥ å‘é€ä¼°ç®—çš„ usage ä¿¡æ¯ç»™ orchestrator
                yield {
                    "type": "usage",
                    "usage": {
                        "thinking_tokens": estimated_thinking_tokens,
                        "content_tokens": estimated_content_tokens,
                        "total_output_tokens": estimated_total_output,
                        "thinking_chars": len(full_thinking),
                        "content_chars": len(full_content),
                        "generation_time": total_time,
                        "model": model_to_use,
                        "source": "estimated"
                    }
                }
            
            # ğŸ”¥ ä¸åœ¨è¿™é‡Œå‘é€doneäº‹ä»¶ï¼
            # doneäº‹ä»¶åº”è¯¥ç”±skill_orchestratorå‘é€ï¼ŒåŒ…å«è§£æåçš„content
            # è¿™é‡Œåªæ˜¯æ ‡è®°æµå¼ç»“æŸ
            logger.info(f"ğŸ Stream ended (orchestrator will send done event)")
        
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Streaming generation error: {e}")
            
            # 503 é”™è¯¯å¤„ç†
            if "503" in error_msg or "overloaded" in error_msg.lower():
                yield {
                    "type": "error",
                    "error": "AIæœåŠ¡æš‚æ—¶è¿‡è½½ï¼Œè¯·ç­‰å¾…å‡ ç§’åé‡è¯• (503 Service Overloaded)",
                    "code": 503
                }
            else:
                yield {
                    "type": "error",
                    "error": error_msg,
                    "code": 500
                }
    
    async def generate_json(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 1.0,  # âš¡ æé«˜åˆ° 1.0 åŠ å¿«ç”Ÿæˆé€Ÿåº¦
        max_tokens: int = 4096
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆ JSON æ ¼å¼å†…å®¹
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ1.0 æœ€å¿«ï¼‰
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            è§£æåçš„ JSON å¯¹è±¡
        """
        result = await self.generate(
            prompt=prompt,
            model=model,
            response_format="json",
            temperature=temperature,
            max_tokens=max_tokens,
            return_thinking=False
        )
        return result["content"]

