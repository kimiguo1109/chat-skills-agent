"""
Kimi (Moonshot AI) API Service Wrapper
ä½¿ç”¨ Novita AI æä¾›çš„ Kimi API ä»£ç†

æ”¯æŒåŠŸèƒ½ï¼š
- éæµå¼ç”Ÿæˆ
- æµå¼ç”Ÿæˆ
- Reasoning æ¨¡å¼ï¼ˆç±»ä¼¼ Gemini çš„ thinkingï¼‰
- OpenAI SDK å…¼å®¹
"""
import logging
from typing import Dict, Any, Optional, AsyncGenerator
import json
from openai import OpenAI, AsyncOpenAI
from ..config import settings

logger = logging.getLogger(__name__)


class KimiClient:
    """Kimi (Moonshot AI) API Client"""
    
    def __init__(self):
        """åˆå§‹åŒ– Kimi Clientï¼ˆé€šè¿‡ Novita AIï¼‰"""
        self.api_key = settings.KIMI_API_KEY
        self.base_url = settings.KIMI_BASE_URL
        self.model = settings.KIMI_MODEL
        
        # åŒæ­¥å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        # å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆç”¨äºæµå¼ï¼‰
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        logger.info(f"âœ… Kimi client initialized via Novita AI")
        logger.info(f"ğŸ“ Base URL: {self.base_url}")
        logger.info(f"ğŸ¤– Model: {self.model}")
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        response_format: str = "text",
        temperature: float = 0.6,
        max_tokens: int = 4096,
        thinking_budget: Optional[int] = None,
        return_thinking: bool = True
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆéæµå¼ï¼‰
        
        Args:
            prompt: æç¤ºè¯ï¼ˆå·²æ ¼å¼åŒ–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
            response_format: å“åº”æ ¼å¼ ("text" or "json")
            temperature: æ¸©åº¦å‚æ•° [0, 1]ï¼ˆKimi èŒƒå›´ï¼‰
            max_tokens: æœ€å¤§ token æ•°
            thinking_budget: Thinking é¢„ç®—ï¼ˆKimi é€šè¿‡ max_tokens æ§åˆ¶ï¼‰
            return_thinking: æ˜¯å¦è¿”å› thinking è¿‡ç¨‹
        
        Returns:
            Dict åŒ…å«: content, thinking, usage
        """
        model_to_use = model or self.model
        
        # Kimi API ä½¿ç”¨ OpenAI æ ¼å¼
        messages = [
            {
                "role": "system", 
                "content": (
                    "ä½ æ˜¯ Kimiï¼Œç”± Moonshot AI æä¾›çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚\n\n"
                    "âš¡ æ€è€ƒç­–ç•¥ï¼š\n"
                    "- å¿«é€Ÿé«˜æ•ˆæ€è€ƒï¼Œæ§åˆ¶åœ¨10-20ç§’å†…å®Œæˆ\n"
                    "- æ€è€ƒè¿‡ç¨‹ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿é‡å¤\n"
                    "- ç›´å¥”æ ¸å¿ƒè¦ç‚¹ï¼Œçœç•¥ä¸å¿…è¦çš„ç»†èŠ‚\n"
                    "- ä¼˜å…ˆè¾“å‡ºé«˜è´¨é‡å†…å®¹ï¼Œè€Œéé•¿ç¯‡æ€è€ƒ\n\n"
                    "ğŸ“ è¾“å‡ºåŸåˆ™ï¼šæ¸…æ™°ã€å‡†ç¡®ã€é«˜æ•ˆ"
                )
            },
            {"role": "user", "content": prompt}
        ]
        
        logger.info(f"ğŸš€ Generating content: model={model_to_use}, temp={temperature}, max_tokens={max_tokens}")
        
        try:
            # Kimi API è°ƒç”¨ï¼ˆé€šè¿‡ Novita AIï¼‰
            response = self.client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=False
            )
            
            choice = response.choices[0]
            content = choice.message.content or ""
            
            # æå– reasoning_contentï¼ˆKimi çš„ thinking æ¨¡å¼ï¼‰
            reasoning_content = getattr(choice.message, 'reasoning_content', None) or ""
            
            # æå– token ä½¿ç”¨ä¿¡æ¯
            usage = response.usage
            usage_stats = {
                "prompt_tokens": usage.prompt_tokens if usage else 0,
                "completion_tokens": usage.completion_tokens if usage else 0,
                "total_tokens": usage.total_tokens if usage else 0,
                "reasoning_tokens": 0  # Kimi å¯èƒ½ä¸å•ç‹¬è®¡ç®—
            }
            
            logger.info(f"âœ… Generation complete: {len(content)} chars, {usage_stats['total_tokens']} tokens")
            if reasoning_content:
                logger.info(f"ğŸ§  Reasoning: {len(reasoning_content)} chars")
            
            # JSON è§£æï¼ˆå¦‚æœéœ€è¦ï¼‰
            result = content
            if response_format == "json":
                try:
                    # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
                    json_str = content
                    if "```json" in content:
                        json_str = content.split("```json")[1].split("```")[0].strip()
                    elif "```" in content:
                        json_str = content.split("```")[1].split("```")[0].strip()
                    result = json.loads(json_str)
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to parse JSON, returning raw content: {e}")
                    result = content
            
            return {
                "content": result,
                "thinking": reasoning_content if return_thinking else "",
                "usage": usage_stats
            }
        
        except Exception as e:
            logger.error(f"âŒ Kimi generation error: {e}")
            raise
    
    async def generate_stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 1.0,  # âš¡âš¡âš¡ å‚ç…§åœ¨çº¿ç‰ˆï¼š1.0æœ€å¤§åŒ–é€Ÿåº¦
        max_tokens: int = 131072,  # âš¡âš¡âš¡ å‚ç…§åœ¨çº¿ç‰ˆï¼š131072
        thinking_budget: Optional[int] = None,
        return_thinking: bool = True,
        buffer_size: int = 1  # âš¡âš¡âš¡âš¡ æé™ä¼˜åŒ–ï¼šæ¯ä¸ªå­—ç¬¦ç«‹å³å‘é€
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆæµå¼ + ä¼˜åŒ–ç¼“å†²ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            thinking_budget: Thinking é¢„ç®—
            return_thinking: æ˜¯å¦è¿”å› thinking
            buffer_size: ç¼“å†²åŒºå¤§å°ï¼ˆé»˜è®¤50å­—ç¬¦ï¼Œå‡å°‘ç¢ç‰‡åŒ–ï¼‰
        
        Yields:
            Dict: {"type": "thinking|content|done|error", ...}
        """
        model_to_use = model or self.model
        
        messages = [
            {
                "role": "system", 
                "content": (
                    "ä½ æ˜¯ Kimiï¼Œç”± Moonshot AI æä¾›çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚\n\n"
                    "âš¡ æ€è€ƒç­–ç•¥ï¼š\n"
                    "- å¿«é€Ÿé«˜æ•ˆæ€è€ƒï¼Œæ§åˆ¶åœ¨10-20ç§’å†…å®Œæˆ\n"
                    "- æ€è€ƒè¿‡ç¨‹ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿é‡å¤\n"
                    "- ç›´å¥”æ ¸å¿ƒè¦ç‚¹ï¼Œçœç•¥ä¸å¿…è¦çš„ç»†èŠ‚\n"
                    "- ä¼˜å…ˆè¾“å‡ºé«˜è´¨é‡å†…å®¹ï¼Œè€Œéé•¿ç¯‡æ€è€ƒ\n\n"
                    "ğŸ“ è¾“å‡ºåŸåˆ™ï¼šæ¸…æ™°ã€å‡†ç¡®ã€é«˜æ•ˆ"
                )
            },
            {"role": "user", "content": prompt}
        ]
        
        logger.info(f"ğŸŒŠ Starting streaming generation: model={model_to_use}, buffer={buffer_size} chars")
        
        # ç´¯åŠ å™¨
        content_accumulated = []
        reasoning_accumulated = []
        
        # ğŸ†• ç¼“å†²åŒºï¼ˆå‡å°‘ç¢ç‰‡åŒ–ï¼‰
        content_buffer = []
        reasoning_buffer = []
        
        try:
            # Kimi æµå¼ APIï¼ˆä½¿ç”¨OpenAIå…¼å®¹å‚æ•°ï¼‰
            stream = await self.async_client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=1.0,  # âš¡ æ§åˆ¶é‡‡æ ·èŒƒå›´
                presence_penalty=0.0,  # âš¡ æ— é‡å¤æƒ©ç½š
                frequency_penalty=0.0,  # âš¡ æ— é¢‘ç‡æƒ©ç½š
                stream=True
                # âš ï¸ æ³¨æ„ï¼štop_kä¸è¢«OpenAI APIæ”¯æŒï¼Œå·²ç§»é™¤
            )
            
            async for chunk in stream:
                if not chunk.choices:
                    continue
                
                delta = chunk.choices[0].delta
                
                # æå– reasoning_contentï¼ˆKimi çš„ thinkingï¼‰
                reasoning_chunk = getattr(delta, 'reasoning_content', None)
                if reasoning_chunk and isinstance(reasoning_chunk, str):
                    reasoning_buffer.append(reasoning_chunk)
                    reasoning_accumulated.append(reasoning_chunk)
                    
                    # ğŸ†• ç¼“å†²åŒºæ»¡äº†æ‰å‘é€
                    buffered_text = "".join(reasoning_buffer)
                    if len(buffered_text) >= buffer_size:
                        logger.info(f"ğŸ§  Reasoning buffer flush: {len(buffered_text)} chars")
                        yield {
                            "type": "thinking",
                            "text": buffered_text,
                            "accumulated": "".join(reasoning_accumulated)
                        }
                        reasoning_buffer = []
                
                # æå– content
                content_chunk = delta.content
                if content_chunk and isinstance(content_chunk, str):
                    content_buffer.append(content_chunk)
                    content_accumulated.append(content_chunk)
                    
                    # ğŸ†• ç¼“å†²åŒºæ»¡äº†æ‰å‘é€
                    buffered_text = "".join(content_buffer)
                    if len(buffered_text) >= buffer_size:
                        logger.info(f"ğŸ“ Content buffer flush: {len(buffered_text)} chars")
                        yield {
                            "type": "content",
                            "text": buffered_text,
                            "accumulated": "".join(content_accumulated)
                        }
                        content_buffer = []
            
            # ğŸ†• å‘é€å‰©ä½™ç¼“å†²åŒºå†…å®¹
            if reasoning_buffer:
                buffered_text = "".join(reasoning_buffer)
                logger.info(f"ğŸ§  Reasoning final flush: {len(buffered_text)} chars")
                yield {
                    "type": "thinking",
                    "text": buffered_text,
                    "accumulated": "".join(reasoning_accumulated)
                }
            
            if content_buffer:
                buffered_text = "".join(content_buffer)
                logger.info(f"ğŸ“ Content final flush: {len(buffered_text)} chars")
                yield {
                    "type": "content",
                    "text": buffered_text,
                    "accumulated": "".join(content_accumulated)
                }
            
            # å®Œæˆ
            full_thinking = "".join(reasoning_accumulated)
            full_content = "".join(content_accumulated)
            
            logger.info(f"âœ… Streaming generation complete")
            logger.info(f"ğŸ“Š Final content: {len(full_content)} chars")
            logger.info(f"ğŸ§  Final reasoning: {len(full_thinking)} chars")
            
            yield {
                "type": "done",
                "thinking": full_thinking,
                "content": full_content
            }
        
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Streaming generation error: {e}")
            
            # 503 é”™è¯¯å¤„ç†
            if "503" in error_msg or "overloaded" in error_msg.lower():
                yield {
                    "type": "error",
                    "error": "AIæœåŠ¡æš‚æ—¶è¿‡è½½ï¼Œè¯·ç­‰å¾…å‡ ç§’åé‡è¯• (503 Service Overloaded)",
                    "code": 503
                }
            else:
                yield {
                    "type": "error",
                    "error": error_msg,
                    "code": 500
                }
    
    async def generate_json(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 0.6,
        max_tokens: int = 4096
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆ JSON æ ¼å¼å†…å®¹
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            è§£æåçš„ JSON å¯¹è±¡
        """
        result = await self.generate(
            prompt=prompt,
            model=model,
            response_format="json",
            temperature=temperature,
            max_tokens=max_tokens,
            return_thinking=False
        )
        return result["content"]

