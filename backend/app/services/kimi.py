"""
Kimi (Moonshot AI) API Service Wrapper
ä½¿ç”¨ Novita AI æä¾›çš„ Kimi API ä»£ç†

æ”¯æŒåŠŸèƒ½ï¼š
- éæµå¼ç”Ÿæˆ
- æµå¼ç”Ÿæˆ
- Reasoning æ¨¡å¼ï¼ˆç±»ä¼¼ Gemini çš„ thinkingï¼‰
- OpenAI SDK å…¼å®¹
"""
import logging
from typing import Dict, Any, Optional, AsyncGenerator
import json
from openai import OpenAI, AsyncOpenAI
from ..config import settings

logger = logging.getLogger(__name__)


class KimiClient:
    """Kimi (Moonshot AI) API Client"""
    
    def __init__(self):
        """åˆå§‹åŒ– Kimi Clientï¼ˆé€šè¿‡ Novita AIï¼‰"""
        self.api_key = settings.KIMI_API_KEY
        self.base_url = settings.KIMI_BASE_URL
        self.model = settings.KIMI_MODEL
        
        # åŒæ­¥å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        # å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆç”¨äºæµå¼ï¼‰
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        logger.info(f"âœ… Kimi client initialized via Novita AI")
        logger.info(f"ğŸ“ Base URL: {self.base_url}")
        logger.info(f"ğŸ¤– Model: {self.model}")
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        response_format: str = "text",
        temperature: float = 0.6,
        max_tokens: int = 4096,
        thinking_budget: Optional[int] = None,
        return_thinking: bool = True
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆéæµå¼ï¼‰
        
        Args:
            prompt: æç¤ºè¯ï¼ˆå·²æ ¼å¼åŒ–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
            response_format: å“åº”æ ¼å¼ ("text" or "json")
            temperature: æ¸©åº¦å‚æ•° [0, 1]ï¼ˆKimi èŒƒå›´ï¼‰
            max_tokens: æœ€å¤§ token æ•°
            thinking_budget: Thinking é¢„ç®—ï¼ˆKimi é€šè¿‡ max_tokens æ§åˆ¶ï¼‰
            return_thinking: æ˜¯å¦è¿”å› thinking è¿‡ç¨‹
        
        Returns:
            Dict åŒ…å«: content, thinking, usage
        """
        model_to_use = model or self.model
        
        # âš¡âš¡âš¡ åº”ç”¨ thinking_budget æ§åˆ¶ï¼ˆä¸æµå¼ä¿æŒä¸€è‡´ï¼‰
        if thinking_budget:
            if thinking_budget <= 64:
                content_budget = 3500
            elif thinking_budget <= 128:
                content_budget = 4000
            else:
                content_budget = 5000
            actual_max_tokens = thinking_budget + content_budget
            logger.info(f"âš¡ Token Budget: thinking={thinking_budget}, content={content_budget}, total={actual_max_tokens}")
        else:
            actual_max_tokens = max_tokens
        
        # âš¡âš¡âš¡ æ·»åŠ ç³»ç»Ÿçº§çº¦æŸ
        messages = []
        if thinking_budget and thinking_budget <= 128:
            system_constraint = (
                f"CRITICAL: Strict {thinking_budget}-token thinking limit. "
                f"Be EXTREMELY concise - 2-4 sentences MAX. "
                f"Skip verbose reasoning. Focus only on core logic."
            )
            messages.append({"role": "system", "content": system_constraint})
        
        messages.append({"role": "user", "content": prompt})
        
        logger.info(f"ğŸš€ Generating: model={model_to_use}, temp={temperature}, max_tokens={actual_max_tokens}, thinking_budget={thinking_budget}")
        
        try:
            # Kimi API è°ƒç”¨ï¼ˆé€šè¿‡ Novita AIï¼‰
            response = self.client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=actual_max_tokens,  # âš¡âš¡âš¡ ä½¿ç”¨å®é™…è®¡ç®—çš„ max_tokens
                stream=False
            )
            
            choice = response.choices[0]
            content = choice.message.content or ""
            
            # æå– reasoning_contentï¼ˆKimi çš„ thinking æ¨¡å¼ï¼‰
            reasoning_content = getattr(choice.message, 'reasoning_content', None) or ""
            
            # æå– token ä½¿ç”¨ä¿¡æ¯
            usage = response.usage
            usage_stats = {
                "prompt_tokens": usage.prompt_tokens if usage else 0,
                "completion_tokens": usage.completion_tokens if usage else 0,
                "total_tokens": usage.total_tokens if usage else 0,
                "reasoning_tokens": 0  # Kimi å¯èƒ½ä¸å•ç‹¬è®¡ç®—
            }
            
            logger.info(f"âœ… Generation complete: {len(content)} chars, {usage_stats['total_tokens']} tokens")
            if reasoning_content:
                logger.info(f"ğŸ§  Reasoning: {len(reasoning_content)} chars")
            
            # JSON è§£æï¼ˆå¦‚æœéœ€è¦ï¼‰
            result = content
            if response_format == "json":
                try:
                    # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
                    json_str = content
                    if "```json" in content:
                        json_str = content.split("```json")[1].split("```")[0].strip()
                    elif "```" in content:
                        json_str = content.split("```")[1].split("```")[0].strip()
                    result = json.loads(json_str)
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to parse JSON, returning raw content: {e}")
                    result = content
            
            return {
                "content": result,
                "thinking": reasoning_content if return_thinking else "",
                "usage": usage_stats
            }
        
        except Exception as e:
            logger.error(f"âŒ Kimi generation error: {e}")
            raise
    
    async def generate_stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 1.0,  # âš¡âš¡âš¡ å‚ç…§åœ¨çº¿ç‰ˆï¼š1.0æœ€å¤§åŒ–é€Ÿåº¦
        max_tokens: int = 131072,  # âš¡âš¡âš¡ å‚ç…§åœ¨çº¿ç‰ˆï¼š131072
        thinking_budget: Optional[int] = None,
        return_thinking: bool = True,
        buffer_size: int = 1  # âš¡âš¡âš¡âš¡ æé™ä¼˜åŒ–ï¼šæ¯ä¸ªå­—ç¬¦ç«‹å³å‘é€
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆæµå¼ + ä¼˜åŒ–ç¼“å†²ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°ï¼ˆæ€»è¾“å‡ºé™åˆ¶ï¼‰
            thinking_budget: Thinking é¢„ç®—ï¼ˆå¦‚æœè®¾ç½®ï¼Œä¼šè¦†ç›– max_tokensï¼‰
            return_thinking: æ˜¯å¦è¿”å› thinking
            buffer_size: ç¼“å†²åŒºå¤§å°ï¼ˆé»˜è®¤1å­—ç¬¦ï¼Œæé™æµå¼ï¼‰
        
        Yields:
            Dict: {"type": "thinking|content|done|error", ...}
        """
        model_to_use = model or self.model
        
        # âš¡âš¡âš¡ çœŸæ­£çš„ Token æ§åˆ¶ï¼šæ ¹æ® thinking_budget åŠ¨æ€è°ƒæ•´ max_tokens
        # Thinking æ¨¡å‹çš„è¾“å‡º = thinking_content + actual_content
        # 
        # ç­–ç•¥ï¼š
        # - thinking_budget å° (64) â†’ å¿«é€Ÿæ€è€ƒï¼Œé€‚åˆç®€å•ä»»åŠ¡
        # - content_budget ä¸­ç­‰ (3000-4000) â†’ ç¡®ä¿è¾“å‡ºè´¨é‡ä¸å—å½±å“
        # 
        # å®æµ‹æ•°æ®ï¼š
        # - Explain Skill (ç®€å•æ¦‚å¿µ): thinking ~200 tokens, content ~1500 tokens
        # - Quiz (3é¢˜): thinking ~150 tokens, content ~1200 tokens
        # - Flashcard (5å¼ ): thinking ~100 tokens, content ~800 tokens
        if thinking_budget:
            # æ ¹æ® thinking_budget æ™ºèƒ½åˆ†é… content budget
            if thinking_budget <= 64:
                # æé€Ÿæ¨¡å¼ï¼šé€‚åˆç®€å•ä»»åŠ¡
                content_budget = 3500  # ç¡®ä¿è¾“å‡ºå®Œæ•´
            elif thinking_budget <= 128:
                # æ ‡å‡†æ¨¡å¼ï¼šé€‚åˆä¸­ç­‰ä»»åŠ¡
                content_budget = 4000
            else:
                # æ·±åº¦æ¨¡å¼ï¼šé€‚åˆå¤æ‚ä»»åŠ¡
                content_budget = 5000
            
            actual_max_tokens = thinking_budget + content_budget
            logger.info(f"âš¡ Token Budget: thinking={thinking_budget}, content={content_budget}, total={actual_max_tokens}")
        else:
            actual_max_tokens = max_tokens
            logger.info(f"âš¡ Using default max_tokens={actual_max_tokens}")
        
        # âš¡âš¡âš¡ æ·»åŠ ç³»ç»Ÿçº§çº¦æŸæ¥çœŸæ­£æ§åˆ¶ thinking é•¿åº¦
        # è¿™æ¯”åœ¨ prompt ä¸­"å»ºè®®"æ›´æœ‰æ•ˆï¼Œå› ä¸ºå®ƒä½œä¸ºç³»ç»ŸæŒ‡ä»¤
        messages = []
        
        if thinking_budget and thinking_budget <= 128:
            # å¯¹äºå° thinking_budgetï¼Œæ·»åŠ å¼ºåˆ¶çš„ç³»ç»Ÿçº¦æŸ
            system_constraint = (
                f"CRITICAL: Strict {thinking_budget}-token thinking limit. "
                f"Be EXTREMELY concise - 2-4 sentences MAX. "
                f"Skip verbose reasoning. Focus only on core logic."
            )
            messages.append({"role": "system", "content": system_constraint})
        
        messages.append({"role": "user", "content": prompt})
        
        logger.info(f"ğŸŒŠ Starting streaming: model={model_to_use}, max_tokens={actual_max_tokens}, thinking_budget={thinking_budget}")
        
        # ç´¯åŠ å™¨
        content_accumulated = []
        reasoning_accumulated = []
        
        # ğŸ†• ç¼“å†²åŒºï¼ˆå‡å°‘ç¢ç‰‡åŒ–ï¼‰
        content_buffer = []
        reasoning_buffer = []
        
        try:
            # Kimi æµå¼ APIï¼ˆä½¿ç”¨OpenAIå…¼å®¹å‚æ•°ï¼‰
            stream = await self.async_client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=actual_max_tokens,  # âš¡âš¡âš¡ ä½¿ç”¨å®é™…è®¡ç®—çš„ max_tokens
                top_p=1.0,  # âš¡ æ§åˆ¶é‡‡æ ·èŒƒå›´
                presence_penalty=0.0,  # âš¡ æ— é‡å¤æƒ©ç½š
                frequency_penalty=0.0,  # âš¡ æ— é¢‘ç‡æƒ©ç½š
                stream=True
                # âš ï¸ æ³¨æ„ï¼štop_kä¸è¢«OpenAI APIæ”¯æŒï¼Œå·²ç§»é™¤
            )
            
            async for chunk in stream:
                if not chunk.choices:
                    continue
                
                delta = chunk.choices[0].delta
                
                # æå– reasoning_contentï¼ˆKimi çš„ thinkingï¼‰
                reasoning_chunk = getattr(delta, 'reasoning_content', None)
                if reasoning_chunk and isinstance(reasoning_chunk, str):
                    reasoning_accumulated.append(reasoning_chunk)
                    
                    # ğŸ”¥ äºŒæ¬¡åˆ†å—ï¼šç¡®ä¿thinkingä¹Ÿæ˜¯æµå¼çš„
                    chunk_size = 5  # æ¯5ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªæµå¼å•ä½
                    for i in range(0, len(reasoning_chunk), chunk_size):
                        mini_chunk = reasoning_chunk[i:i+chunk_size]
                        reasoning_buffer.append(mini_chunk)
                        
                        # ç«‹å³å‘é€
                        buffered_text = "".join(reasoning_buffer)
                        if len(buffered_text) >= buffer_size:
                            # logger.info(f"ğŸ§  Thinking stream: {len(buffered_text)} chars")
                            yield {
                                "type": "thinking",
                                "text": buffered_text,
                                "accumulated": "".join(reasoning_accumulated)
                            }
                            reasoning_buffer = []
                
                # æå– content
                content_chunk = delta.content
                if content_chunk and isinstance(content_chunk, str):
                    content_accumulated.append(content_chunk)
                    
                    # ğŸ”¥ äºŒæ¬¡åˆ†å—ï¼šå¦‚æœAPIè¿”å›çš„chunkå¤ªå¤§ï¼Œæ‹†åˆ†æˆå°å—æµå¼å‘é€
                    # è¿™ç¡®ä¿äº†å³ä½¿APIä¸€æ¬¡è¿”å›å¤§å—å†…å®¹ï¼Œç”¨æˆ·ä¹Ÿèƒ½çœ‹åˆ°æµå¼æ•ˆæœ
                    chunk_size = 5  # æ¯5ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªæµå¼å•ä½
                    for i in range(0, len(content_chunk), chunk_size):
                        mini_chunk = content_chunk[i:i+chunk_size]
                        content_buffer.append(mini_chunk)
                        
                        # ç«‹å³å‘é€ï¼ˆbuffer_size=1æ„å‘³ç€ä¸å†ç´¯ç§¯ï¼‰
                        buffered_text = "".join(content_buffer)
                        if len(buffered_text) >= buffer_size:
                            # logger.info(f"ğŸ“ Content stream: {len(buffered_text)} chars")
                            yield {
                                "type": "content",
                                "text": buffered_text,
                                "accumulated": "".join(content_accumulated)
                            }
                            content_buffer = []
            
            # ğŸ†• å‘é€å‰©ä½™ç¼“å†²åŒºå†…å®¹
            if reasoning_buffer:
                buffered_text = "".join(reasoning_buffer)
                # logger.info(f"ğŸ§  Reasoning final flush: {len(buffered_text)} chars")
                yield {
                    "type": "thinking",
                    "text": buffered_text,
                    "accumulated": "".join(reasoning_accumulated)
                }
            
            if content_buffer:
                buffered_text = "".join(content_buffer)
                # logger.info(f"ğŸ“ Content final flush: {len(buffered_text)} chars")
                yield {
                    "type": "content",
                    "text": buffered_text,
                    "accumulated": "".join(content_accumulated)
                }
            
            # å®Œæˆ
            full_thinking = "".join(reasoning_accumulated)
            full_content = "".join(content_accumulated)
            
            logger.info(f"âœ… Streaming generation complete")
            logger.info(f"ğŸ“Š Final content: {len(full_content)} chars")
            logger.info(f"ğŸ§  Final reasoning: {len(full_thinking)} chars")
            
            # ğŸ”¥ ä¸åœ¨è¿™é‡Œå‘é€doneäº‹ä»¶ï¼
            # doneäº‹ä»¶åº”è¯¥ç”±skill_orchestratorå‘é€ï¼ŒåŒ…å«è§£æåçš„content
            # è¿™é‡Œåªæ˜¯æ ‡è®°æµå¼ç»“æŸ
            logger.info(f"ğŸ Stream ended (orchestrator will send done event)")
        
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Streaming generation error: {e}")
            
            # 503 é”™è¯¯å¤„ç†
            if "503" in error_msg or "overloaded" in error_msg.lower():
                yield {
                    "type": "error",
                    "error": "AIæœåŠ¡æš‚æ—¶è¿‡è½½ï¼Œè¯·ç­‰å¾…å‡ ç§’åé‡è¯• (503 Service Overloaded)",
                    "code": 503
                }
            else:
                yield {
                    "type": "error",
                    "error": error_msg,
                    "code": 500
                }
    
    async def generate_json(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 0.6,
        max_tokens: int = 4096
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆ JSON æ ¼å¼å†…å®¹
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            è§£æåçš„ JSON å¯¹è±¡
        """
        result = await self.generate(
            prompt=prompt,
            model=model,
            response_format="json",
            temperature=temperature,
            max_tokens=max_tokens,
            return_thinking=False
        )
        return result["content"]

