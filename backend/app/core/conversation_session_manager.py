"""
Conversation Session Manager - å¯¹è¯ Session ç®¡ç†å™¨

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ğŸ†• æ™ºèƒ½é•¿åº¦æ£€æµ‹ï¼Œè‡ªåŠ¨åˆ†å‰² MDï¼ˆæ›¿ä»£æ—¶é—´ cooldownï¼‰
2. ğŸ†• ä¸Šä¸‹æ–‡ç»§æ‰¿ï¼ˆsummary + ä¸»é¢˜ + artifactsï¼‰
3. ç”Ÿæˆ Markdown æ ¼å¼çš„å¯¹è¯è®°å½•
4. åµŒå…¥ JSON ç»“æ„åŒ–æ•°æ®
5. Session äº’è”ï¼ˆè·¨ session å¼•ç”¨ï¼‰
6. S3 åŒæ­¥
7. ğŸ†• æ™ºèƒ½å‹ç¼©ï¼ˆä¿ç•™æœ€è¿‘å¯¹è¯ï¼Œå‹ç¼©æ—§å¯¹è¯ï¼‰
"""

import os
import json
import logging
import re
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class ConversationSessionManager:
    """
    ç®¡ç†è¿ç»­å¯¹è¯ session
    
    ğŸ†• æ ¸å¿ƒé€»è¾‘ï¼ˆæ™ºèƒ½é•¿åº¦åˆ†å‰²ï¼‰ï¼š
    - æ£€æµ‹ MD æ–‡ä»¶é•¿åº¦ï¼ˆå­—ç¬¦æ•° or token ä¼°ç®—ï¼‰
    - è½¯é™åˆ¶ï¼ˆ50K charsï¼‰ï¼šå¼€å§‹å‹ç¼©æ—§å¯¹è¯
    - ç¡¬é™åˆ¶ï¼ˆ100K charsï¼‰ï¼šå¼ºåˆ¶åˆ›å»ºæ–° MD
    - æ–° MD ç»§æ‰¿ä¸Šä¸‹æ–‡ï¼ˆsummary + ä¸»é¢˜ + artifactsï¼‰
    - å…œåº•ï¼šé•¿æ—¶é—´ä¸æ´»åŠ¨ï¼ˆ1 å°æ—¶ï¼‰ä¹Ÿæ–°å»º MD
    """
    
    # ğŸ†• é•¿åº¦é˜ˆå€¼ç­–ç•¥ï¼ˆç”¨äºè§¦å‘å‹ç¼©ï¼Œä¸å†å¼ºåˆ¶åˆ†å·ï¼‰
    SOFT_LIMIT_CHARS = 50_000   # 50KB: å¼€å§‹å‹ç¼©æ—§å¯¹è¯
    HARD_LIMIT_CHARS = 100_000  # 100KB: å¼ºåˆ¶å‹ç¼©ï¼ˆä¸åˆ†å·ï¼‰
    MAX_LINES = 2000            # 2000è¡Œ: å¤‡é€‰é˜ˆå€¼
    TOKEN_ESTIMATION_RATIO = 0.4  # 1 char â‰ˆ 0.4 tokensï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
    
    # ğŸ†• Turn æ•°é‡é˜ˆå€¼ï¼ˆè§¦å‘å‹ç¼©ï¼Œä¸åˆ†å·ï¼‰
    COMPRESS_TRIGGER_TURNS = 30   # 30 turns: è§¦å‘å‹ç¼©æ£€æŸ¥
    KEEP_RECENT_TURNS = 10        # ä¿ç•™æœ€è¿‘ 10 è½®å®Œæ•´å¯¹è¯ï¼Œå…¶ä½™å‹ç¼©ä¸º summary
    
    # ğŸ†• åˆ†å·æ¡ä»¶ï¼ˆä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æˆ–ç‰¹æ®Šæƒ…å†µï¼‰
    # ä¸å†åŸºäº turn æ•°å¼ºåˆ¶åˆ†å·
    
    # å…œåº•ï¼šæ—¶é—´ cooldownï¼ˆé•¿æ—¶é—´ä¸æ´»åŠ¨æ‰è€ƒè™‘æ–°å»ºï¼‰
    INACTIVITY_TIMEOUT = 3600  # 1 å°æ—¶ï¼ˆç§’ï¼‰
    
    # ğŸ†• æœåŠ¡å¯åŠ¨æ ‡å¿—ï¼ˆå…¨å±€ï¼Œç”¨äºæ£€æµ‹æœåŠ¡é‡å¯ï¼‰
    _server_start_id: str = None
    
    def __init__(
        self,
        user_id: str,
        storage_path: str,
        s3_manager: Optional[Any] = None,
        server_start_id: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– Session ç®¡ç†å™¨
        
        Args:
            user_id: ç”¨æˆ· ID
            storage_path: æœ¬åœ°å­˜å‚¨è·¯å¾„ï¼ˆå¦‚ï¼šbackend/artifacts/user_kimi/ï¼‰
            s3_manager: S3 å­˜å‚¨ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            server_start_id: æœåŠ¡å¯åŠ¨ IDï¼ˆç”¨äºæ£€æµ‹æœåŠ¡é‡å¯ï¼‰
        """
        self.user_id = user_id
        self.storage_path = Path(storage_path)
        self.s3_manager = s3_manager
        
        # ğŸ†• è®°å½•å½“å‰æœåŠ¡å¯åŠ¨ ID
        self._current_server_start_id = server_start_id
        
        # ğŸ”’ å¹¶å‘å®‰å…¨ï¼šå†™å…¥é”ï¼Œé˜²æ­¢åŒä¸€ç”¨æˆ·çš„å¹¶å‘è¯·æ±‚å¯¼è‡´æ–‡ä»¶æŸå
        self._write_lock = asyncio.Lock()
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # å½“å‰ session çŠ¶æ€
        self.current_session_id: Optional[str] = None
        self.last_activity_time: Optional[datetime] = None
        self.current_session_file: Optional[Path] = None
        self.turn_counter: int = 0
        
        # Session å…ƒæ•°æ®
        self.session_metadata: Dict[str, Any] = {}
        
        # ğŸ†• å°è¯•æ¢å¤æœ€æ–° sessionï¼ˆå¦‚æœæœªè¶…è¿‡é˜ˆå€¼ï¼‰
        self._try_restore_latest_session()
        
        logger.info(f"âœ… ConversationSessionManager initialized for user: {user_id}")
    
    def _try_restore_latest_session(self):
        """
        ğŸ†• å°è¯•æ¢å¤æœ€æ–°çš„ sessionï¼ˆå¦‚æœæœªè¶…è¿‡é˜ˆå€¼ï¼‰
        
        é€»è¾‘ï¼š
        1. æŸ¥æ‰¾æœ€æ–°çš„ session MD æ–‡ä»¶
        2. æ£€æŸ¥æ–‡ä»¶å¤§å°/è¡Œæ•°æ˜¯å¦åœ¨é˜ˆå€¼ä»¥å†…
        3. å¦‚æœåœ¨é˜ˆå€¼ä»¥å†…ï¼Œæ¢å¤è¯¥ session çŠ¶æ€
        4. å¦åˆ™ï¼Œä¿æŒ current_session_id = Noneï¼ˆä¼šåœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶æ–°å»ºï¼‰
        """
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„ session æ–‡ä»¶
            session_files = sorted(
                self.storage_path.glob("session_*.md"),
                key=lambda f: f.stat().st_mtime,
                reverse=True
            )
            
            if not session_files:
                logger.info("ğŸ“ No existing sessions found, will create new one")
                return
            
            latest_file = session_files[0]
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = latest_file.stat().st_size
            file_chars = len(latest_file.read_text(encoding='utf-8'))
            file_lines = sum(1 for _ in open(latest_file, 'r', encoding='utf-8'))
            
            logger.info(
                f"ğŸ“Š Latest session: {latest_file.name} | "
                f"Size: {file_size/1024:.1f}KB | Chars: {file_chars:,} | Lines: {file_lines}"
            )
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
            if file_chars >= self.HARD_LIMIT_CHARS:
                logger.info(f"ğŸ“ Latest session exceeds hard limit ({file_chars:,} >= {self.HARD_LIMIT_CHARS:,}), will create new")
                return
            
            if file_lines >= self.MAX_LINES:
                logger.info(f"ğŸ“ Latest session exceeds max lines ({file_lines} >= {self.MAX_LINES}), will create new")
                return
            
            # ğŸ†• æ£€æŸ¥æœåŠ¡å¯åŠ¨ IDï¼ˆå¦‚æœ metadata ä¸­è®°å½•äº†ä¸åŒçš„ server_start_idï¼Œè¯´æ˜æœåŠ¡é‡å¯è¿‡ï¼‰
            metadata_file = latest_file.with_suffix('.md').with_name(
                latest_file.stem + '_metadata.json'
            )
            if metadata_file.exists():
                import json
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    old_server_id = metadata.get('server_start_id')
                    if old_server_id and self._current_server_start_id and old_server_id != self._current_server_start_id:
                        logger.info(f"ğŸ”„ Server restarted (old: {old_server_id[:8]}..., new: {self._current_server_start_id[:8]}...), will create new session")
                        return
                except Exception as e:
                    logger.warning(f"âš ï¸  Failed to read metadata: {e}")
            
            # âœ… æ¢å¤ session
            session_id = latest_file.stem  # å¦‚ "session_20251125_082222"
            self.current_session_id = session_id
            self.current_session_file = latest_file
            self.last_activity_time = datetime.fromtimestamp(latest_file.stat().st_mtime)
            
            # ä» metadata æ¢å¤ turn counter
            if metadata_file.exists():
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    self.turn_counter = metadata.get('total_turns', 0)
                    self.session_metadata = metadata
                except Exception as e:
                    logger.warning(f"âš ï¸  Failed to restore metadata: {e}")
                    self.turn_counter = self._count_turns_from_file(latest_file)
            else:
                self.turn_counter = self._count_turns_from_file(latest_file)
            
            logger.info(
                f"â™»ï¸  Restored session: {session_id} | "
                f"Turns: {self.turn_counter} | Chars: {file_chars:,}/{self.HARD_LIMIT_CHARS:,}"
            )
            
        except Exception as e:
            logger.error(f"âŒ Failed to restore session: {e}")
            # å¤±è´¥æ—¶ä¿æŒ Noneï¼Œä¸‹æ¬¡è¯·æ±‚ä¼šæ–°å»º
    
    def _count_turns_from_file(self, file_path: Path) -> int:
        """ä» MD æ–‡ä»¶ç»Ÿè®¡ turn æ•°é‡"""
        try:
            content = file_path.read_text(encoding='utf-8')
            # ç»Ÿè®¡ "## Turn" çš„æ•°é‡
            return content.count("## Turn ")
        except Exception:
            return 0
    
    async def start_or_continue_session(
        self,
        user_message: str,
        timestamp: Optional[datetime] = None,
        session_id: Optional[str] = None
    ) -> str:
        """
        ğŸ†• å¼€å§‹æˆ–ç»§ç»­ sessionï¼ˆæ”¯æŒå¼ºåˆ¶æŒ‡å®š session_idï¼‰
        
        é€»è¾‘ï¼š
        1. å¦‚æœä¼ å…¥ session_idï¼Œå¼ºåˆ¶ä½¿ç”¨è¯¥ sessionï¼ˆç”¨äº API è°ƒç”¨ï¼‰
        2. å¦åˆ™è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦æ–° session
        3. å‹ç¼©ç­–ç•¥æ§åˆ¶æ–‡ä»¶å¤§å°
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºæ£€æµ‹å…³è”å’Œæ–­ç‚¹ï¼‰
            timestamp: å½“å‰æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º nowï¼‰
            session_id: å¼ºåˆ¶ä½¿ç”¨çš„ session_idï¼ˆå¯é€‰ï¼Œæ¥è‡ª API è°ƒç”¨ï¼‰
        
        Returns:
            session_id
        """
        now = timestamp or datetime.now()
        
        # ğŸ†• å¦‚æœä¼ å…¥äº†æ˜ç¡®çš„ session_idï¼Œå¼ºåˆ¶ä½¿ç”¨è¯¥ session
        if session_id:
            await self._force_use_session(session_id, now)
            logger.info(f"ğŸ“Œ Forced to use session: {session_id}")
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–° session
        elif self._should_start_new_session(now, user_message):
            # åˆ›å»ºæ–° sessionï¼ˆå¸¦ä¸Šä¸‹æ–‡ç»§æ‰¿ï¼‰
            await self._start_new_session(now, user_message)
            logger.info(f"ğŸ†• Started new session: {self.current_session_id}")
        else:
            # ç»§ç»­å½“å‰ session
            logger.info(f"â™»ï¸  Continuing session: {self.current_session_id}")
        
        # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
        self.last_activity_time = now
        
        return self.current_session_id
    
    async def _force_use_session(self, session_id: str, timestamp: datetime):
        """
        ğŸ†• å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šçš„ sessionï¼ˆç”¨äº API ä¼ å…¥çš„ session_idï¼‰
        
        é€»è¾‘ï¼š
        1. å¦‚æœ session æ–‡ä»¶å­˜åœ¨ï¼Œæ£€æŸ¥ server_start_id
        2. å¦‚æœæœåŠ¡é‡å¯äº†ï¼Œå½’æ¡£æ—§ session å¹¶é‡æ–°åˆ›å»º
        3. å¦åˆ™åŠ è½½å¹¶ç»§ç»­
        4. å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„ï¼ˆä½¿ç”¨ä¼ å…¥çš„ session_idï¼‰
        """
        session_file = self.storage_path / f"{session_id}.md"
        metadata_file = self.storage_path / f"{session_id}_metadata.json"
        
        if session_file.exists():
            # ğŸ†• æ£€æŸ¥ server_start_idï¼ˆæœåŠ¡é‡å¯æ£€æµ‹ï¼‰
            should_archive = False
            old_server_id = None
            
            if metadata_file.exists():
                try:
                    import json
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        old_metadata = json.load(f)
                    old_server_id = old_metadata.get('server_start_id')
                    
                    # ğŸ”§ æ£€æŸ¥æœåŠ¡æ˜¯å¦é‡å¯
                    if old_server_id and self._current_server_start_id and old_server_id != self._current_server_start_id:
                        logger.info(f"ğŸ”„ Server restarted (old: {old_server_id[:8]}..., new: {self._current_server_start_id[:8]}...), archiving old session")
                        should_archive = True
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to check server_start_id: {e}")
            
            if should_archive:
                # ğŸ†• å½’æ¡£æ—§ session
                archive_timestamp = timestamp.strftime("%Y%m%d_%H%M%S")
                archive_file = self.storage_path / f"{session_id}_archived_{archive_timestamp}.md"
                
                try:
                    # ç§»åŠ¨æ—§çš„ MD æ–‡ä»¶åˆ°å½’æ¡£
                    import shutil
                    shutil.move(str(session_file), str(archive_file))
                    logger.info(f"ğŸ“¦ Archived old session to: {archive_file.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to archive old session: {e}")
                
                # åˆ›å»ºæ–°çš„ sessionï¼ˆé‡ç½® turn_counterï¼‰
                self.current_session_id = session_id
                self.current_session_file = session_file
                self.turn_counter = 0
                
                # åˆå§‹åŒ–æ–°çš„ metadata
                self.session_metadata = {
                    "session_id": session_id,
                    "user_id": self.user_id,
                    "start_time": timestamp.isoformat(),
                    "last_updated": timestamp.isoformat(),
                    "status": "active",
                    "total_turns": 0,
                    "inherited_context": {},
                    "previous_session_id": None,
                    "topics": [],
                    "last_topic": None,
                    "skills_used": {},
                    "artifacts_generated": [],
                    "server_start_id": self._current_server_start_id
                }
                
                # åˆ›å»ºæ–°çš„ MD æ–‡ä»¶å¤´
                await self._write_session_header_with_inheritance({})
                logger.info(f"ğŸ“ Created new session after server restart: {session_id}")
                return
            
            # æ­£å¸¸åŠ è½½ç°æœ‰ session
            self.current_session_id = session_id
            self.current_session_file = session_file
            
            # åŠ è½½ metadata
            if metadata_file.exists():
                try:
                    import json
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        self.session_metadata = json.load(f)
                    
                    # ğŸ”§ ä½¿ç”¨ MD æ–‡ä»¶ä¸­å®é™…çš„ turn æ•°ï¼ˆæ›´å¯é ï¼‰
                    actual_turns = self._count_turns_from_file(session_file)
                    metadata_turns = self.session_metadata.get('total_turns', 0)
                    
                    # å¦‚æœä¸ä¸€è‡´ï¼Œä»¥ MD æ–‡ä»¶ä¸ºå‡†
                    if actual_turns != metadata_turns:
                        logger.warning(f"âš ï¸ Turn count mismatch: MD={actual_turns}, metadata={metadata_turns}, using MD count")
                        self.turn_counter = actual_turns
                        self.session_metadata['total_turns'] = actual_turns
                    else:
                        self.turn_counter = metadata_turns
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to load metadata for {session_id}: {e}")
                    self.turn_counter = self._count_turns_from_file(session_file)
            else:
                self.turn_counter = self._count_turns_from_file(session_file)
            
            logger.info(f"ğŸ“‚ Loaded existing session: {session_id} (turns: {self.turn_counter})")
        else:
            # åˆ›å»ºæ–° sessionï¼ˆä½¿ç”¨ä¼ å…¥çš„ session_idï¼‰
            self.current_session_id = session_id
            self.current_session_file = session_file
            self.turn_counter = 0
            
            # åˆå§‹åŒ– metadata
            self.session_metadata = {
                "session_id": session_id,
                "user_id": self.user_id,
                "start_time": timestamp.isoformat(),
                "last_updated": timestamp.isoformat(),
                "status": "active",
                "total_turns": 0,
                "inherited_context": {},
                "previous_session_id": None,
                "topics": [],
                "last_topic": None,
                "skills_used": {},
                "artifacts_generated": [],
                "server_start_id": self._current_server_start_id
            }
            
            # åˆ›å»º MD æ–‡ä»¶å¤´
            await self._write_session_header_with_inheritance({})
            
            logger.info(f"ğŸ“ Created new session with forced ID: {session_id}")
    
    def _should_start_new_session(self, now: datetime, user_message: str) -> bool:
        """
        ğŸ†• åˆ¤æ–­æ˜¯å¦éœ€è¦æ–° sessionï¼ˆä¿å®ˆç­–ç•¥ - ä¼˜å…ˆå‹ç¼©è€Œéåˆ†å·ï¼‰
        
        ä¼˜å…ˆçº§ï¼š
        1. æ²¡æœ‰ session â†’ åˆ›å»º
        2. ç”¨æˆ·æ˜ç¡®è¦æ±‚æ–° sessionï¼ˆ"æ–°å¯¹è¯", "é‡æ–°å¼€å§‹"ï¼‰â†’ åˆ›å»º
        3. é•¿æ—¶é—´ä¸æ´»åŠ¨ï¼ˆ1 å°æ—¶ï¼‰â†’ åˆ›å»º
        
        æ³¨æ„ï¼šä¸å†åŸºäºæ–‡ä»¶å¤§å°/turn æ•°å¼ºåˆ¶åˆ†å·ï¼Œæ”¹ä¸ºå‹ç¼©ç­–ç•¥
        
        Returns:
            True if should start new session
        """
        # 1. å¦‚æœæ²¡æœ‰ sessionï¼Œç›´æ¥åˆ›å»º
        if not self.current_session_id:
            logger.debug("ğŸ“ No existing session, creating new one")
            return True
        
        # 2. ç”¨æˆ·æ˜ç¡®è¦æ±‚æ–° session
        if self._user_requests_new_session(user_message):
            logger.info("ğŸ†• User explicitly requested new session")
            return True
        
        # 3. å…œåº•ï¼šé•¿æ—¶é—´ä¸æ´»åŠ¨ï¼ˆ1 å°æ—¶ï¼‰
        if self.last_activity_time:
            time_diff = (now - self.last_activity_time).total_seconds()
            if time_diff > self.INACTIVITY_TIMEOUT:
                logger.info(
                    f"â° Long inactivity: {time_diff:.1f}s > {self.INACTIVITY_TIMEOUT}s"
                )
                return True
        
        # ğŸ†• å…¶ä»–æƒ…å†µç»§ç»­å½“å‰ sessionï¼Œé€šè¿‡å‹ç¼©æ§åˆ¶å¤§å°
        return False
    
    def _user_requests_new_session(self, user_message: str) -> bool:
        """æ£€æµ‹ç”¨æˆ·æ˜¯å¦æ˜ç¡®è¦æ±‚æ–°å»º session"""
        new_session_keywords = [
            "æ–°å¯¹è¯", "æ–°ä¼šè¯", "é‡æ–°å¼€å§‹", "æ¸…é™¤è®°å¿†", "å¿˜æ‰ä¹‹å‰",
            "new session", "new conversation", "start fresh", "forget everything",
            "é‡ç½®", "ä»å¤´å¼€å§‹"
        ]
        message_lower = user_message.lower()
        return any(kw in message_lower for kw in new_session_keywords)
    
    def _get_file_size_chars(self) -> int:
        """è®¡ç®— MD æ–‡ä»¶å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰"""
        if not self.current_session_file or not self.current_session_file.exists():
            return 0
        try:
            content = self.current_session_file.read_text(encoding='utf-8')
            return len(content)
        except Exception as e:
            logger.error(f"âŒ Failed to read MD file size: {e}")
            return 0
    
    def _estimate_tokens_from_chars(self, char_count: int) -> int:
        """
        ä¼°ç®— token æ•°é‡
        
        è§„åˆ™ï¼š1 char â‰ˆ 0.4 tokensï¼ˆä¸­è‹±æ–‡æ··åˆå¹³å‡å€¼ï¼‰
        - çº¯è‹±æ–‡ï¼š1 char â‰ˆ 0.25 tokens
        - çº¯ä¸­æ–‡ï¼š1 char â‰ˆ 0.6 tokens
        - æ··åˆï¼š1 char â‰ˆ 0.4 tokens
        """
        return int(char_count * self.TOKEN_ESTIMATION_RATIO)
    
    def _is_natural_breakpoint(self, user_message: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦æ˜¯è‡ªç„¶æ–­ç‚¹ï¼ˆæ–°ä¸»é¢˜æˆ–æ€»ç»“è¯·æ±‚ï¼‰
        
        è§¦å‘æ¡ä»¶ï¼š
        - ç”¨æˆ·æ˜ç¡®åˆ‡æ¢ä¸»é¢˜ï¼ˆ"æˆ‘æƒ³å­¦ä¹  XXX", "æ¢ä¸ªè¯é¢˜"ï¼‰
        - ç”¨æˆ·è¯·æ±‚ summaryï¼ˆ"æ€»ç»“ä¸€ä¸‹", "å›é¡¾ä¸€ä¸‹"ï¼‰
        - ç”¨æˆ·æ˜ç¡®ç»“æŸå½“å‰è¯é¢˜ï¼ˆ"ç†è§£äº†", "æ‡‚äº†", "è°¢è°¢"ï¼‰
        
        Returns:
            True if this is a natural breakpoint
        """
        breakpoint_keywords = [
            # åˆ‡æ¢ä¸»é¢˜
            "æ¢ä¸ªè¯é¢˜", "å­¦ä¹ æ–°çš„", "å¼€å§‹æ–°çš„", "åˆ‡æ¢åˆ°", "è®²è®²å…¶ä»–",
            "new topic", "switch to", "let's talk about",
            # æ€»ç»“è¯·æ±‚
            "æ€»ç»“ä¸€ä¸‹", "å›é¡¾ä¸€ä¸‹", "æ¢³ç†ä¸€ä¸‹", "å½’çº³ä¸€ä¸‹",
            "summarize", "recap", "review",
            # ç»“æŸå½“å‰è¯é¢˜
            "ç†è§£äº†", "æ‡‚äº†", "æ˜ç™½äº†", "æ¸…æ¥šäº†", "è°¢è°¢",
            "got it", "understand", "thank you",
            # æ˜ç¡®çš„æ–°å­¦ä¹ è¯·æ±‚
            "æˆ‘æƒ³å­¦", "æ•™æˆ‘", "ç»™æˆ‘è®²è®²", "ä»‹ç»ä¸€ä¸‹",
            "teach me", "explain", "tell me about"
        ]
        
        message_lower = user_message.lower()
        return any(kw in message_lower for kw in breakpoint_keywords)
    
    async def _start_new_session(self, timestamp: datetime, user_message: str):
        """
        ğŸ†• åˆ›å»ºæ–° sessionï¼ˆå¸¦ä¸Šä¸‹æ–‡ç»§æ‰¿ï¼‰
        
        Args:
            timestamp: åˆ›å»ºæ—¶é—´
            user_message: é¦–æ¡æ¶ˆæ¯ï¼ˆç”¨äºæ£€æµ‹å…³è”ï¼‰
        """
        # ğŸ†• ä»æ—§ session åˆ›å»ºç»§æ‰¿ä¸Šä¸‹æ–‡
        inherited_context = await self._create_inherited_context()
        
        # ç”Ÿæˆæ–° session ID
        self.current_session_id = self._generate_session_id(timestamp)
        
        # åˆ›å»º MD æ–‡ä»¶è·¯å¾„
        self.current_session_file = self.storage_path / f"{self.current_session_id}.md"
        
        # é‡ç½® turn è®¡æ•°å™¨
        self.turn_counter = 0
        
        # åˆå§‹åŒ– session å…ƒæ•°æ®ï¼ˆåŒ…å«ç»§æ‰¿ä¿¡æ¯ï¼‰
        previous_session_id = self.session_metadata.get("session_id") if self.session_metadata else None
        
        self.session_metadata = {
            "session_id": self.current_session_id,
            "user_id": self.user_id,
            "start_time": timestamp.isoformat(),
            "last_updated": timestamp.isoformat(),
            "status": "active",
            "total_turns": 0,
            "inherited_context": inherited_context,  # ğŸ†• ç»§æ‰¿çš„å®Œæ•´ä¸Šä¸‹æ–‡
            "previous_session_id": previous_session_id,  # ğŸ†• çˆ¶ session ID
            "topics": inherited_context.get("key_topics", []),  # ğŸ†• ç»§æ‰¿ä¸»é¢˜
            "last_topic": inherited_context.get("key_topics", [None])[-1] if inherited_context.get("key_topics") else None,
            "skills_used": {},
            "artifacts_generated": inherited_context.get("last_artifacts", []),  # ğŸ†• ç»§æ‰¿æœ€åçš„ artifacts
            "server_start_id": self._current_server_start_id  # ğŸ†• æœåŠ¡å¯åŠ¨ IDï¼ˆç”¨äºæ£€æµ‹é‡å¯ï¼‰
        }
        
        # ğŸ”— æ£€æŸ¥æ˜¯å¦ä¸æ—§ sessions ç›¸å…³ï¼ˆè¯­ä¹‰æœç´¢ï¼‰
        related_sessions = await self._find_related_sessions(user_message)
        if related_sessions:
            self.session_metadata["related_sessions"] = related_sessions
            logger.info(f"ğŸ”— Found {len(related_sessions)} related sessions")
        
        # åˆ›å»º MD æ–‡ä»¶å¤´éƒ¨ï¼ˆåŒ…å«ç»§æ‰¿ä¿¡æ¯ï¼‰
        await self._write_session_header_with_inheritance(inherited_context)
    
    def _generate_session_id(self, timestamp: datetime) -> str:
        """ç”Ÿæˆ session ID"""
        return f"session_{timestamp.strftime('%Y%m%d_%H%M%S')}"
    
    async def _create_inherited_context(self) -> Dict[str, Any]:
        """
        ğŸ†• åˆ›å»ºç»§æ‰¿ä¸Šä¸‹æ–‡ï¼ˆç»™æ–° session ä½¿ç”¨ï¼‰
        
        ç­–ç•¥ï¼š
        1. ç”Ÿæˆå½“å‰ session çš„ summaryï¼ˆå‹ç¼©ç‰ˆï¼‰
        2. æå–å…³é”®ä¸»é¢˜
        3. æ”¶é›†æœ€åç”Ÿæˆçš„ artifactsï¼ˆå¼•ç”¨ï¼‰
        4. ç”Ÿæˆå»¶ç»­æç¤ºï¼ˆç»™ LLM çš„ä¸Šä¸‹æ–‡ï¼‰
        
        Returns:
            {
                "previous_session_id": "session_xxx",
                "summary": "...",
                "key_topics": [...],
                "last_artifacts": [...],
                "continuation_prompt": "..."
            }
        """
        if not self.session_metadata or not self.current_session_id:
            return {}
        
        try:
            # 1. ç”Ÿæˆ session summaryï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…è°ƒç”¨ LLMï¼‰
            summary = await self._generate_session_summary()
            
            # 2. æå–å…³é”®ä¸»é¢˜
            key_topics = self.session_metadata.get("topics", [])
            if not key_topics and self.session_metadata.get("last_topic"):
                key_topics = [self.session_metadata["last_topic"]]
            
            # 3. æ”¶é›†æœ€åçš„ artifacts
            last_artifacts = self.session_metadata.get("artifacts_generated", [])[-3:]  # æœ€å 3 ä¸ª
            
            # 4. ç”Ÿæˆå»¶ç»­æç¤º
            continuation_prompt = self._generate_continuation_prompt(
                key_topics, 
                last_artifacts,
                self.turn_counter
            )
            
            inherited_context = {
                "previous_session_id": self.current_session_id,
                "summary": summary,
                "key_topics": key_topics,
                "last_artifacts": last_artifacts,
                "continuation_prompt": continuation_prompt,
                "total_turns": self.turn_counter
            }
            
            logger.info(
                f"ğŸ“š Created inherited context from {self.current_session_id}: "
                f"{len(key_topics)} topics, {len(last_artifacts)} artifacts, "
                f"{self.turn_counter} turns"
            )
            
            return inherited_context
        
        except Exception as e:
            logger.error(f"âŒ Failed to create inherited context: {e}")
            return {}
    
    async def _generate_session_summary(self) -> str:
        """
        ç”Ÿæˆå½“å‰ session çš„ summaryï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        ç­–ç•¥ï¼ˆä¸è°ƒç”¨ LLMï¼Œä½¿ç”¨è§„åˆ™æå–ï¼‰ï¼š
        1. ç»Ÿè®¡ turns æ•°é‡
        2. æå–ä¸»é¢˜åˆ—è¡¨
        3. æå–ä½¿ç”¨çš„ skills
        4. æå– artifacts ç±»å‹
        
        TODO: åç»­å¯ä»¥è°ƒç”¨ LLM ç”Ÿæˆæ›´é«˜è´¨é‡çš„ summary
        """
        if not self.current_session_file or not self.current_session_file.exists():
            return ""
        
        try:
            # ç®€å•è§„åˆ™æå–
            topics = self.session_metadata.get("topics", [])
            skills_used = self.session_metadata.get("skills_used", {})
            artifacts_count = len(self.session_metadata.get("artifacts_generated", []))
            
            summary_parts = []
            
            # åŸºæœ¬ä¿¡æ¯
            summary_parts.append(
                f"åœ¨ä¹‹å‰çš„å­¦ä¹ ä¸­ï¼Œç”¨æˆ·è¿›è¡Œäº† {self.turn_counter} è½®å¯¹è¯ã€‚"
            )
            
            # ä¸»é¢˜
            if topics:
                topics_str = "ã€".join(topics[:5])  # æœ€å¤šåˆ—ä¸¾ 5 ä¸ª
                summary_parts.append(f"å­¦ä¹ çš„ä¸»é¢˜åŒ…æ‹¬ï¼š{topics_str}ã€‚")
            
            # Skills
            if skills_used:
                skills_list = list(skills_used.keys())[:3]  # æœ€å¤šåˆ—ä¸¾ 3 ä¸ª
                skills_str = "ã€".join(skills_list)
                summary_parts.append(f"ä½¿ç”¨äº†ä»¥ä¸‹æŠ€èƒ½ï¼š{skills_str}ã€‚")
            
            # Artifacts
            if artifacts_count > 0:
                summary_parts.append(f"ç”Ÿæˆäº† {artifacts_count} ä¸ªå­¦ä¹ äº§ç‰©ï¼ˆquizã€notesã€mindmap ç­‰ï¼‰ã€‚")
            
            return " ".join(summary_parts)
        
        except Exception as e:
            logger.error(f"âŒ Failed to generate session summary: {e}")
            return ""
    
    def _generate_continuation_prompt(
        self, 
        key_topics: List[str], 
        last_artifacts: List[Dict[str, Any]],
        total_turns: int
    ) -> str:
        """
        ç”Ÿæˆå»¶ç»­æç¤ºï¼ˆç»™ LLM çš„ä¸Šä¸‹æ–‡ï¼‰
        
        è¿™ä¸ª prompt ä¼šè¢«æ·»åŠ åˆ°æ–° session çš„ system message ä¸­
        """
        prompt_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        prompt_parts.append(
            f"è¿™æ˜¯ç”¨æˆ·çš„å»¶ç»­å­¦ä¹  sessionï¼ˆå…± {total_turns} è½®å¯¹è¯ï¼‰ã€‚"
        )
        
        # ä¸»é¢˜ä¸Šä¸‹æ–‡
        if key_topics:
            topics_str = "ã€".join(key_topics[:3])
            prompt_parts.append(f"ç”¨æˆ·æ­£åœ¨å­¦ä¹ ï¼š{topics_str}ã€‚")
        
        # Artifacts ä¸Šä¸‹æ–‡
        if last_artifacts:
            artifacts_types = [a.get("type", "artifact") for a in last_artifacts]
            artifacts_str = "ã€".join(artifacts_types)
            prompt_parts.append(f"å·²ç”Ÿæˆçš„å­¦ä¹ äº§ç‰©ï¼š{artifacts_str}ã€‚")
        
        prompt_parts.append("è¯·ä¿æŒå­¦ä¹ çš„è¿è´¯æ€§ï¼Œè‡ªç„¶è¡”æ¥ä¹‹å‰çš„å†…å®¹ã€‚")
        
        return " ".join(prompt_parts)
    
    async def _find_related_sessions(
        self,
        user_message: str,
        max_results: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾ç›¸å…³çš„æ—§ sessionsï¼ˆè¯­ä¹‰æœç´¢ï¼‰
        
        ç­–ç•¥ï¼š
        1. æå–ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å…³é”®è¯/ä¸»é¢˜
        2. åœ¨æ—§ sessions ä¸­æœç´¢ç›¸å…³ä¸»é¢˜
        3. è®¡ç®—ç›¸å…³åº¦è¯„åˆ†
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            max_results: æœ€å¤šè¿”å›å‡ ä¸ªç›¸å…³ session
        
        Returns:
            ç›¸å…³ sessions åˆ—è¡¨
        """
        # ğŸ” ç®€å•å®ç°ï¼šåŸºäºä¸»é¢˜åŒ¹é…
        # é«˜çº§å®ç°å¯ä»¥ä½¿ç”¨ embedding è¯­ä¹‰æœç´¢
        
        related = []
        
        try:
            # åˆ—å‡ºæ‰€æœ‰ session MD æ–‡ä»¶
            session_files = list(self.storage_path.glob("session_*.md"))
            
            # æå–ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å…³é”®è¯ï¼ˆç®€å•åˆ†è¯ï¼‰
            keywords = self._extract_keywords(user_message)
            
            for session_file in session_files:
                # è·³è¿‡å½“å‰ session
                if self.current_session_id and session_file.stem == self.current_session_id:
                    continue
                
                # è¯»å– session metadataï¼ˆä»æ–‡ä»¶å¤´éƒ¨æˆ–å•ç‹¬çš„ JSONï¼‰
                metadata = await self._load_session_metadata(session_file)
                
                if not metadata:
                    continue
                
                # è®¡ç®—ç›¸å…³åº¦
                relevance = self._calculate_relevance(keywords, metadata)
                
                if relevance > 0.5:  # é˜ˆå€¼ï¼š50%
                    related.append({
                        "session_id": session_file.stem,
                        "relevance_score": relevance,
                        "topics": metadata.get("topics", []),
                        "start_time": metadata.get("start_time", "")
                    })
            
            # æŒ‰ç›¸å…³åº¦æ’åº
            related.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            return related[:max_results]
        
        except Exception as e:
            logger.error(f"âŒ Failed to find related sessions: {e}")
            return []
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰"""
        # ç§»é™¤å¸¸è§åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'å—', 'å‘¢', 'å•Š', 'å§', 'ç»™æˆ‘', 'å¸®æˆ‘', 'æˆ‘è¦', 'ä»€ä¹ˆæ˜¯', 'æ˜¯ä»€ä¹ˆ'}
        
        # ç®€å•åˆ†è¯ï¼ˆåŸºäºç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        import re
        words = re.findall(r'[\w]+', text)
        
        # è¿‡æ»¤åœç”¨è¯ï¼Œä¿ç•™é•¿åº¦ >= 2 çš„è¯
        keywords = [w for w in words if w not in stopwords and len(w) >= 2]
        
        return keywords[:10]  # æœ€å¤š 10 ä¸ªå…³é”®è¯
    
    async def _load_session_metadata(self, session_file: Path) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½ session å…ƒæ•°æ®
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä»å•ç‹¬çš„ JSON æ–‡ä»¶åŠ è½½ï¼ˆsession_xxx_metadata.jsonï¼‰
        2. å¦åˆ™ä» MD æ–‡ä»¶æœ«å°¾çš„ JSON ä»£ç å—è§£æ
        """
        # æ–¹æ¡ˆ 1ï¼šå•ç‹¬çš„ metadata JSON æ–‡ä»¶
        metadata_file = session_file.parent / f"{session_file.stem}_metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"âŒ Failed to load metadata from {metadata_file}: {e}")
        
        # æ–¹æ¡ˆ 2ï¼šä» MD æ–‡ä»¶è§£æï¼ˆæš‚ä¸å®ç°ï¼Œé¿å…è¯»å–å¤§æ–‡ä»¶ï¼‰
        # TODO: å®ç°ä» MD æœ«å°¾æå– JSON
        
        return None
    
    def _calculate_relevance(
        self,
        keywords: List[str],
        metadata: Dict[str, Any]
    ) -> float:
        """
        è®¡ç®—ç›¸å…³åº¦
        
        ç­–ç•¥ï¼š
        - keywords ä¸ metadata.topics çš„é‡å åº¦
        
        Returns:
            ç›¸å…³åº¦ (0.0 - 1.0)
        """
        if not keywords or not metadata.get("topics"):
            return 0.0
        
        topics = metadata.get("topics", [])
        
        # è®¡ç®—å…³é”®è¯åœ¨ topics ä¸­å‡ºç°çš„æ¯”ä¾‹
        matches = 0
        for keyword in keywords:
            for topic in topics:
                if keyword in topic or topic in keyword:
                    matches += 1
                    break
        
        relevance = matches / len(keywords) if keywords else 0.0
        
        return min(relevance, 1.0)
    
    async def _write_session_header(self):
        """å†™å…¥ MD æ–‡ä»¶å¤´éƒ¨ï¼ˆæ—§ç‰ˆï¼Œä¿æŒå…¼å®¹æ€§ï¼‰"""
        header = self._format_session_header()
        
        # ğŸ”§ ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆé˜²æ­¢ç›®å½•è¢«å¤–éƒ¨åˆ é™¤åå†™å…¥å¤±è´¥ï¼‰
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        with open(self.current_session_file, 'w', encoding='utf-8') as f:
            f.write(header)
        
        logger.info(f"ğŸ“ Created session file: {self.current_session_file}")
    
    async def _write_session_header_with_inheritance(self, inherited_context: Dict[str, Any]):
        """
        ğŸ†• å†™å…¥ MD æ–‡ä»¶å¤´éƒ¨ï¼ˆå¸¦ç»§æ‰¿ä¿¡æ¯ï¼‰
        
        Args:
            inherited_context: ä»æ—§ session ç»§æ‰¿çš„ä¸Šä¸‹æ–‡
        """
        header = self._format_session_header_with_inheritance(inherited_context)
        
        # ğŸ”§ ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆé˜²æ­¢ç›®å½•è¢«å¤–éƒ¨åˆ é™¤åå†™å…¥å¤±è´¥ï¼‰
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        with open(self.current_session_file, 'w', encoding='utf-8') as f:
            f.write(header)
        
        logger.info(f"ğŸ“ Created session file with inherited context: {self.current_session_file}")
    
    def _format_session_header(self) -> str:
        """æ ¼å¼åŒ– session å¤´éƒ¨ï¼ˆæ—§ç‰ˆï¼‰"""
        metadata = self.session_metadata
        timestamp = datetime.fromisoformat(metadata["start_time"])
        
        header = f"""# Learning Session - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}

**User**: {self.user_id}  
**Session ID**: {metadata['session_id']}  
**Started**: {metadata['start_time']}  
**Last Updated**: {metadata['last_updated']}  
**Status**: {metadata['status']}

"""
        
        # æ·»åŠ ç›¸å…³ sessions å¼•ç”¨
        if metadata.get("related_sessions"):
            header += "**Related Sessions**:\n"
            for related in metadata["related_sessions"]:
                header += f"- ğŸ“ [{related['session_id']}]({related['session_id']}.md) - {', '.join(related['topics'])} (ç›¸å…³åº¦: {related['relevance_score']:.0%})\n"
            header += "\n"
        
        header += "---\n\n"
        
        return header
    
    def _format_session_header_with_inheritance(self, inherited_context: Dict[str, Any]) -> str:
        """
        ğŸ†• æ ¼å¼åŒ– session å¤´éƒ¨ï¼ˆå¸¦ç»§æ‰¿ä¿¡æ¯ï¼‰
        
        æ·»åŠ ï¼š
        1. çˆ¶ session é“¾æ¥
        2. ç»§æ‰¿çš„ summary
        3. å…³é”®ä¸»é¢˜
        4. æœ€åçš„ artifacts å¼•ç”¨
        """
        metadata = self.session_metadata
        timestamp = datetime.fromisoformat(metadata["start_time"])
        
        # åŸºæœ¬ä¿¡æ¯
        is_continuation = bool(inherited_context.get("previous_session_id"))
        title = f"Learning Session - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}"
        if is_continuation:
            title += " (Continued)"
        
        header = f"""# {title}

**User**: {self.user_id}  
**Session ID**: {metadata['session_id']}  
**Started**: {metadata['start_time']}  
**Last Updated**: {metadata['last_updated']}  
**Status**: {metadata['status']}
"""
        
        # ğŸ†• ç»§æ‰¿ä¿¡æ¯
        if is_continuation:
            prev_session_id = inherited_context["previous_session_id"]
            header += f"**Previous Session**: ğŸ”— [{prev_session_id}](./{prev_session_id}.md)\n"
        
        header += "\n"
        
        # ğŸ†• ç»§æ‰¿çš„ä¸Šä¸‹æ–‡æ‘˜è¦
        if inherited_context.get("summary"):
            header += "---\n\n## ğŸ“š Inherited Context\n\n"
            header += f"> **Summary of Previous Session:**\n"
            header += f"> {inherited_context['summary']}\n\n"
            
            # å…³é”®ä¸»é¢˜
            if inherited_context.get("key_topics"):
                topics_str = ", ".join(inherited_context["key_topics"])
                header += f"**Key Topics**: {topics_str}\n\n"
            
            # æœ€åçš„ artifacts
            if inherited_context.get("last_artifacts"):
                header += "**Last Artifacts**:\n"
                for artifact in inherited_context["last_artifacts"]:
                    artifact_type = artifact.get("type", "artifact")
                    artifact_ref = artifact.get("content_reference", "N/A")
                    header += f"- {artifact_type}: `{artifact_ref}`\n"
                header += "\n"
            
            header += "---\n\n"
        
        # ç›¸å…³ sessions å¼•ç”¨
        if metadata.get("related_sessions"):
            header += "**Related Sessions**:\n"
            for related in metadata["related_sessions"]:
                session_id = related['session_id']
                topics = ', '.join(related.get('topics', []))
                relevance = related['relevance_score']
                header += f"- ğŸ“ [{session_id}](./{session_id}.md) - {topics} (ç›¸å…³åº¦: {relevance:.0%})\n"
            header += "\n---\n\n"
        
        return header
    
    async def append_turn(
        self,
        turn_data: Dict[str, Any]
    ) -> bool:
        """
        è¿½åŠ ä¸€ä¸ªå¯¹è¯è½®æ¬¡åˆ° MD æ–‡ä»¶
        
        ğŸ”’ å¹¶å‘å®‰å…¨ï¼šä½¿ç”¨ async lock ä¿æŠ¤æ–‡ä»¶å†™å…¥
        
        Args:
            turn_data: {
                "user_query": str,
                "agent_response": Dict[str, Any],
                "response_type": str,  # explanation, quiz_set, flashcard_set, etc.
                "timestamp": datetime,
                "intent": Dict[str, Any],
                "metadata": Dict[str, Any]  # thinking_tokens, output_tokens, duration, model
            }
        
        Returns:
            æˆåŠŸè¿”å› True
        """
        # ğŸ”’ è·å–å†™å…¥é”ï¼Œé˜²æ­¢å¹¶å‘å†™å…¥å¯¼è‡´æ–‡ä»¶æŸå
        async with self._write_lock:
            try:
                # å¢åŠ  turn è®¡æ•°
                self.turn_counter += 1
                turn_data["turn_number"] = self.turn_counter
                
                # æ ¼å¼åŒ– Turn
                from .markdown_formatter import MarkdownFormatter
                formatter = MarkdownFormatter()
                
                turn_md = formatter.format_turn(turn_data)
                
                # ğŸ”§ ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆé˜²æ­¢ç›®å½•è¢«å¤–éƒ¨åˆ é™¤åå†™å…¥å¤±è´¥ï¼‰
                self.storage_path.mkdir(parents=True, exist_ok=True)
                
                # è¿½åŠ åˆ°æ–‡ä»¶
                with open(self.current_session_file, 'a', encoding='utf-8') as f:
                    f.write(turn_md)
                    f.write("\n---\n\n")
                
                # æ›´æ–° session å…ƒæ•°æ®
                await self._update_session_metadata(turn_data)
                
                # ä¸Šä¼ åˆ° S3
                if self.s3_manager:
                    if self.s3_manager.is_available():
                        await self._upload_to_s3()
                    else:
                        logger.warning(f"âš ï¸  S3 not available (is_available=False), skipping upload")
                else:
                    logger.warning(f"âš ï¸  S3 manager not set, skipping upload")
                
                logger.info(f"âœ… Appended Turn {self.turn_counter} to {self.current_session_file.name}")
                
                # ğŸ†• æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©æ—§å¯¹è¯
                if self._should_compress_old_turns():
                    await self._compress_old_turns()
                
                return True
            
            except Exception as e:
                logger.error(f"âŒ Failed to append turn: {e}")
                return False
    
    def _should_compress_old_turns(self) -> bool:
        """
        ğŸ†• æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©æ—§å¯¹è¯
        
        è§¦å‘æ¡ä»¶ï¼š
        1. Turn æ•°è¶…è¿‡ COMPRESS_TRIGGER_TURNSï¼ˆ30ï¼‰
        2. ä¸”æœ‰è¶³å¤Ÿå¤šçš„æ—§å¯¹è¯å¯å‹ç¼©ï¼ˆè¶…è¿‡ KEEP_RECENT_TURNSï¼‰
        """
        if self.turn_counter < self.COMPRESS_TRIGGER_TURNS:
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å‹ç¼©è¿‡ï¼ˆé¿å…é‡å¤å‹ç¼©ï¼‰
        if self.session_metadata.get("compressed_history"):
            # å·²æœ‰å‹ç¼©å†å²ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å†æ¬¡å‹ç¼©
            last_compression_turn = self.session_metadata.get("last_compression_turn", 0)
            turns_since_compression = self.turn_counter - last_compression_turn
            # æ¯ 20 è½®å‹ç¼©ä¸€æ¬¡
            if turns_since_compression < 20:
                return False
        
        return True
    
    async def _compress_old_turns(self):
        """
        ğŸ†• å‹ç¼©æ—§å¯¹è¯ä¸º summaryï¼Œä¿ç•™æœ€è¿‘ N è½®å®Œæ•´å¯¹è¯
        
        ç­–ç•¥ï¼ˆæ”¹è¿›ç‰ˆ - ä¿ç•™å½’æ¡£ï¼‰ï¼š
        1. è¯»å– MD æ–‡ä»¶å†…å®¹
        2. åˆ†ç¦»å‡ºï¼šHeaderã€å‹ç¼©å†å²åŒºã€æœ€è¿‘ N è½®å®Œæ•´å¯¹è¯
        3. ğŸ†• å°†æ—§å¯¹è¯å®Œæ•´ä¿å­˜åˆ°å½’æ¡£æ–‡ä»¶ï¼ˆsession_xxx_archive_001.mdï¼‰
        4. åœ¨ä¸»æ–‡ä»¶ä¸­åªä¿ç•™æ‘˜è¦ + å½’æ¡£æ–‡ä»¶å¼•ç”¨
        5. é‡å†™ MD æ–‡ä»¶ï¼šHeader + å‹ç¼©å†å²ï¼ˆå«å½’æ¡£å¼•ç”¨ï¼‰+ æœ€è¿‘ N è½®
        """
        if not self.current_session_file or not self.current_session_file.exists():
            return
        
        try:
            logger.info(f"ğŸ—œï¸ Starting compression for {self.current_session_file.name}...")
            
            # è¯»å–å½“å‰ MD å†…å®¹
            content = self.current_session_file.read_text(encoding='utf-8')
            
            # åˆ†å‰²æˆå„ä¸ªéƒ¨åˆ†
            parts = self._parse_md_structure(content)
            
            header = parts.get("header", "")
            existing_compressed = parts.get("compressed_history", "")
            turns = parts.get("turns", [])
            
            total_turns = len(turns)
            
            if total_turns <= self.KEEP_RECENT_TURNS:
                logger.info(f"ğŸ“ Not enough turns to compress ({total_turns} <= {self.KEEP_RECENT_TURNS})")
                return
            
            # åˆ†ç¦»ï¼šè¦å‹ç¼©çš„æ—§å¯¹è¯ vs ä¿ç•™çš„æœ€è¿‘å¯¹è¯
            turns_to_compress = turns[:-self.KEEP_RECENT_TURNS]
            turns_to_keep = turns[-self.KEEP_RECENT_TURNS:]
            
            # ğŸ†• åˆ›å»ºå½’æ¡£æ–‡ä»¶ä¿ç•™å®Œæ•´çš„æ—§å¯¹è¯
            archive_file = await self._archive_old_turns(turns_to_compress)
            archive_filename = archive_file.name if archive_file else None
            
            # ç”Ÿæˆå‹ç¼© summaryï¼ˆåŒ…å«å½’æ¡£å¼•ç”¨ï¼‰
            new_summary = self._generate_compression_summary_with_archive(
                turns_to_compress, 
                archive_filename
            )
            
            # åˆå¹¶åˆ°ç°æœ‰å‹ç¼©å†å²
            if existing_compressed:
                combined_compressed = f"{existing_compressed}\n\n{new_summary}"
            else:
                combined_compressed = new_summary
            
            # é‡å†™ MD æ–‡ä»¶
            await self._rewrite_md_with_compression(
                header=header,
                compressed_history=combined_compressed,
                recent_turns=turns_to_keep
            )
            
            # æ›´æ–°å…ƒæ•°æ®
            self.session_metadata["compressed_history"] = True
            self.session_metadata["last_compression_turn"] = self.turn_counter
            self.session_metadata["compression_count"] = self.session_metadata.get("compression_count", 0) + 1
            self.session_metadata["compressed_turns_total"] = self.session_metadata.get("compressed_turns_total", 0) + len(turns_to_compress)
            
            # ğŸ†• è®°å½•å½’æ¡£æ–‡ä»¶
            if archive_filename:
                if "archive_files" not in self.session_metadata:
                    self.session_metadata["archive_files"] = []
                self.session_metadata["archive_files"].append({
                    "filename": archive_filename,
                    "turns_range": self._extract_turns_range(turns_to_compress),
                    "created_at": datetime.now().isoformat()
                })
            
            await self._save_session_metadata()
            
            logger.info(
                f"âœ… Compressed {len(turns_to_compress)} turns â†’ archived to {archive_filename}, kept {len(turns_to_keep)} recent turns"
            )
            
        except Exception as e:
            logger.error(f"âŒ Failed to compress old turns: {e}", exc_info=True)
    
    async def _archive_old_turns(self, turns: List[str]) -> Optional[Path]:
        """
        ğŸ†• å°†æ—§å¯¹è¯å½’æ¡£åˆ°ç‹¬ç«‹æ–‡ä»¶
        
        Args:
            turns: è¦å½’æ¡£çš„ Turn å†…å®¹åˆ—è¡¨
            
        Returns:
            å½’æ¡£æ–‡ä»¶ Pathï¼ˆå¦‚ session_xxx_archive_001.mdï¼‰
        """
        if not turns:
            return None
        
        try:
            # ç¡®å®šå½’æ¡£æ–‡ä»¶å
            compression_count = self.session_metadata.get("compression_count", 0) + 1
            archive_filename = f"{self.current_session_id}_archive_{compression_count:03d}.md"
            archive_path = self.storage_path / archive_filename
            
            # æå– turn èŒƒå›´
            turns_range = self._extract_turns_range(turns)
            
            # æ„å»ºå½’æ¡£æ–‡ä»¶å†…å®¹
            archive_content_parts = []
            
            # å½’æ¡£æ–‡ä»¶å¤´éƒ¨
            archive_header = f"""# ğŸ“¦ å¯¹è¯å½’æ¡£ - {self.current_session_id}

**å½’æ¡£æ—¶é—´**: {datetime.now().isoformat()}  
**åŸ Session**: [{self.current_session_id}](./{self.current_session_id}.md)  
**è½®æ¬¡èŒƒå›´**: Turn {turns_range['start']} - {turns_range['end']}ï¼ˆå…± {len(turns)} è½®ï¼‰  
**å½’æ¡£ç¼–å·**: #{compression_count}

---

> âš ï¸ æ­¤æ–‡ä»¶åŒ…å«å·²å‹ç¼©å¯¹è¯çš„å®Œæ•´åŸå§‹è®°å½•ã€‚
> ä¸» Session æ–‡ä»¶ä¸­ä¿ç•™äº†è¿™äº›å¯¹è¯çš„æ‘˜è¦ã€‚
> æ™ºèƒ½æ£€ç´¢åŠŸèƒ½å¯è‡ªåŠ¨ä»æ­¤å½’æ¡£ä¸­æ£€ç´¢è¯¦ç»†å†…å®¹ã€‚

---

"""
            archive_content_parts.append(archive_header)
            
            # æ·»åŠ æ‰€æœ‰å½’æ¡£çš„å¯¹è¯
            for turn in turns:
                archive_content_parts.append(turn)
                archive_content_parts.append("\n---\n\n")
            
            # å†™å…¥å½’æ¡£æ–‡ä»¶
            archive_content = "".join(archive_content_parts)
            archive_path.write_text(archive_content, encoding='utf-8')
            
            logger.info(f"ğŸ“¦ Archived {len(turns)} turns to {archive_filename}")
            
            return archive_path
            
        except Exception as e:
            logger.error(f"âŒ Failed to create archive: {e}")
            return None
    
    def _extract_turns_range(self, turns: List[str]) -> Dict[str, int]:
        """æå– turn èŒƒå›´"""
        start = end = 0
        
        for t in turns:
            match = re.search(r'## Turn (\d+)', t)
            if match:
                num = int(match.group(1))
                if start == 0 or num < start:
                    start = num
                if num > end:
                    end = num
        
        return {"start": start, "end": end}
    
    def _generate_compression_summary_with_archive(
        self, 
        turns: List[str], 
        archive_filename: Optional[str]
    ) -> str:
        """
        ğŸ†• ç”Ÿæˆå‹ç¼©æ‘˜è¦ï¼ˆåŒ…å«å½’æ¡£æ–‡ä»¶å¼•ç”¨ï¼‰
        
        Args:
            turns: è¦å‹ç¼©çš„ Turn å†…å®¹åˆ—è¡¨
            archive_filename: å½’æ¡£æ–‡ä»¶å
        
        Returns:
            å‹ç¼©åçš„æ‘˜è¦æ–‡æœ¬ï¼ˆåŒ…å«å½’æ¡£å¼•ç”¨ï¼‰
        """
        # ä½¿ç”¨åŸæœ‰é€»è¾‘ç”Ÿæˆæ‘˜è¦å†…å®¹
        base_summary = self._generate_compression_summary(turns)
        
        # æ·»åŠ å½’æ¡£å¼•ç”¨
        if archive_filename:
            archive_note = f"\n> ğŸ“¦ **å®Œæ•´å¯¹è¯å½’æ¡£**: [{archive_filename}](./{archive_filename}) - å¦‚éœ€æŸ¥çœ‹è¯¦ç»†å†…å®¹è¯·å‚è€ƒæ­¤æ–‡ä»¶"
            return f"{base_summary}\n{archive_note}"
        
        return base_summary
    
    def _parse_md_structure(self, content: str) -> Dict[str, Any]:
        """
        è§£æ MD æ–‡ä»¶ç»“æ„
        
        Returns:
            {
                "header": str,           # Session Header
                "compressed_history": str,  # å‹ç¼©çš„å†å²æ‘˜è¦
                "turns": List[str]       # å„ä¸ª Turn çš„å®Œæ•´å†…å®¹
            }
        """
        result = {
            "header": "",
            "compressed_history": "",
            "turns": []
        }
        
        # åˆ†å‰² Header å’Œ å†…å®¹
        # Header é€šå¸¸ä»¥ "## Turn" å¼€å§‹å‰çš„éƒ¨åˆ†
        header_pattern = r'^(# Learning Session.*?)(?=## Turn|\Z)'
        header_match = re.search(header_pattern, content, re.DOTALL)
        
        if header_match:
            result["header"] = header_match.group(1).strip()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‹ç¼©å†å²åŒºï¼ˆç”¨ç‰¹æ®Šæ ‡è®°è¯†åˆ«ï¼‰
        compressed_pattern = r'## ğŸ“š å†å²æ‘˜è¦\n(.*?)(?=## Turn|\Z)'
        compressed_match = re.search(compressed_pattern, content, re.DOTALL)
        
        if compressed_match:
            result["compressed_history"] = compressed_match.group(1).strip()
        
        # æå–æ‰€æœ‰ Turns
        turn_pattern = r'(## Turn \d+.*?)(?=## Turn \d+|---\s*$|\Z)'
        turns = re.findall(turn_pattern, content, re.DOTALL)
        
        # æ¸…ç†æ¯ä¸ª turn
        result["turns"] = [t.strip() for t in turns if t.strip()]
        
        return result
    
    def _generate_compression_summary(self, turns: List[str]) -> str:
        """
        ç”Ÿæˆå‹ç¼©æ‘˜è¦ï¼ˆä¸è°ƒç”¨ LLMï¼Œä½¿ç”¨è§„åˆ™æå–ï¼‰
        
        Args:
            turns: è¦å‹ç¼©çš„ Turn å†…å®¹åˆ—è¡¨
        
        Returns:
            å‹ç¼©åçš„æ‘˜è¦æ–‡æœ¬
        """
        if not turns:
            return ""
        
        # æå–å…³é”®ä¿¡æ¯
        topics_mentioned = set()
        skills_used = {}
        key_queries = []
        
        for turn_content in turns:
            # æå–ç”¨æˆ·é—®é¢˜
            query_match = re.search(r'### ğŸ‘¤ User Query\n(.+?)(?=\n###|\Z)', turn_content, re.DOTALL)
            if query_match:
                query = query_match.group(1).strip()[:100]  # é™åˆ¶é•¿åº¦
                key_queries.append(query)
            
            # æå– topic
            topic_match = re.search(r'\*\*Topic\*\*:\s*([^\n|]+)', turn_content)
            if topic_match:
                topics_mentioned.add(topic_match.group(1).strip())
            
            # æå– response type
            type_match = re.search(r'\*\*Type\*\*:\s*(\w+)', turn_content)
            if type_match:
                skill = type_match.group(1)
                skills_used[skill] = skills_used.get(skill, 0) + 1
        
        # æ„å»ºæ‘˜è¦
        summary_parts = []
        
        # æ—¶é—´èŒƒå›´
        turn_numbers = []
        for t in turns:
            num_match = re.search(r'## Turn (\d+)', t)
            if num_match:
                turn_numbers.append(int(num_match.group(1)))
        
        if turn_numbers:
            summary_parts.append(f"**è½®æ¬¡ {min(turn_numbers)}-{max(turn_numbers)}**ï¼ˆå…± {len(turns)} è½®ï¼‰")
        
        # ä¸»é¢˜
        if topics_mentioned:
            topics_str = "ã€".join(list(topics_mentioned)[:5])
            summary_parts.append(f"- ğŸ“– **å­¦ä¹ ä¸»é¢˜**: {topics_str}")
        
        # æŠ€èƒ½ä½¿ç”¨
        if skills_used:
            skills_str = ", ".join([f"{k}Ã—{v}" for k, v in list(skills_used.items())[:4]])
            summary_parts.append(f"- ğŸ› ï¸ **ä½¿ç”¨æŠ€èƒ½**: {skills_str}")
        
        # å…³é”®é—®é¢˜ï¼ˆå–å‰ 3 ä¸ªï¼‰
        if key_queries:
            summary_parts.append("- ğŸ’¬ **å…³é”®é—®é¢˜**:")
            for q in key_queries[:3]:
                summary_parts.append(f"  - {q[:60]}{'...' if len(q) > 60 else ''}")
        
        return "\n".join(summary_parts)
    
    async def _rewrite_md_with_compression(
        self,
        header: str,
        compressed_history: str,
        recent_turns: List[str]
    ):
        """
        é‡å†™ MD æ–‡ä»¶ï¼ŒåŒ…å«å‹ç¼©å†å²
        """
        new_content_parts = []
        
        # 1. Header
        new_content_parts.append(header)
        new_content_parts.append("")
        
        # 2. å‹ç¼©å†å²åŒº
        if compressed_history:
            new_content_parts.append("## ğŸ“š å†å²æ‘˜è¦")
            new_content_parts.append("")
            new_content_parts.append("> *ä»¥ä¸‹æ˜¯æ—©æœŸå¯¹è¯çš„å‹ç¼©æ‘˜è¦ã€‚å®Œæ•´çš„åŸå§‹å¯¹è¯å·²ä¿å­˜åˆ°å½’æ¡£æ–‡ä»¶ä¸­ã€‚*")
            new_content_parts.append("> *æ™ºèƒ½æ£€ç´¢åŠŸèƒ½å¯è‡ªåŠ¨ä»å½’æ¡£ä¸­æ£€ç´¢è¯¦ç»†å†…å®¹ã€‚*")
            new_content_parts.append("")
            new_content_parts.append(compressed_history)
            new_content_parts.append("")
            new_content_parts.append("---")
            new_content_parts.append("")
        
        # 3. æœ€è¿‘çš„å®Œæ•´å¯¹è¯
        for turn in recent_turns:
            new_content_parts.append(turn)
            new_content_parts.append("")
            new_content_parts.append("---")
            new_content_parts.append("")
        
        # å†™å…¥æ–‡ä»¶
        new_content = "\n".join(new_content_parts)
        self.current_session_file.write_text(new_content, encoding='utf-8')
        
        logger.info(f"ğŸ“ Rewrote {self.current_session_file.name} with compressed history")
    
    async def _update_session_metadata(self, turn_data: Dict[str, Any]):
        """æ›´æ–° session å…ƒæ•°æ®"""
        metadata = self.session_metadata
        
        # æ›´æ–° last_updated
        if isinstance(turn_data["timestamp"], datetime):
            metadata["last_updated"] = turn_data["timestamp"].isoformat()
        else:
            metadata["last_updated"] = turn_data["timestamp"]
        
        # æ›´æ–° total_turns
        metadata["total_turns"] = self.turn_counter
        
        # æ›´æ–° topics
        if "topic" in turn_data.get("intent", {}):
            topic = turn_data["intent"]["topic"]
            if topic:
                if topic not in metadata["topics"]:
                    metadata["topics"].append(topic)
                # ğŸ†• æ›´æ–° last_topicï¼ˆç”¨äºè·¨ session ç»§æ‰¿ï¼‰
                metadata["last_topic"] = topic
                logger.debug(f"ğŸ“š Updated last_topic: {topic}")
        
        # æ›´æ–° skills_used
        response_type = turn_data.get("response_type", "unknown")
        metadata["skills_used"][response_type] = metadata["skills_used"].get(response_type, 0) + 1
        
        # è®°å½• artifacts
        if "artifact_id" in turn_data.get("agent_response", {}):
            metadata["artifacts_generated"].append({
                "turn": self.turn_counter,
                "type": response_type,
                "artifact_id": turn_data["agent_response"]["artifact_id"],
                "topic": turn_data.get("intent", {}).get("topic", "")
            })
        
        # ä¿å­˜å…ƒæ•°æ®åˆ°å•ç‹¬çš„ JSON æ–‡ä»¶
        await self._save_session_metadata()
    
    async def _save_session_metadata(self):
        """ä¿å­˜ session å…ƒæ•°æ®åˆ°å•ç‹¬çš„ JSON æ–‡ä»¶"""
        metadata_file = self.storage_path / f"{self.current_session_id}_metadata.json"
        
        try:
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.session_metadata, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ Saved session metadata: {metadata_file.name}")
        
        except Exception as e:
            logger.error(f"âŒ Failed to save session metadata: {e}")
    
    async def _upload_to_s3(self):
        """ä¸Šä¼  MD æ–‡ä»¶å’Œ metadata JSON æ–‡ä»¶åˆ° S3"""
        if not self.s3_manager:
            logger.warning("âš ï¸  S3 manager not initialized, skipping upload")
            return
        
        if not self.s3_manager.is_available():
            logger.warning("âš ï¸  S3 not available, skipping upload")
            return
        
        logger.info("ğŸ”„ Starting S3 upload...")
        
        try:
            # ä¸Šä¼  MD æ–‡ä»¶
            if self.current_session_file and self.current_session_file.exists():
                s3_key_md = f"{self.user_id}/{self.current_session_file.name}"
                result = self.s3_manager.save(
                    s3_key_md,
                    self.current_session_file.read_text(encoding='utf-8'),
                    content_type="text/markdown"
                )
                if result:
                    logger.info(f"â˜ï¸  Uploaded MD to S3: s3://{self.s3_manager.bucket}/{s3_key_md}")
                else:
                    logger.warning(f"âš ï¸  Failed to upload MD to S3: {s3_key_md}")
            else:
                logger.warning(f"âš ï¸  MD file not found: {self.current_session_file}")
            
            # ä¸Šä¼  metadata JSON æ–‡ä»¶
            if self.current_session_id:
                metadata_file = self.storage_path / f"{self.current_session_id}_metadata.json"
                if metadata_file.exists():
                    s3_key_metadata = f"{self.user_id}/{metadata_file.name}"
                    result = self.s3_manager.save(
                        s3_key_metadata,
                        metadata_file.read_text(encoding='utf-8'),
                        content_type="application/json"
                    )
                    if result:
                        logger.info(f"â˜ï¸  Uploaded metadata to S3: s3://{self.s3_manager.bucket}/{s3_key_metadata}")
                    else:
                        logger.warning(f"âš ï¸  Failed to upload metadata to S3: {s3_key_metadata}")
                else:
                    logger.warning(f"âš ï¸  Metadata file not found: {metadata_file}")
        
        except Exception as e:
            logger.error(f"âŒ Failed to upload to S3: {e}")
    
    async def get_recent_turns(self, num_turns: int = 5) -> str:
        """
        ğŸ†• è·å–æœ€è¿‘ N è½®å¯¹è¯å†…å®¹ï¼ˆç”¨äºæ„å»º LLM contextï¼‰
        
        Args:
            num_turns: è¦è·å–çš„è½®æ¬¡æ•°é‡
        
        Returns:
            æœ€è¿‘ N è½®å¯¹è¯çš„ Markdown æ–‡æœ¬
        """
        if not self.current_session_file or not self.current_session_file.exists():
            return ""
        
        try:
            content = self.current_session_file.read_text(encoding='utf-8')
            
            # è§£æ turnsï¼ˆåŒ¹é… "## Turn X"ï¼‰
            turn_pattern = re.compile(r'^## Turn \d+', re.MULTILINE)
            turn_positions = [m.start() for m in turn_pattern.finditer(content)]
            
            if not turn_positions:
                return ""
            
            # è·å–æœ€å N ä¸ª turn çš„èµ·å§‹ä½ç½®
            recent_turn_positions = turn_positions[-num_turns:]
            
            # æå–å†…å®¹ï¼ˆä»ç¬¬ä¸€ä¸ª recent turn åˆ°æ–‡ä»¶æœ«å°¾ï¼‰
            if recent_turn_positions:
                recent_content = content[recent_turn_positions[0]:]
                return recent_content
            
            return ""
        
        except Exception as e:
            logger.error(f"âŒ Failed to get recent turns: {e}")
            return ""
    
    async def get_session_context_for_llm(
        self,
        include_recent_turns: int = 5,
        include_inherited: bool = True
    ) -> str:
        """
        ğŸ†• ä¸º LLM æ„å»ºå®Œæ•´çš„ session contextï¼ˆæ™ºèƒ½åŠ è½½ï¼‰
        
        åŒ…å«ï¼š
        1. ç»§æ‰¿çš„ summaryï¼ˆå¦‚æœæœ‰ï¼‰
        2. æœ€è¿‘ N è½®å¯¹è¯
        
        Args:
            include_recent_turns: åŒ…å«æœ€è¿‘å‡ è½®å¯¹è¯
            include_inherited: æ˜¯å¦åŒ…å«ç»§æ‰¿çš„ä¸Šä¸‹æ–‡
        
        Returns:
            LLM context string
        """
        context_parts = []
        
        # 1. ç»§æ‰¿çš„ä¸Šä¸‹æ–‡
        if include_inherited and self.session_metadata.get("inherited_context"):
            inherited = self.session_metadata["inherited_context"]
            if inherited.get("continuation_prompt"):
                context_parts.append(f"### Context from Previous Session\n{inherited['continuation_prompt']}")
        
        # 2. æœ€è¿‘çš„å¯¹è¯
        recent_turns = await self.get_recent_turns(num_turns=include_recent_turns)
        if recent_turns:
            context_parts.append(f"### Recent Conversation\n{recent_turns}")
        
        return "\n\n---\n\n".join(context_parts) if context_parts else ""
    
    async def finalize_session(self):
        """
        ç»“æŸ sessionï¼Œæ·»åŠ æ‘˜è¦
        """
        if not self.current_session_id or not self.current_session_file:
            return
        
        try:
            # ç”Ÿæˆ session æ‘˜è¦
            summary = await self._generate_session_summary()
            
            # è¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾
            with open(self.current_session_file, 'a', encoding='utf-8') as f:
                f.write(f"\n\n---\n\n## ğŸ“Š Session Summary\n\n{summary}\n")
            
            # æ›´æ–°çŠ¶æ€
            self.session_metadata["status"] = "completed"
            await self._save_session_metadata()
            
            # æœ€åä¸€æ¬¡ä¸Šä¼ åˆ° S3
            if self.s3_manager and self.s3_manager.is_available():
                await self._upload_to_s3()
            
            logger.info(f"âœ… Finalized session: {self.current_session_id}")
        
        except Exception as e:
            logger.error(f"âŒ Failed to finalize session: {e}")
    
    def _generate_session_summary(self) -> str:
        """ç”Ÿæˆ session æ‘˜è¦"""
        metadata = self.session_metadata
        
        start_time = datetime.fromisoformat(metadata["start_time"])
        end_time = datetime.fromisoformat(metadata["last_updated"])
        duration = (end_time - start_time).total_seconds() / 60  # åˆ†é’Ÿ
        
        summary = f"""## ğŸ“Š Session Summary

**Duration**: {duration:.1f} minutes ({start_time.strftime('%H:%M:%S')} - {end_time.strftime('%H:%M:%S')})  
**Total Turns**: {metadata['total_turns']}  
**Topics Discussed**: {', '.join(metadata['topics']) if metadata['topics'] else 'N/A'}  
**Skills Used**: 
"""
        
        for skill, count in metadata.get("skills_used", {}).items():
            summary += f"- {skill} ({count} time{'s' if count > 1 else ''})\n"
        
        summary += f"""
**Learning Progress**:
- âœ… Generated {len(metadata.get('artifacts_generated', []))} artifacts

**Next Session Suggestions**:
- å¯ä»¥ç»§ç»­å­¦ä¹ ç›¸å…³ä¸»é¢˜
- æˆ–è€…åˆ‡æ¢æ–°ä¸»é¢˜

<details>
<summary>ğŸ“¦ <b>Session å…ƒæ•°æ®ï¼ˆJSONï¼‰</b> - ç‚¹å‡»å±•å¼€</summary>

```json
{json.dumps(metadata, ensure_ascii=False, indent=2)}
```

</details>

---

*Last saved: {end_time.strftime('%Y-%m-%d %H:%M:%S')}*  
*Storage: {self.current_session_file}*  
"""
        
        if self.s3_manager and self.s3_manager.is_available():
            summary += f"*S3: s3://{self.s3_manager.bucket}/{self.user_id}/{self.current_session_file.name}*  \n"
        
        return summary
    
    async def load_recent_context(
        self,
        max_sessions: int = 1,
        max_turns_per_session: int = 10
    ) -> str:
        """
        åŠ è½½æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äº LLMï¼‰
        
        Args:
            max_sessions: åŠ è½½æœ€è¿‘å‡ ä¸ª session
            max_turns_per_session: æ¯ä¸ª session æœ€å¤šåŠ è½½å‡ ä¸ª turn
        
        Returns:
            å®Œæ•´çš„ Markdown æ–‡æœ¬
        """
        try:
            # åˆ—å‡ºæ‰€æœ‰ session MD æ–‡ä»¶
            session_files = sorted(
                self.storage_path.glob("session_*.md"),
                key=lambda f: f.stat().st_mtime,
                reverse=True
            )
            
            # å–æœ€è¿‘çš„ N ä¸ª
            recent_sessions = session_files[:max_sessions]
            
            context = ""
            for session_file in recent_sessions:
                content = session_file.read_text(encoding='utf-8')
                
                # TODO: å¦‚æœéœ€è¦ï¼Œå¯ä»¥æˆªæ–­æ¯ä¸ª session åªå–æœ€å N ä¸ª turns
                # å½“å‰ç›´æ¥è¿”å›å®Œæ•´å†…å®¹
                
                context += content + "\n\n---\n\n"
            
            logger.info(f"ğŸ“š Loaded context from {len(recent_sessions)} sessions")
            
            return context
        
        except Exception as e:
            logger.error(f"âŒ Failed to load recent context: {e}")
            return ""

