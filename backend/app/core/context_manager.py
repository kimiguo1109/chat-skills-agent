"""
Context Manager - ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (Manus é£æ ¼)

åŸºäº Manus çš„ä¸Šä¸‹æ–‡ç®¡ç†ç†å¿µå®ç°ï¼š
1. å‹ç¼© (Compaction) - å¯é€†çš„ï¼Œå°†ä¿¡æ¯è½¬ç§»åˆ°å¤–éƒ¨å­˜å‚¨
2. æ‘˜è¦ (Summarization) - ä¸å¯é€†çš„ï¼Œåªåœ¨å‹ç¼©ä¸è¶³æ—¶ä½¿ç”¨
3. æ£€ç´¢ (Retrieval) - ä»å½’æ¡£æ–‡ä»¶æŒ‰éœ€åŠ è½½

æ ¸å¿ƒåŸåˆ™ï¼š
- å¯é€†æ€§ä¼˜å…ˆï¼šå‹ç¼©ä¸ä¸¢å¤±ä¿¡æ¯ï¼Œåªè½¬ç§»åˆ°å¤–éƒ¨
- ä¿ç•™æœ€æ–°ï¼šå§‹ç»ˆä¿ç•™æœ€è¿‘ N è½®çš„å®Œæ•´ç»†èŠ‚
- æŒ‰éœ€æ£€ç´¢ï¼šæ”¯æŒä»å½’æ¡£æ–‡ä»¶ä¸­æ¢å¤è¯¦ç»†å†…å®¹
"""

import logging
import json
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


@dataclass
class TurnData:
    """å¯¹è¯è½®æ¬¡æ•°æ®"""
    turn_number: int
    user_query: str
    agent_response: Dict[str, Any]
    intent: str
    topic: Optional[str] = None
    artifact_id: Optional[str] = None
    artifact_type: Optional[str] = None
    timestamp: Optional[str] = None
    
    # ğŸ†• å®Œæ•´å†…å®¹ vs ç´§å‡‘å¼•ç”¨
    full_content: Optional[Dict[str, Any]] = None  # å®Œæ•´å†…å®¹ï¼ˆå¯èƒ½å¾ˆå¤§ï¼‰
    compact_reference: Optional[str] = None  # ç´§å‡‘å¼•ç”¨ï¼ˆæ–‡ä»¶è·¯å¾„æˆ– artifact_idï¼‰
    is_compacted: bool = False  # æ˜¯å¦å·²å‹ç¼©ä¸ºç´§å‡‘æ ¼å¼


@dataclass
class ContextState:
    """ä¸Šä¸‹æ–‡çŠ¶æ€"""
    total_chars: int = 0
    total_tokens_estimated: int = 0  # ä¼°ç®—çš„ token æ•°
    turn_count: int = 0
    compacted_turns: int = 0  # å·²å‹ç¼©ä¸ºç´§å‡‘æ ¼å¼çš„è½®æ•°
    summarized_turns: int = 0  # å·²æ‘˜è¦çš„è½®æ•°
    archive_files: List[str] = field(default_factory=list)


class ContextManager:
    """
    Manus é£æ ¼çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    
    å®ç°ä¸‰å±‚ä¸Šä¸‹æ–‡ç®¡ç†ï¼š
    1. å®Œæ•´æ ¼å¼ (Full Format) - ä¿ç•™æ‰€æœ‰ç»†èŠ‚
    2. ç´§å‡‘æ ¼å¼ (Compact Format) - åªä¿ç•™å¼•ç”¨ï¼Œå¯é€†
    3. æ‘˜è¦æ ¼å¼ (Summary Format) - å‹ç¼©ä¸ºæ‘˜è¦ï¼Œä¸å¯é€†
    """
    
    # ============= é˜ˆå€¼é…ç½® =============
    # Token ä¼°ç®—ï¼š1 char â‰ˆ 0.4 tokensï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
    TOKEN_ESTIMATION_RATIO = 0.4
    
    # ğŸ†• "è…çƒ‚å‰"é˜ˆå€¼ - åŸºäº Manus çš„ç»éªŒå€¼
    # å¤§å¤šæ•°æ¨¡å‹åœ¨ 200k tokens å·¦å³å¼€å§‹æ€§èƒ½ä¸‹é™
    SOFT_LIMIT_TOKENS = 50_000   # 50K tokens: å¼€å§‹ç´§å‡‘å‹ç¼©
    HARD_LIMIT_TOKENS = 128_000  # 128K tokens: è§¦å‘æ‘˜è¦
    
    # ä¿ç•™çš„å®Œæ•´è½®æ•°
    KEEP_FULL_TURNS = 6  # ä¿ç•™æœ€è¿‘ 6 è½®å®Œæ•´ç»†èŠ‚
    KEEP_COMPACT_TURNS = 20  # ä¿ç•™æœ€è¿‘ 20 è½®ç´§å‡‘æ ¼å¼
    
    # ç´§å‡‘å‹ç¼©ç™¾åˆ†æ¯”
    COMPACT_OLDEST_PERCENT = 0.5  # å‹ç¼©æœ€æ—§çš„ 50%
    
    def __init__(
        self,
        user_id: str,
        session_id: str,
        storage_path: Path
    ):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            user_id: ç”¨æˆ· ID
            session_id: ä¼šè¯ ID
            storage_path: å­˜å‚¨è·¯å¾„
        """
        self.user_id = user_id
        self.session_id = session_id
        self.storage_path = storage_path
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # ä¸Šä¸‹æ–‡çŠ¶æ€
        self.state = ContextState()
        
        # å¯¹è¯å†å²ï¼ˆå†…å­˜ä¸­ï¼‰
        self.turns: List[TurnData] = []
        
        # å½’æ¡£æ–‡ä»¶å¼•ç”¨
        self.archives: Dict[str, Path] = {}  # archive_id -> file_path
        
        logger.info(f"âœ… ContextManager initialized for {user_id}/{session_id}")
    
    # ============= æ ¸å¿ƒæ–¹æ³• =============
    
    def add_turn(self, turn_data: Dict[str, Any]) -> TurnData:
        """
        æ·»åŠ ä¸€è½®å¯¹è¯
        
        Args:
            turn_data: å¯¹è¯æ•°æ®
            
        Returns:
            TurnData å¯¹è±¡
        """
        turn = TurnData(
            turn_number=len(self.turns) + 1,
            user_query=turn_data.get("user_query", ""),
            agent_response=turn_data.get("agent_response", {}),
            intent=turn_data.get("intent", {}).get("intent", "other"),
            topic=turn_data.get("intent", {}).get("topic"),
            artifact_id=turn_data.get("agent_response", {}).get("artifact_id"),
            artifact_type=turn_data.get("response_type"),
            timestamp=datetime.now().isoformat(),
            full_content=turn_data.get("agent_response", {}).get("content"),
            is_compacted=False
        )
        
        self.turns.append(turn)
        self._update_state()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        self._check_and_compress()
        
        logger.info(f"ğŸ“ Added turn {turn.turn_number}: {turn.intent}, topic={turn.topic}")
        return turn
    
    def get_context_for_llm(
        self,
        max_tokens: int = 50000,
        include_artifacts: bool = True
    ) -> Tuple[str, Dict[str, Any]]:
        """
        è·å–ç”¨äº LLM çš„ä¸Šä¸‹æ–‡
        
        è¿”å›æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œå¹¶é™„å¸¦å…ƒæ•°æ®
        
        Args:
            max_tokens: æœ€å¤§ token æ•°
            include_artifacts: æ˜¯å¦åŒ…å« artifact å†…å®¹
            
        Returns:
            (context_string, metadata)
        """
        context_parts = []
        metadata = {
            "total_turns": len(self.turns),
            "loaded_turns": 0,
            "compacted_turns": 0,
            "retrieved_from_archive": 0,
            "estimated_tokens": 0
        }
        
        # ğŸ†• åˆ†å±‚åŠ è½½ä¸Šä¸‹æ–‡
        # 1. æœ€è¿‘çš„å®Œæ•´è½®æ¬¡ï¼ˆæœ€é‡è¦ï¼‰
        # 2. è¾ƒæ—©çš„ç´§å‡‘è½®æ¬¡
        # 3. æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
        
        current_tokens = 0
        
        # Step 1: åŠ è½½æœ€è¿‘çš„å®Œæ•´è½®æ¬¡
        recent_turns = self.turns[-self.KEEP_FULL_TURNS:] if len(self.turns) > self.KEEP_FULL_TURNS else self.turns
        
        for turn in reversed(recent_turns):  # ä»æœ€æ–°åˆ°æœ€æ—§
            turn_text = self._format_turn_for_context(turn, full=True)
            turn_tokens = self._estimate_tokens(turn_text)
            
            if current_tokens + turn_tokens > max_tokens:
                break
                
            context_parts.insert(0, turn_text)  # æ’å…¥åˆ°å¼€å¤´ä¿æŒé¡ºåº
            current_tokens += turn_tokens
            metadata["loaded_turns"] += 1
        
        # Step 2: å¦‚æœè¿˜æœ‰ç©ºé—´ï¼ŒåŠ è½½ç´§å‡‘æ ¼å¼çš„è¾ƒæ—©è½®æ¬¡
        if current_tokens < max_tokens * 0.8:  # ç•™ 20% ä½™é‡
            older_turns = self.turns[:-self.KEEP_FULL_TURNS] if len(self.turns) > self.KEEP_FULL_TURNS else []
            
            for turn in reversed(older_turns[-self.KEEP_COMPACT_TURNS:]):
                turn_text = self._format_turn_for_context(turn, full=False)
                turn_tokens = self._estimate_tokens(turn_text)
                
                if current_tokens + turn_tokens > max_tokens * 0.9:
                    break
                    
                context_parts.insert(0, turn_text)
                current_tokens += turn_tokens
                metadata["compacted_turns"] += 1
        
        # Step 3: æ·»åŠ æ‘˜è¦å¤´éƒ¨ï¼ˆå¦‚æœæœ‰å½’æ¡£ï¼‰
        if self.archives:
            summary_header = self._generate_archive_summary()
            context_parts.insert(0, summary_header)
            current_tokens += self._estimate_tokens(summary_header)
        
        metadata["estimated_tokens"] = current_tokens
        
        context_string = "\n\n---\n\n".join(context_parts)
        
        logger.info(
            f"ğŸ“Š Context loaded: {metadata['loaded_turns']} full + "
            f"{metadata['compacted_turns']} compact turns, "
            f"~{current_tokens} tokens"
        )
        
        return context_string, metadata
    
    # ============= å‹ç¼©æ–¹æ³• =============
    
    def _check_and_compress(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©ï¼Œå¹¶æ‰§è¡Œç›¸åº”æ“ä½œ"""
        current_tokens = self.state.total_tokens_estimated
        
        # ğŸ†• åˆ†å±‚å‹ç¼©ç­–ç•¥ï¼ˆå‚è€ƒ Manusï¼‰
        if current_tokens >= self.HARD_LIMIT_TOKENS:
            # è§¦å‘æ‘˜è¦ï¼ˆä¸å¯é€†ï¼‰
            logger.warning(f"âš ï¸ Context exceeds hard limit ({current_tokens} >= {self.HARD_LIMIT_TOKENS}), triggering summarization")
            self._summarize_old_turns()
        elif current_tokens >= self.SOFT_LIMIT_TOKENS:
            # è§¦å‘ç´§å‡‘å‹ç¼©ï¼ˆå¯é€†ï¼‰
            logger.info(f"ğŸ“¦ Context exceeds soft limit ({current_tokens} >= {self.SOFT_LIMIT_TOKENS}), triggering compaction")
            self._compact_old_turns()
    
    def _compact_old_turns(self):
        """
        ğŸ†• ç´§å‡‘å‹ç¼©ï¼šå°†æ—§è½®æ¬¡è½¬æ¢ä¸ºç´§å‡‘æ ¼å¼ï¼ˆå¯é€†ï¼‰
        
        ç­–ç•¥ï¼šå‹ç¼©æœ€æ—§çš„ 50%ï¼Œä¿ç•™æœ€æ–°çš„ 50% å®Œæ•´ç»†èŠ‚
        """
        if len(self.turns) <= self.KEEP_FULL_TURNS:
            return
        
        # ç¡®å®šè¦å‹ç¼©çš„è½®æ¬¡æ•°
        compactable_turns = len(self.turns) - self.KEEP_FULL_TURNS
        turns_to_compact = int(compactable_turns * self.COMPACT_OLDEST_PERCENT)
        
        if turns_to_compact == 0:
            return
        
        compacted_count = 0
        for i in range(turns_to_compact):
            turn = self.turns[i]
            if not turn.is_compacted:
                self._compact_turn(turn)
                compacted_count += 1
        
        self._update_state()
        logger.info(f"ğŸ“¦ Compacted {compacted_count} turns (reversible)")
    
    def _compact_turn(self, turn: TurnData):
        """
        å°†å•ä¸ªè½®æ¬¡è½¬æ¢ä¸ºç´§å‡‘æ ¼å¼
        
        ç´§å‡‘æ ¼å¼åªä¿ç•™ï¼š
        - ç”¨æˆ·é—®é¢˜æ‘˜è¦ï¼ˆå‰50å­—ï¼‰
        - artifact å¼•ç”¨ï¼ˆID + ç±»å‹ï¼‰
        - intent å’Œ topic
        
        å®Œæ•´å†…å®¹è¢«å¸è½½åˆ°å¤–éƒ¨æ–‡ä»¶
        """
        if turn.is_compacted:
            return
        
        # å¸è½½å®Œæ•´å†…å®¹åˆ°æ–‡ä»¶
        if turn.full_content:
            offload_path = self._offload_turn_content(turn)
            turn.compact_reference = str(offload_path)
        
        # æ¸…ç©ºå†…å­˜ä¸­çš„å®Œæ•´å†…å®¹
        turn.full_content = None
        turn.is_compacted = True
        
        logger.debug(f"ğŸ“¦ Compacted turn {turn.turn_number} â†’ {turn.compact_reference}")
    
    def _offload_turn_content(self, turn: TurnData) -> Path:
        """
        å°†è½®æ¬¡å†…å®¹å¸è½½åˆ°æ–‡ä»¶
        
        Returns:
            å¸è½½æ–‡ä»¶çš„è·¯å¾„
        """
        offload_dir = self.storage_path / "offloaded"
        offload_dir.mkdir(exist_ok=True)
        
        filename = f"{self.session_id}_turn_{turn.turn_number:03d}.json"
        filepath = offload_dir / filename
        
        offload_data = {
            "turn_number": turn.turn_number,
            "user_query": turn.user_query,
            "agent_response": turn.agent_response,
            "full_content": turn.full_content,
            "intent": turn.intent,
            "topic": turn.topic,
            "timestamp": turn.timestamp,
            "offloaded_at": datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(offload_data, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    def _summarize_old_turns(self):
        """
        ğŸ†• æ‘˜è¦å‹ç¼©ï¼šå°†ç´§å‡‘æ ¼å¼çš„æ—§è½®æ¬¡è½¬æ¢ä¸ºæ‘˜è¦ï¼ˆä¸å¯é€†ï¼‰
        
        ç­–ç•¥ï¼š
        1. å…ˆå°†è¦æ‘˜è¦çš„è½®æ¬¡å®Œæ•´å½’æ¡£åˆ°æ–‡ä»¶
        2. ç”Ÿæˆæ‘˜è¦
        3. åˆ é™¤å†…å­˜ä¸­çš„è½®æ¬¡ï¼ˆä½†ä¿ç•™å½’æ¡£å¼•ç”¨ï¼‰
        """
        if len(self.turns) <= self.KEEP_COMPACT_TURNS:
            return
        
        # ç¡®å®šè¦æ‘˜è¦çš„è½®æ¬¡
        turns_to_summarize = self.turns[:-self.KEEP_COMPACT_TURNS]
        
        if not turns_to_summarize:
            return
        
        # ğŸ†• å…³é”®ï¼šå…ˆå½’æ¡£å®Œæ•´å†…å®¹ï¼ˆç¡®ä¿å¯æ¢å¤ï¼‰
        archive_path = self._archive_turns(turns_to_summarize)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_turns_summary(turns_to_summarize)
        
        # è®°å½•å½’æ¡£
        archive_id = f"archive_{len(self.archives) + 1:03d}"
        self.archives[archive_id] = archive_path
        
        # ä»å†…å­˜ä¸­ç§»é™¤ï¼ˆä½†æ‘˜è¦ä¿ç•™ï¼‰
        self.turns = self.turns[-self.KEEP_COMPACT_TURNS:]
        
        # æ›´æ–°çŠ¶æ€
        self.state.summarized_turns += len(turns_to_summarize)
        self._update_state()
        
        logger.info(
            f"ğŸ“ Summarized {len(turns_to_summarize)} turns â†’ archived to {archive_path}"
        )
    
    def _archive_turns(self, turns: List[TurnData]) -> Path:
        """
        å°†è½®æ¬¡å½’æ¡£åˆ°æ–‡ä»¶ï¼ˆç”¨äºæ‘˜è¦å‰ä¿å­˜å®Œæ•´æ•°æ®ï¼‰
        
        Returns:
            å½’æ¡£æ–‡ä»¶è·¯å¾„
        """
        archive_dir = self.storage_path / "archives"
        archive_dir.mkdir(exist_ok=True)
        
        archive_num = len(self.archives) + 1
        filename = f"{self.session_id}_archive_{archive_num:03d}.json"
        filepath = archive_dir / filename
        
        # æ¢å¤ç´§å‡‘æ ¼å¼çš„å®Œæ•´å†…å®¹
        archive_data = {
            "session_id": self.session_id,
            "user_id": self.user_id,
            "archived_at": datetime.now().isoformat(),
            "turns_range": {
                "start": turns[0].turn_number if turns else 0,
                "end": turns[-1].turn_number if turns else 0
            },
            "turns": []
        }
        
        for turn in turns:
            turn_data = {
                "turn_number": turn.turn_number,
                "user_query": turn.user_query,
                "intent": turn.intent,
                "topic": turn.topic,
                "artifact_id": turn.artifact_id,
                "artifact_type": turn.artifact_type,
                "timestamp": turn.timestamp
            }
            
            # ğŸ†• å¦‚æœæ˜¯ç´§å‡‘æ ¼å¼ï¼Œä»å¸è½½æ–‡ä»¶ä¸­æ¢å¤å®Œæ•´å†…å®¹
            if turn.is_compacted and turn.compact_reference:
                full_content = self._recover_turn_content(turn.compact_reference)
                if full_content:
                    turn_data["full_content"] = full_content
            else:
                turn_data["full_content"] = turn.full_content
            
            archive_data["turns"].append(turn_data)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(archive_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“¦ Archived {len(turns)} turns to {filepath}")
        return filepath
    
    def _recover_turn_content(self, reference_path: str) -> Optional[Dict[str, Any]]:
        """
        ä»å¸è½½æ–‡ä»¶ä¸­æ¢å¤è½®æ¬¡å†…å®¹ï¼ˆå®ç°å¯é€†æ€§ï¼‰
        
        Args:
            reference_path: å¸è½½æ–‡ä»¶è·¯å¾„
            
        Returns:
            å®Œæ•´å†…å®¹ï¼Œå¦‚æœæ— æ³•æ¢å¤åˆ™è¿”å› None
        """
        try:
            filepath = Path(reference_path)
            if not filepath.exists():
                logger.warning(f"âš ï¸ Offload file not found: {reference_path}")
                return None
            
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return data.get("full_content")
        except Exception as e:
            logger.error(f"âŒ Failed to recover content from {reference_path}: {e}")
            return None
    
    # ============= æ£€ç´¢æ–¹æ³• =============
    
    def retrieve_from_archive(
        self,
        query: str,
        archive_id: Optional[str] = None,
        max_results: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ğŸ†• ä»å½’æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
        
        æ”¯æŒï¼š
        - å…³é”®è¯æœç´¢
        - è½®æ¬¡èŒƒå›´æ£€ç´¢
        - ä¸»é¢˜æ£€ç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            archive_id: æŒ‡å®šå½’æ¡£æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            max_results: æœ€å¤§è¿”å›æ•°é‡
            
        Returns:
            åŒ¹é…çš„è½®æ¬¡åˆ—è¡¨
        """
        results = []
        
        # ç¡®å®šè¦æœç´¢çš„å½’æ¡£æ–‡ä»¶
        archives_to_search = []
        if archive_id and archive_id in self.archives:
            archives_to_search = [self.archives[archive_id]]
        else:
            archives_to_search = list(self.archives.values())
        
        for archive_path in archives_to_search:
            try:
                with open(archive_path, 'r', encoding='utf-8') as f:
                    archive_data = json.load(f)
                
                for turn_data in archive_data.get("turns", []):
                    # ç®€å•çš„å…³é”®è¯åŒ¹é…
                    if self._match_query(turn_data, query):
                        results.append({
                            "source": str(archive_path),
                            "turn_number": turn_data.get("turn_number"),
                            "user_query": turn_data.get("user_query"),
                            "topic": turn_data.get("topic"),
                            "intent": turn_data.get("intent"),
                            "full_content": turn_data.get("full_content")
                        })
                        
                        if len(results) >= max_results:
                            break
                            
            except Exception as e:
                logger.error(f"âŒ Failed to search archive {archive_path}: {e}")
        
        logger.info(f"ğŸ” Retrieved {len(results)} results from archives for query: {query[:50]}...")
        return results
    
    def _match_query(self, turn_data: Dict[str, Any], query: str) -> bool:
        """ç®€å•çš„å…³é”®è¯åŒ¹é…"""
        query_lower = query.lower()
        
        # æœç´¢ç”¨æˆ·é—®é¢˜
        if query_lower in turn_data.get("user_query", "").lower():
            return True
        
        # æœç´¢ä¸»é¢˜
        if turn_data.get("topic") and query_lower in turn_data["topic"].lower():
            return True
        
        # æœç´¢å†…å®¹
        content = turn_data.get("full_content")
        if content:
            content_str = json.dumps(content, ensure_ascii=False)
            if query_lower in content_str.lower():
                return True
        
        return False
    
    # ============= è¾…åŠ©æ–¹æ³• =============
    
    def _update_state(self):
        """æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€"""
        total_chars = 0
        compacted = 0
        
        for turn in self.turns:
            if turn.is_compacted:
                # ç´§å‡‘æ ¼å¼çš„ä¼°ç®—å¤§å°ï¼ˆåªæœ‰å¼•ç”¨ï¼‰
                total_chars += len(turn.user_query[:50]) + 100  # æ‘˜è¦ + å…ƒæ•°æ®
                compacted += 1
            else:
                # å®Œæ•´æ ¼å¼çš„å¤§å°
                total_chars += len(turn.user_query)
                if turn.full_content:
                    total_chars += len(json.dumps(turn.full_content, ensure_ascii=False))
        
        self.state.total_chars = total_chars
        self.state.total_tokens_estimated = int(total_chars * self.TOKEN_ESTIMATION_RATIO)
        self.state.turn_count = len(self.turns)
        self.state.compacted_turns = compacted
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ token æ•°"""
        return int(len(text) * self.TOKEN_ESTIMATION_RATIO)
    
    def _format_turn_for_context(self, turn: TurnData, full: bool = True) -> str:
        """
        æ ¼å¼åŒ–è½®æ¬¡ç”¨äº LLM ä¸Šä¸‹æ–‡
        
        Args:
            turn: è½®æ¬¡æ•°æ®
            full: æ˜¯å¦ä½¿ç”¨å®Œæ•´æ ¼å¼
        """
        if full and not turn.is_compacted:
            # å®Œæ•´æ ¼å¼
            content_preview = ""
            if turn.full_content:
                content_str = json.dumps(turn.full_content, ensure_ascii=False)
                content_preview = content_str[:500] + "..." if len(content_str) > 500 else content_str
            
            return f"""**Turn {turn.turn_number}** ({turn.intent}, topic: {turn.topic or 'N/A'})
ç”¨æˆ·: {turn.user_query}
åŠ©æ‰‹: {content_preview}"""
        else:
            # ç´§å‡‘æ ¼å¼
            return f"""**Turn {turn.turn_number}** [ç´§å‡‘] ({turn.intent}, topic: {turn.topic or 'N/A'})
ç”¨æˆ·: {turn.user_query[:100]}{'...' if len(turn.user_query) > 100 else ''}
åŠ©æ‰‹: [{turn.artifact_type or 'text'}] å†…å®¹å·²å½’æ¡£ â†’ {turn.compact_reference or 'N/A'}"""
    
    def _generate_archive_summary(self) -> str:
        """ç”Ÿæˆå½’æ¡£æ‘˜è¦å¤´éƒ¨"""
        if not self.archives:
            return ""
        
        summary = "## ğŸ“š å†å²å¯¹è¯æ‘˜è¦\n\n"
        summary += f"> æœ¬ä¼šè¯å…±æœ‰ {self.state.summarized_turns} è½®æ—§å¯¹è¯å·²å½’æ¡£ã€‚\n"
        summary += "> å¦‚éœ€æŸ¥çœ‹è¯¦ç»†å†…å®¹ï¼Œå¯ä½¿ç”¨æ£€ç´¢åŠŸèƒ½ä»å½’æ¡£ä¸­æ¢å¤ã€‚\n\n"
        
        for archive_id, archive_path in self.archives.items():
            try:
                with open(archive_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                turns_range = data.get("turns_range", {})
                summary += f"- ğŸ“¦ {archive_id}: Turn {turns_range.get('start', '?')}-{turns_range.get('end', '?')}\n"
            except:
                summary += f"- ğŸ“¦ {archive_id}: (æ— æ³•è¯»å–)\n"
        
        return summary
    
    def _generate_turns_summary(self, turns: List[TurnData]) -> str:
        """ç”Ÿæˆè½®æ¬¡æ‘˜è¦"""
        if not turns:
            return ""
        
        topics = set()
        intents = {}
        
        for turn in turns:
            if turn.topic:
                topics.add(turn.topic)
            intents[turn.intent] = intents.get(turn.intent, 0) + 1
        
        summary = f"**è½®æ¬¡ {turns[0].turn_number}-{turns[-1].turn_number}**ï¼ˆå…± {len(turns)} è½®ï¼‰\n"
        
        if topics:
            summary += f"- ğŸ“– **å­¦ä¹ ä¸»é¢˜**: {', '.join(list(topics)[:5])}\n"
        
        if intents:
            intents_str = ", ".join([f"{k}Ã—{v}" for k, v in list(intents.items())[:4]])
            summary += f"- ğŸ› ï¸ **æ„å›¾åˆ†å¸ƒ**: {intents_str}\n"
        
        return summary
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "session_id": self.session_id,
            "total_turns": self.state.turn_count,
            "compacted_turns": self.state.compacted_turns,
            "summarized_turns": self.state.summarized_turns,
            "total_chars": self.state.total_chars,
            "estimated_tokens": self.state.total_tokens_estimated,
            "archive_count": len(self.archives),
            "soft_limit": self.SOFT_LIMIT_TOKENS,
            "hard_limit": self.HARD_LIMIT_TOKENS
        }

