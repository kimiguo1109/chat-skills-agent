"""
Skill Registry - æŠ€èƒ½æ³¨å†Œè¡¨

è´Ÿè´£åŠ è½½ã€ç®¡ç†å’ŒæŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„ Skillsã€‚
ä» YAML é…ç½®æ–‡ä»¶å’Œ skill.md å…ƒæ•°æ®ä¸­åŠ è½½ Skill å®šä¹‰ã€‚

Phase 4: å®ç° 0-token æ„å›¾åŒ¹é…åŠŸèƒ½
"""
import logging
import os
import re
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import yaml

from ..models.skill import SkillDefinition
from ..config import settings

logger = logging.getLogger(__name__)


@dataclass
class SkillMatch:
    """æŠ€èƒ½åŒ¹é…ç»“æœ"""
    skill_id: str
    confidence: float
    parameters: Dict[str, Any]
    matched_keywords: List[str]


class SkillRegistry:
    """æŠ€èƒ½æ³¨å†Œè¡¨ - ç®¡ç†æ‰€æœ‰å¯ç”¨çš„ Skills"""
    
    def __init__(self, config_dir: Optional[str] = None):
        """
        åˆå§‹åŒ– Skill Registry
        
        Args:
            config_dir: Skills é…ç½®æ–‡ä»¶ç›®å½•ï¼ˆé»˜è®¤ä¸º skills_config/ï¼‰
        """
        if config_dir is None:
            # é»˜è®¤é…ç½®ç›®å½•åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ skills_config/
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            config_dir = os.path.join(base_dir, "skills_config")
            self.skills_metadata_dir = os.path.join(base_dir, "skills")
        else:
            self.skills_metadata_dir = os.path.join(os.path.dirname(config_dir), "skills")
        
        self.config_dir = config_dir
        self._skills: Dict[str, SkillDefinition] = {}
        self._intent_map: Dict[str, List[str]] = {}  # intent -> [skill_ids]
        
        # ğŸ†• Phase 4: åŠ è½½ skill.md å…ƒæ•°æ®
        self._skill_metadata: Dict[str, Dict[str, Any]] = {}  # skill_id -> metadata
        
        # åŠ è½½æ‰€æœ‰ skills
        self._load_skills()
        
        # ğŸ†• åŠ è½½ skill.md å…ƒæ•°æ®ï¼ˆç”¨äº 0-token åŒ¹é…ï¼‰
        self._load_skill_metadata()
        
        logger.info(f"âœ… SkillRegistry initialized with {len(self._skills)} skills ({len(self._skill_metadata)} with metadata)")
    
    def _load_skills(self):
        """ä»é…ç½®ç›®å½•åŠ è½½æ‰€æœ‰ Skill å®šä¹‰"""
        if not os.path.exists(self.config_dir):
            logger.warning(f"Skills config directory not found: {self.config_dir}")
            return
        
        yaml_files = [f for f in os.listdir(self.config_dir) if f.endswith('.yaml') or f.endswith('.yml')]
        
        for filename in yaml_files:
            filepath = os.path.join(self.config_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                # ä½¿ç”¨ Pydantic æ¨¡å‹éªŒè¯
                skill_def = SkillDefinition(**config)
                
                # ğŸ†• ä¿å­˜åŸå§‹é…ç½®
                skill_def.raw_config = config
                
                # æ³¨å†Œ skill
                self._skills[skill_def.id] = skill_def
                
                # å»ºç«‹ intent æ˜ å°„
                for intent_tag in skill_def.intent_tags:
                    if intent_tag not in self._intent_map:
                        self._intent_map[intent_tag] = []
                    self._intent_map[intent_tag].append(skill_def.id)
                
                logger.info(f"âœ… Loaded skill: {skill_def.id} ({skill_def.display_name})")
            
            except Exception as e:
                logger.error(f"âŒ Failed to load skill from {filename}: {e}")
    
    def get_skill(self, skill_id: str) -> Optional[SkillDefinition]:
        """
        æ ¹æ® ID è·å– Skill å®šä¹‰
        
        Args:
            skill_id: Skill ID
        
        Returns:
            SkillDefinition æˆ– Noneï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        """
        return self._skills.get(skill_id)
    
    def get_skills_by_intent(self, intent: str) -> List[SkillDefinition]:
        """
        æ ¹æ®æ„å›¾è·å–åŒ¹é…çš„ Skills
        
        Args:
            intent: ç”¨æˆ·æ„å›¾æ ‡ç­¾
        
        Returns:
            åŒ¹é…çš„ Skill å®šä¹‰åˆ—è¡¨
        """
        skill_ids = self._intent_map.get(intent, [])
        return [self._skills[sid] for sid in skill_ids if sid in self._skills]
    
    def list_all_skills(self) -> List[SkillDefinition]:
        """
        åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„ Skills
        
        Returns:
            æ‰€æœ‰ Skill å®šä¹‰çš„åˆ—è¡¨
        """
        return list(self._skills.values())
    
    def get_skill_ids(self) -> List[str]:
        """
        è·å–æ‰€æœ‰ Skill ID
        
        Returns:
            Skill ID åˆ—è¡¨
        """
        return list(self._skills.keys())
    
    def get_all_intents(self) -> List[str]:
        """
        è·å–æ‰€æœ‰æ”¯æŒçš„æ„å›¾æ ‡ç­¾
        
        Returns:
            æ„å›¾æ ‡ç­¾åˆ—è¡¨
        """
        return list(self._intent_map.keys())
    
    def validate_skill_dependencies(self, skill_id: str) -> bool:
        """
        éªŒè¯ Skill çš„ä¾èµ–æ˜¯å¦éƒ½å·²æ³¨å†Œ
        
        Args:
            skill_id: Skill ID
        
        Returns:
            True å¦‚æœæ‰€æœ‰ä¾èµ–éƒ½æ»¡è¶³ï¼Œå¦åˆ™ False
        """
        skill = self.get_skill(skill_id)
        if not skill:
            return False
        
        for dep_id in skill.dependencies:
            if dep_id not in self._skills:
                logger.warning(f"âš ï¸  Skill {skill_id} depends on {dep_id}, but it's not registered")
                return False
        
        return True
    
    def get_composable_skills(self) -> List[SkillDefinition]:
        """
        è·å–æ‰€æœ‰å¯ç»„åˆçš„ Skills
        
        Returns:
            å¯ç»„åˆçš„ Skill åˆ—è¡¨
        """
        return [skill for skill in self._skills.values() if skill.composable]
    
    # ==================== Phase 4: 0-Token Matching ====================
    
    def _load_skill_metadata(self):
        """
        ä» YAML é…ç½®åŠ è½½ skill å…ƒæ•°æ®ï¼ˆprimary_keywordsï¼‰
        ç”¨äº 0-token æ„å›¾åŒ¹é…
        
        ğŸ†• ä¼˜å…ˆä» YAML æ–‡ä»¶çš„ primary_keywords å­—æ®µè¯»å–
        """
        for skill_id, skill_def in self._skills.items():
            metadata = {
                'id': skill_id,
                'primary_keywords': [],
                'quantity_patterns': [],
                'topic_patterns': [],
                'context_patterns': []
            }
            
            # ğŸ”¥ ä» YAML é…ç½®è¯»å– primary_keywords
            if hasattr(skill_def, 'raw_config') and skill_def.raw_config:
                yaml_keywords = skill_def.raw_config.get('primary_keywords', [])
                if yaml_keywords:
                    metadata['primary_keywords'] = yaml_keywords
                    logger.debug(f"ğŸ“ Loaded {len(yaml_keywords)} keywords for {skill_id} from YAML")
            
            # å¦‚æœ YAML æ²¡æœ‰ primary_keywordsï¼Œä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤å€¼
            if not metadata['primary_keywords']:
                metadata['primary_keywords'] = self._get_default_keywords(skill_id)
                logger.debug(f"ğŸ“ Using default keywords for {skill_id}")
            
            self._skill_metadata[skill_id] = metadata
            logger.info(f"âœ… Loaded metadata for: {skill_id}")
    
    def _get_default_keywords(self, skill_id: str) -> List[str]:
        """
        è·å– skill çš„é»˜è®¤å…³é”®è¯ï¼ˆYAML æœªé…ç½®æ—¶ä½¿ç”¨ï¼‰
        
        ğŸ”¥ å…³é”®è¯é€‰æ‹©åŸåˆ™ï¼š
        1. åªåŒ…å«ã€æ˜ç¡®çš„ã€‘skill è§¦å‘è¯
        2. æ’é™¤æ—¥å¸¸å¯¹è¯å¸¸ç”¨è¯ï¼ˆå¦‚"å­¦ä¹ "ã€"ä»€ä¹ˆæ˜¯"ï¼‰
        3. è¿™äº›å®½æ³›è¯æ±‡åº”ç”± is_inquiry_message + LLM fallback å¤„ç†
        """
        default_keywords = {
            # ğŸ”¥ explain_skill: ç§»é™¤ "å­¦ä¹ "ã€"ä»€ä¹ˆæ˜¯"ã€"æ•™æˆ‘"ã€"å‘Šè¯‰æˆ‘" ç­‰å®½æ³›è¯
            # è¿™äº›è¯åœ¨å¯¹è¯ä¸­å¤ªå¸¸è§ï¼Œåº”è¯¥è®© LLM åˆ¤æ–­æ˜¯å¦æ˜¯æ¦‚å¿µè®²è§£è¯·æ±‚
            'explain_skill': ['è§£é‡Šä¸€ä¸‹', 'è¯¦ç»†è®²è§£', 'è¯¦ç»†è§£é‡Š', 'æ·±å…¥è®²è§£', 'ç³»ç»Ÿè®²è§£', 'ç§‘æ™®ä¸€ä¸‹', 
                             'è®²è®²', 'è®²ä¸€ä¸‹', 'ç®€å•è®²è®²', 'ç®€å•è®²è§£', 'ç»™æˆ‘è®²è®²', 'å¸®æˆ‘è®²è®²',
                             'explain', 'explain in detail'],
            
            # ğŸ”¥ quiz_skill: ç§»é™¤å•å­— "é¢˜"ï¼ˆå¤ªå®½æ³›ï¼Œå¦‚"é—®é¢˜"ã€"è¯é¢˜"ï¼‰
            # ä¿ç•™æ˜ç¡®çš„å‡ºé¢˜è§¦å‘è¯
            'quiz_skill': ['é“é¢˜', 'å‡ºé¢˜', 'åšé¢˜', 'åˆ·é¢˜', 'ç»ƒä¹ é¢˜', 'æµ‹éªŒ', 'æµ‹è¯•é¢˜', 'è€ƒé¢˜', 
                          'ä¹ é¢˜', 'è¯•é¢˜', 'é€‰æ‹©é¢˜', 'åˆ¤æ–­é¢˜', 'å¡«ç©ºé¢˜', 'ç®€ç­”é¢˜',  # ğŸ†• æ·»åŠ é¢˜å‹
                          'quiz', 'test questions', 'exam questions'],
            
            # flashcard_skill: è¿™äº›è¯æ¯”è¾ƒæ˜ç¡®
            'flashcard_skill': ['é—ªå¡', 'è®°å¿†å¡', 'æŠ½è®¤å¡', 'èƒŒè¯µå¡', 'å¤ä¹ å¡', 'å•è¯å¡', 'å¡ç‰‡',
                               'ç”Ÿæˆé—ªå¡', 'åšé—ªå¡', 'åˆ¶ä½œå¡', 
                               'flashcard', 'flash card', 'anki'],
            
            # notes_skill: ç§»é™¤ "æ€»ç»“"ï¼ˆå¤ªå®½æ³›ï¼‰
            'notes_skill': ['åšç¬”è®°', 'æ•´ç†ç¬”è®°', 'å­¦ä¹ ç¬”è®°', 'è¯¾å ‚ç¬”è®°', 'notes', 'take notes'],
            
            # mindmap_skill: è¿™äº›è¯æ¯”è¾ƒæ˜ç¡®
            'mindmap_skill': ['æ€ç»´å¯¼å›¾', 'çŸ¥è¯†å¯¼å›¾', 'è„‘å›¾', 'çŸ¥è¯†å›¾è°±', 'æ¦‚å¿µå›¾', 'ç»“æ„å›¾', 
                             'mindmap', 'mind map'],
            
            # learning_plan_skill: å­¦ä¹ è®¡åˆ’/è§„åˆ’
            'learning_plan_skill': ['å­¦ä¹ è®¡åˆ’', 'å­¦ä¹ è§„åˆ’', 'å¤ä¹ è®¡åˆ’', 'å­¦ä¹ æ–¹æ¡ˆ', 'å­¦ä¹ è·¯çº¿',
                                   'åˆ¶å®šè®¡åˆ’', 'åˆ¶å®šæ–¹æ¡ˆ', 'è§„åˆ’ä¸€ä¸‹', 'å®‰æ’ä¸€ä¸‹å­¦ä¹ ',
                                   'å¸®æˆ‘åˆ¶å®š', 'ç»™æˆ‘åˆ¶å®š', 'å¸®æˆ‘è§„åˆ’', 'ç»™æˆ‘è§„åˆ’',
                                   'learning plan', 'study plan', 'make a plan'],
            
            # learning_bundle_skill: å­¦ä¹ åŒ…ï¼ˆç»¼åˆï¼‰
            'learning_bundle_skill': ['å­¦ä¹ åŒ…', 'å­¦ä¹ å¥—è£…', 'å­¦ä¹ å¥—é¤', 'å…¨å¥—èµ„æ–™', 'å­¦ä¹ ææ–™åŒ…',
                                     'learning bundle', 'study pack', 'learning package']
        }
        return default_keywords.get(skill_id, [])
    
    def _parse_skill_md(self, filepath: str) -> Dict[str, Any]:
        """
        è§£æ skill.md æ–‡ä»¶ï¼Œæå–æ„å›¾è§¦å‘è§„åˆ™
        
        Returns:
            metadata dict with:
                - id: skill_id
                - primary_keywords: List[str]
                - quantity_patterns: List[str]
                - topic_patterns: List[str]
                - context_patterns: List[str]
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        metadata = {}
        
        # æå– Skill ID
        id_match = re.search(r'\*\*æŠ€èƒ½ID\*\*:\s*`(.+?)`', content)
        if id_match:
            metadata['id'] = id_match.group(1)
        
        # æå– Primary Keywords
        keywords_section = re.search(
            r'### Primary Keywords.*?```\n(.*?)\n```',
            content,
            re.DOTALL
        )
        if keywords_section:
            keywords_text = keywords_section.group(1).strip()
            # åˆ†å‰²å¹¶æ¸…ç†å…³é”®è¯ï¼ˆæ”¯æŒé€—å·åˆ†éš”ï¼‰
            keywords = [kw.strip() for kw in re.split(r'[,ï¼Œ\s]+', keywords_text) if kw.strip()]
            metadata['primary_keywords'] = keywords
        else:
            metadata['primary_keywords'] = []
        
        # æå– Quantity Patterns
        quantity_section = re.search(
            r'### Quantity Patterns.*?```(?:regex)?\n(.*?)\n```',
            content,
            re.DOTALL
        )
        if quantity_section:
            patterns_text = quantity_section.group(1).strip()
            patterns = [p.strip() for p in patterns_text.split('\n') if p.strip() and not p.strip().startswith('_N/A')]
            metadata['quantity_patterns'] = patterns
        else:
            metadata['quantity_patterns'] = []
        
        # æå– Topic Patterns
        topic_section = re.search(
            r'### Topic Patterns.*?```(?:regex)?\n(.*?)\n```',
            content,
            re.DOTALL
        )
        if topic_section:
            patterns_text = topic_section.group(1).strip()
            patterns = [p.strip() for p in patterns_text.split('\n') if p.strip()]
            metadata['topic_patterns'] = patterns
        else:
            metadata['topic_patterns'] = []
        
        # æå– Context Patterns
        context_section = re.search(
            r'### Context Patterns.*?```\n(.*?)\n```',
            content,
            re.DOTALL
        )
        if context_section:
            patterns_text = context_section.group(1).strip()
            patterns = [p.strip() for p in patterns_text.split('\n') if p.strip()]
            metadata['context_patterns'] = patterns
        else:
            metadata['context_patterns'] = []
        
        return metadata
    
    def is_inquiry_message(self, message: str) -> bool:
        """
        ğŸ†• æ£€æµ‹æ˜¯å¦ä¸º"è¯¢é—®ç±»"æˆ–"å¯¹è¯ç±»"æ¶ˆæ¯ï¼ˆåº”è¯¥è¿”å›çº¯ Chat è€Œéè§¦å‘æŠ€èƒ½ï¼‰
        
        å¯¹è¯ç±»æ¶ˆæ¯ç‰¹å¾ï¼š
        - é—®å€™/é—²èŠï¼ˆ"ä½ å¥½"ã€"è°¢è°¢"ï¼‰
        - ç®€å•è¿½é—®ï¼ˆ"è®²è®²"ã€"ç»§ç»­"ã€"ç„¶åå‘¢"ï¼‰
        - å­¦ä¹ è®¨è®ºï¼ˆ"æˆ‘æƒ³å­¦ä¹ X"ã€"æ•™æˆ‘Y"ï¼‰- ä¸æ˜¯æ˜ç¡®çš„"å‡ºé¢˜/ç”Ÿæˆé—ªå¡"
        
        è¯¢é—®ç±»æ¶ˆæ¯ç‰¹å¾ï¼š
        - è¯¢é—®å†…å®¹/æè¿°ï¼ˆ"è¿™æ˜¯ä»€ä¹ˆ"ã€"è®²äº†ä»€ä¹ˆ"ï¼‰
        - è¯·æ±‚è§£ç­”/è§£é‡Šï¼ˆ"å¸®æˆ‘è§£ç­”"ã€"æ€ä¹ˆåš"ï¼‰
        - æ¯”è¾ƒ/åˆ†æï¼ˆ"æ¯”è¾ƒä¸€ä¸‹"ã€"æœ‰ä»€ä¹ˆä¸åŒ"ï¼‰
        - è¿½é—®/æ¾„æ¸…ï¼ˆ"ä¸å¤ªæ‡‚"ã€"èƒ½è¯¦ç»†è¯´å—"ï¼‰
        
        è¿™äº›æ¶ˆæ¯åº”è¯¥ç”± LLM ç›´æ¥å›ç­”ï¼Œè€Œä¸æ˜¯ç”Ÿæˆ Quiz/Flashcard
        """
        message_lower = message.lower()
        
        # ğŸ†• Step 1: æ£€æµ‹æ˜¯å¦æœ‰æ˜ç¡®çš„ skill è§¦å‘è¯ï¼ˆå¦‚æœæœ‰ï¼Œä¸ç®—è¯¢é—®ç±»ï¼‰
        explicit_skill_triggers = [
            # Quiz æ˜ç¡®è§¦å‘
            r'\d+\s*é“é¢˜',        # "ä¸‰é“é¢˜"ã€"5é“é¢˜"
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤]\s*é“é¢˜',
            r'å‡º\s*\d*\s*[é“ä¸ª]?é¢˜',  # "å‡ºé¢˜"ã€"å‡ºä¸‰é“é¢˜"
            r'åš\s*\d*\s*[é“ä¸ª]?é¢˜',  # "åšé¢˜"
            r'åˆ·é¢˜',
            r'ç»ƒä¹ é¢˜',
            r'æµ‹éªŒ',
            r'è€ƒ[è¯•é¢˜]',
            
            # Flashcard æ˜ç¡®è§¦å‘
            r'\d+\s*å¼ .*?[é—ªå¡ç‰‡]',   # "ä¸‰å¼ é—ªå¡"
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤]\s*å¼ .*?[é—ªå¡ç‰‡]',
            r'(åš|ç”Ÿæˆ|åˆ¶ä½œ|åˆ›å»º).*?é—ªå¡',
            r'(åš|ç”Ÿæˆ|åˆ¶ä½œ|åˆ›å»º).*?å¡ç‰‡',
            
            # Mindmap æ˜ç¡®è§¦å‘
            r'(ç”»|ç”Ÿæˆ|åˆ¶ä½œ|åš).*?(æ€ç»´å¯¼å›¾|å¯¼å›¾|è„‘å›¾)',
            
            # Notes æ˜ç¡®è§¦å‘
            r'(åš|æ•´ç†|å†™).*?ç¬”è®°',
            r'(æ€»ç»“|å½’çº³).*?(è¦ç‚¹|å†…å®¹)',
            
            # Learning Bundle æ˜ç¡®è§¦å‘
            r'å­¦ä¹ åŒ…',
            r'å­¦ä¹ å¥—[è£…é¤]',
            r'(ç»™æˆ‘|æ¥ä¸ª).*?å…¨å¥—',
            
            # ğŸ†• Plan æ˜ç¡®è§¦å‘
            r'åˆ¶å®š.*?(è®¡åˆ’|æ–¹æ¡ˆ|è§„åˆ’)',  # "åˆ¶å®šå­¦ä¹ è®¡åˆ’"
            r'è§„åˆ’.*(å­¦ä¹ |å¤ä¹ |å¤‡è€ƒ)',  # "è§„åˆ’ä¸€ä¸‹å­¦ä¹ "
            r'(å­¦ä¹ |å¤ä¹ |å¤‡è€ƒ).*(è®¡åˆ’|æ–¹æ¡ˆ|è§„åˆ’|è·¯çº¿)',  # "å­¦ä¹ è®¡åˆ’"
        ]
        
        for pattern in explicit_skill_triggers:
            if re.search(pattern, message_lower, re.IGNORECASE):
                logger.info(f"ğŸ¯ Explicit skill trigger detected: '{pattern}', NOT inquiry")
                return False  # æœ‰æ˜ç¡®è§¦å‘è¯ï¼Œä¸æ˜¯è¯¢é—®ç±»
        
        # ğŸ†• Step 2: æ£€æµ‹å¯¹è¯/è¯¢é—®ç±»æ¨¡å¼
        inquiry_patterns = [
            # ğŸ†• é—®å€™/é—²èŠï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
            r'^(ä½ å¥½|æ‚¨å¥½|hi|hello|hey)',
            r'(è°¢è°¢|æ„Ÿè°¢|å¤šè°¢|thanks)',
            r'^(å¥½çš„|è¡Œ|å¯ä»¥|æ²¡é—®é¢˜|OK)',
            
            # ğŸ†• ç®€å•å­¦ä¹ è¯·æ±‚ï¼ˆä¸æ˜¯ç”Ÿæˆç±»ï¼‰- ç§»é™¤ ^ é™åˆ¶
            r'æˆ‘æƒ³(å­¦ä¹ ?|äº†è§£|çŸ¥é“)',
            r'æˆ‘ä»¬(æ¥|ä¸€èµ·)?(å­¦ä¹ |äº†è§£|çœ‹çœ‹)',  # "æˆ‘ä»¬æ¥å­¦ä¹ X"ã€"æˆ‘ä»¬ä¸€èµ·å­¦ä¹ "
            r'(æ•™æˆ‘|å‘Šè¯‰æˆ‘)',
            r'(å…ˆ|å†)?(è®²è®²|è¯´è¯´|èŠèŠ)',  # åŒ¹é… "å…ˆè®²è®²"ã€"è®²è®²"
            r'è®²ä¸€?ä¸‹',  # "è®²ä¸‹"ã€"è®²ä¸€ä¸‹"
            
            # ğŸ†• è¿½é—®/ç»§ç»­ï¼ˆç§»é™¤ ^ é™åˆ¶ï¼‰
            r'(ç»§ç»­|ç„¶åå‘¢|æ¥ç€|è¿˜æœ‰å‘¢)',
            r'(å†|ç»§ç»­)(è®²|è¯´|è§£é‡Š)',
            r'(èƒ½|å¯ä»¥).*?(ä¸¾.*?ä¾‹|è¯¦ç»†.*?è¯´)',
            r'èƒ½.*?å—[ï¼Ÿ?]?$',  # "èƒ½ä¸¾ä¸ªä¾‹å­å—"
            
            # è¯¢é—®å†…å®¹/æè¿°
            r'(è¿™|é‚£|æ–‡ä»¶|å›¾ç‰‡|ææ–™)[^çš„]*?(è®²|è¯´|æ˜¯|æè¿°|ä»‹ç»)[çš„äº†]?(ä»€ä¹ˆ|å†…å®¹)',
            r'(ä»€ä¹ˆ|å“ªäº›?)[^çš„]*?(å†…å®¹|ä¸»é¢˜|çŸ¥è¯†ç‚¹)',
            
            # è¯·æ±‚è§£ç­”/è§£é‡Šï¼ˆä¸æ˜¯"å¸®æˆ‘å‡ºé¢˜"ï¼‰
            r'å¸®æˆ‘(è§£ç­”|è§£å†³|åš|ç®—|ç®—å‡º)',
            r'(æ€ä¹ˆ|å¦‚ä½•)(åš|è§£|ç®—|æ±‚|è¯æ˜|ç”¨|ä½¿ç”¨)',
            r'(è¿™é“|é‚£é“)é¢˜[^å‡ºç”Ÿæˆ]*?(æ€ä¹ˆ|å¦‚ä½•|ç­”æ¡ˆ)',
            r'æ€ä¹ˆç”¨',  # "è¿™ä¸ªå…¬å¼æ€ä¹ˆç”¨"
            
            # æ¯”è¾ƒ/åˆ†æ
            r'æ¯”è¾ƒä¸€ä¸‹|å¯¹æ¯”ä¸€ä¸‹|æœ‰ä»€ä¹ˆä¸åŒ|æœ‰ä»€ä¹ˆåŒºåˆ«|æœ‰ä»€ä¹ˆè”ç³»|æœ‰ä»€ä¹ˆå…³ç³»|æœ‰ä»€ä¹ˆå½±å“|æœ‰ä»€ä¹ˆä½œç”¨',
            r'(è¿™|é‚£)(ä¸¤ä¸ª?|å‡ ä¸ª?|äº›)[^å‡ºç”Ÿæˆ]*?(æ¯”è¾ƒ|ä¸åŒ|åŒºåˆ«|è”ç³»)',
            
            # è¿½é—®/æ¾„æ¸…
            r'(ä¸å¤ª?|æ²¡å¤ª?|è¿˜æ˜¯ä¸)(æ‡‚|ç†è§£|æ˜ç™½|æ¸…æ¥š)',  # "ä¸å¤ªæ‡‚"ã€"æ²¡å¤ªæ‡‚"
            r'èƒ½[^å‡ºç”Ÿæˆ]*?(è¯¦ç»†|ç®€å•|å†)[^å‡ºç”Ÿæˆ]*?(è¯´|è®²|è§£é‡Š)',
            r'(å†|æ›´)(è¯¦ç»†|ç®€å•|å…·ä½“)ä¸€ç‚¹',
            r'ä¸¾ä¸ª?(ä¾‹å­|ä¾‹|æ —å­)',
            
            # è¯·æ±‚æç¤º
            r'ç»™[æˆ‘ä¸€]?[äº›ç‚¹ä¸ª]?(æç¤º|çº¿ç´¢|æ€è·¯|æ–¹å‘)',
            r'(æœ‰|ç»™)[^å‡ºç”Ÿæˆ]*?(æç¤º|çº¿ç´¢|æ€è·¯)',
            
            # ğŸ†• æ¦‚å¿µè¯¢é—®ï¼ˆ"ä»€ä¹ˆæ˜¯X" ä½†ä¸æ˜¯ "ä»€ä¹ˆæ˜¯Xï¼Œå‡ºä¸‰é“é¢˜"ï¼‰
            r'^ä»€ä¹ˆæ˜¯[^ï¼Œ,ã€‚.ï¼Ÿ?!ï¼]*$',  # å•ç‹¬çš„"ä»€ä¹ˆæ˜¯X"
            r'^.*?æ˜¯ä»€ä¹ˆ[ï¼Ÿ?]?$',  # "Xæ˜¯ä»€ä¹ˆï¼Ÿ"
            r'æ˜¯.{0,10}å‘¢[ï¼Ÿ?]?$',  # "é‚£ç»†èƒå‘¼å¸æ˜¯ä»€ä¹ˆå‘¢"
            
            # ğŸ†• ç®€å•äº‹å®æ€§é—®é¢˜ï¼ˆä¸éœ€è¦è¯¦ç»†è®²è§£ï¼‰
            r'.{2,10}æ˜¯(å“ª|ä»€ä¹ˆ|å¤šå°‘|å‡ ).{0,2}(å¹´|æ—¶å€™|ä¸ª|ç§)',  # "Xæ˜¯å“ªä¸€å¹´"ã€"Xæ˜¯ä»€ä¹ˆæ—¶å€™"
            r'(æœ‰å“ªäº›|æœ‰å‡ ä¸ª|æœ‰å¤šå°‘|å“ªäº›)',  # "æœ‰å“ªäº›å›½å®¶"ã€"å“ªäº›å›½å®¶å‚æˆ˜"
            r'.{2,10}(å¼€å§‹|ç»“æŸ|å‘ç”Ÿ)(äº|åœ¨|çš„)',  # "Xå¼€å§‹äº"ã€"Xå‘ç”Ÿåœ¨"
            
            # ğŸ†• ç®€çŸ­è¿½é—®ï¼ˆ"Xå‘¢"ã€"é‚£Yå‘¢"ï¼‰
            r'^.{2,15}å‘¢[ï¼Ÿ?]?$',  # "ç¬¬ä¸‰å®šå¾‹å‘¢"ã€"åŒ–å­¦é”®å‘¢"ã€"é‚£ä¸ªå‘¢"
            r'^é‚£.{1,10}å‘¢[ï¼Ÿ?]?$',  # "é‚£ç»†èƒå‘¼å¸å‘¢"
            
            # ğŸ†• å›å¿†/æ£€ç´¢ç±»ï¼ˆä¸æ˜¯å‡ºé¢˜ï¼ï¼‰
            r'å›åˆ°.*(å¼€å§‹|æœ€åˆ|æœ€æ—©|ä¹‹å‰)',  # "å›åˆ°æœ€å¼€å§‹"
            r'æˆ‘ä»¬(èŠ|è®²|è¯´|å­¦|è®¨è®º)äº†(ä»€ä¹ˆ|å“ªäº›|å‡ ä¸ª)',  # "æˆ‘ä»¬èŠäº†ä»€ä¹ˆ"
            r'(ä»Šå¤©|åˆšæ‰|ä¹‹å‰).*(å­¦|èŠ|è®²|è®¨è®º)äº†.*(ä»€ä¹ˆ|å“ªäº›|å‡ ä¸ª|å†…å®¹)',  # "ä»Šå¤©å­¦äº†ä»€ä¹ˆ"
            r'(ä¸€å…±|æ€»å…±).*(å­¦|èŠ|è®²)äº†.*(å‡ |å¤šå°‘|ä»€ä¹ˆ)',  # "ä¸€å…±å­¦äº†å‡ ä¸ªä¸»é¢˜"
            r'(æ–‡æ¡£|æ–‡ä»¶|ææ–™|å›¾ç‰‡).*(æåˆ°|è¯´|è®²|ä»‹ç»|æè¿°)äº†?.*(ä»€ä¹ˆ|å“ªäº›|å†…å®¹)',  # "æ–‡æ¡£ä¸­æåˆ°äº†ä»€ä¹ˆ"
            r'(æ–‡æ¡£|æ–‡ä»¶).*(ä¸­|é‡Œ).*(é‡è¦|ä¸»è¦|å…³é”®)',  # "æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›é‡è¦äº‹ä»¶"
            
            # ğŸ†• æ€»ç»“/å›é¡¾ç±»ï¼ˆåŒºåˆ†äº notes skill çš„"æ€»ç»“çŸ¥è¯†ç‚¹"ï¼‰
            r'(å¸®æˆ‘|ç»™æˆ‘)?åš.*(å®Œæ•´|å…¨é¢).*æ€»ç»“',  # "åšä¸€ä¸ªå®Œæ•´çš„å­¦ä¹ æ€»ç»“"
            r'(å›é¡¾|å¤ç›˜).*(ä»Šå¤©|åˆšæ‰|ä¹‹å‰)',  # "å›é¡¾ä¸€ä¸‹ä»Šå¤©"
            
            # ğŸ†• å¸¸è§é”™è¯¯/é—®é¢˜è®¨è®ºç±»ï¼ˆå¿«æ·æŒ‰é’® common_mistakesï¼‰
            # "å­¦ç”Ÿé€šå¸¸åœ¨è¿™ç±»é—®é¢˜ä¸­çŠ¯ä»€ä¹ˆé”™è¯¯" - è¿™æ˜¯è®¨è®ºï¼Œä¸æ˜¯å‡ºé¢˜
            r'(è¿™ç±»|è¿™ç§|è¿™ä¸ª|è¿™äº›|æ­¤ç±»)é—®é¢˜',  # "è¿™ç±»é—®é¢˜"ã€"è¿™ä¸ªé—®é¢˜"
            r'çŠ¯(ä»€ä¹ˆ|å“ªäº›)(é”™è¯¯?|é”™|mistake)',  # "çŠ¯ä»€ä¹ˆé”™è¯¯"
            r'(å¸¸è§|å®¹æ˜“|ç»å¸¸|é€šå¸¸).*(é”™è¯¯|é”™|mistake|error)',  # "å¸¸è§é”™è¯¯"
            r'(é”™è¯¯|è¯¯åŒº|é™·é˜±|å‘).*(æœ‰å“ªäº›|æ˜¯ä»€ä¹ˆ)',  # "å¸¸è§è¯¯åŒºæœ‰å“ªäº›"
            r'(å­¦ç”Ÿ|åŒå­¦|å¤§å®¶|äººä»¬?).*(çŠ¯|å‡º).*(é”™|é”™è¯¯)',  # "å­¦ç”Ÿå¸¸çŠ¯çš„é”™è¯¯"
            r'common.*(mistake|error)',  # "common mistakes"
            r'(mistake|error).*(make|do)',  # "mistakes students make"
        ]
        
        for pattern in inquiry_patterns:
            if re.search(pattern, message_lower, re.IGNORECASE):
                logger.info(f"ğŸ” Detected inquiry/conversation message: pattern='{pattern}'")
                return True
        
        return False
    
    def match_message(
        self, 
        message: str, 
        current_topic: Optional[str] = None,
        session_topics: Optional[List[str]] = None,
        has_files: bool = False
    ) -> Optional[SkillMatch]:
        """
        åŒ¹é…ç”¨æˆ·æ¶ˆæ¯åˆ°æŠ€èƒ½ï¼ˆ0 tokensï¼‰
        
        æ ¸å¿ƒæ–¹æ³•ï¼šå®ç° Phase 4 çš„ 0-token æ„å›¾è¯†åˆ«
        
        åŒ¹é…ä¼˜å…ˆçº§ï¼š
        1. Plan Skill ç‰¹æ®Šæ¨¡å¼ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        2. ğŸ†• è¯­ä¹‰åŒ¹é…ï¼ˆEmbeddingï¼Œé«˜ç½®ä¿¡åº¦æ—¶ä½¿ç”¨ï¼‰
        3. è¯¢é—®ç±»æ¶ˆæ¯æ£€æµ‹
        4. å…³é”®è¯åŒ¹é…ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            current_topic: å½“å‰å¯¹è¯ä¸»é¢˜ï¼ˆä» session_context è·å–ï¼‰
            session_topics: å†å²topicsåˆ—è¡¨ï¼ˆä» session_contextï¼‰
            has_files: æ˜¯å¦æœ‰æ–‡ä»¶é™„ä»¶
        
        Returns:
            SkillMatch æˆ– Noneï¼ˆæœªåŒ¹é…ï¼‰
        """
        if not self._skill_metadata:
            logger.warning("âš ï¸ No skill metadata loaded, falling back to LLM")
            return None
        
        # ğŸ†• Step 0: å…ˆæ£€æŸ¥ Plan Skill çš„ç‰¹æ®Šæ¨¡å¼ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        # "å¸®æˆ‘åˆ¶å®šä¸€ä¸ªå­¦ä¹ ç‰©ç†çš„è®¡åˆ’" è¿™ç§æ¨¡å¼éœ€è¦ç‰¹æ®Šå¤„ç†
        plan_match = self._check_plan_skill_patterns(message, current_topic)
        if plan_match:
            logger.info(f"ğŸ“‹ Plan skill pattern matched: {plan_match.matched_keywords}")
            return plan_match
        
        # ğŸ†• Step 0.5: å°è¯•è¯­ä¹‰åŒ¹é…ï¼ˆEmbeddingï¼‰
        # é«˜ç½®ä¿¡åº¦æ—¶ç›´æ¥ä½¿ç”¨ï¼Œé¿å…å…³é”®è¯è¯¯åŒ¹é…
        semantic_match = self._try_semantic_match(message, current_topic)
        if semantic_match:
            return semantic_match
        
        # ğŸ†• Step 1: æ£€æµ‹è¯¢é—®ç±»/å¯¹è¯ç±»æ¶ˆæ¯
        # å¯¹è¯ç±»æ¶ˆæ¯åº”è¯¥ç›´æ¥è¿”å› "other" intentï¼Œä¸èµ° LLM fallback
        is_inquiry = self.is_inquiry_message(message)
        if is_inquiry:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¯¹è¯ç±»æ¶ˆæ¯ç›´æ¥è¿”å› "other" skillï¼Œä¸èµ° LLM
            # è¿™æ ·å¯ä»¥é¿å… LLM é”™è¯¯åœ°å°†å¯¹è¯è¯†åˆ«ä¸º quiz/flashcard
            # ğŸ†• å³ä½¿æœ‰æ–‡ä»¶é™„ä»¶ï¼ˆå¦‚"å¸®æˆ‘è§£ç­”è¿™é“å‡ ä½•é¢˜"ï¼‰ï¼Œä¹Ÿåº”è¯¥è¿”å› other è®© LLM å¤„ç†
            logger.info(f"ğŸ’¬ Conversation message detected (has_files={has_files}), returning 'other' skill directly")
            return SkillMatch(
                skill_id="other",  # ç›´æ¥æ ‡è®°ä¸ºå¯¹è¯
                confidence=0.95,   # é«˜ç½®ä¿¡åº¦
                parameters={
                    'topic': current_topic,  # ç»§æ‰¿å½“å‰ä¸»é¢˜
                    'is_conversation': True,
                    'has_files': has_files  # ğŸ†• ä¼ é€’æ–‡ä»¶æ ‡è®°
                },
                matched_keywords=['conversation']
            )
        
        # ğŸ†• Step 0.5: å…ˆæ¸…ç†å¼•ç”¨æ¨¡å¼ï¼Œé¿å…æŠŠå¼•ç”¨å½“ä½œå…³é”®è¯
        message_for_matching = self._clean_reference_patterns(message)
        
        # ğŸ†• Phase 4.1: å…ˆæ£€æµ‹æ··åˆæ„å›¾ï¼ˆä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯ï¼‰
        mixed_match = self._detect_mixed_intent(message, current_topic)
        if mixed_match:
            logger.info(f"ğŸ”€ Detected mixed intent, matched to: {mixed_match.skill_id}")
            return mixed_match
        
        best_match: Optional[SkillMatch] = None
        best_confidence = 0.0
        
        # éå†æ‰€æœ‰æŠ€èƒ½ï¼Œè®¡ç®—åŒ¹é…åº¦ï¼ˆä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯ï¼‰
        for skill_id, metadata in self._skill_metadata.items():
            # æ£€æŸ¥ä¸»è¦å…³é”®è¯ï¼ˆä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯ï¼‰
            matched_keywords = self._check_keywords(message_for_matching, metadata.get('primary_keywords', []))
            if not matched_keywords:
                continue  # æ²¡æœ‰åŒ¹é…å…³é”®è¯ï¼Œè·³è¿‡
            
            # æå–å‚æ•°ï¼ˆä¼ é€’ current_topic å’Œ session_topicsï¼‰
            parameters = self._extract_parameters(message, metadata, skill_id, current_topic, session_topics)
            
            # ğŸ”¥ å¦‚æœå‚æ•°ä¸­æ ‡è®°éœ€è¦ clarificationï¼Œç«‹å³è¿”å›
            if parameters.get('needs_clarification'):
                return SkillMatch(
                    skill_id="clarification_needed",
                    confidence=1.0,
                    parameters=parameters,
                    matched_keywords=['clarification']
                )
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(
                message,
                metadata,
                matched_keywords,
                parameters
            )
            
            # æ›´æ–°æœ€ä½³åŒ¹é…
            if confidence > best_confidence:
                best_confidence = confidence
                best_match = SkillMatch(
                    skill_id=skill_id,
                    confidence=confidence,
                    parameters=parameters,
                    matched_keywords=matched_keywords
                )
        
        # ğŸ†• è¯¢é—®ç±»æ¶ˆæ¯ç‰¹æ®Šå¤„ç†ï¼šé™ä½ç½®ä¿¡åº¦ï¼Œè®© LLM fallback è¿›è¡Œæ„å›¾åˆ†æ
        is_inquiry = self.is_inquiry_message(message)
        if is_inquiry and best_match:
            # è¯¢é—®ç±»æ¶ˆæ¯ï¼šå¼ºåˆ¶é™ä½ç½®ä¿¡åº¦åˆ° 0.5ï¼Œè§¦å‘ LLM fallback
            original_confidence = best_match.confidence
            best_match = SkillMatch(
                skill_id=best_match.skill_id,
                confidence=min(0.5, best_match.confidence),  # æœ€é«˜ 0.5
                parameters=best_match.parameters,
                matched_keywords=best_match.matched_keywords
            )
            logger.info(f"ğŸ“ Inquiry message: lowered confidence {original_confidence:.2f} â†’ {best_match.confidence:.2f}")
        
        # åªè¿”å›ç½®ä¿¡åº¦ >= 0.7 çš„åŒ¹é…
        if best_match and best_match.confidence >= 0.7:
            logger.info(f"âœ… Matched skill: {best_match.skill_id} (confidence: {best_match.confidence:.2f})")
            return best_match
        
        # è¿”å›ä½ç½®ä¿¡åº¦åŒ¹é…ï¼ˆè®© Intent Router å†³å®šæ˜¯å¦ä½¿ç”¨ LLM fallbackï¼‰
        if best_match and best_match.confidence > 0:
            logger.info(f"âš ï¸ Low confidence match: {best_match.skill_id} ({best_match.confidence:.2f}), suggesting LLM fallback")
            return best_match
        
        logger.debug(f"âš ï¸ No confident match found (best: {best_confidence:.2f})")
        return None
    
    def _try_semantic_match(
        self, 
        message: str, 
        current_topic: Optional[str] = None
    ) -> Optional[SkillMatch]:
        """
        ğŸ†• å°è¯•ä½¿ç”¨è¯­ä¹‰åŒ¹é…ï¼ˆEmbeddingï¼‰
        
        ä½¿ç”¨ Sentence Transformer è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ã€‚
        
        ğŸ”¥ ä¸¥æ ¼åŒ¹é…ç­–ç•¥ï¼ˆæ”¯æŒ30+è¯­è¨€ï¼‰ï¼š
        1. åªæœ‰åœ¨éå¸¸ç¡®å®šæ—¶æ‰è¿”å›åŒ¹é…ç»“æœ
        2. å¯¹äºç”Ÿæˆç±»æŠ€èƒ½ï¼ˆquiz/flashcard/explainç­‰ï¼‰ï¼Œéœ€è¦æ›´é«˜ç½®ä¿¡åº¦
        3. ä¸ç¡®å®šæ—¶è¿”å› Noneï¼Œè®© LLM å¤„ç†ï¼ˆä½œä¸º other æ„å›¾ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            current_topic: å½“å‰ä¸»é¢˜
            
        Returns:
            SkillMatch æˆ– Noneï¼ˆä¸ç¡®å®šæ—¶è¿”å› Noneï¼Œäº¤ç»™ LLM å¤„ç†ï¼‰
        """
        try:
            from .semantic_skill_matcher import get_semantic_matcher
            
            matcher = get_semantic_matcher()
            if matcher is None:
                return None
            
            # ğŸ†• ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼
            # - threshold=0.65: åŸºç¡€é˜ˆå€¼
            # - negative_threshold=0.6: æ›´å®¹æ˜“æ’é™¤
            # - confidence_gap=0.15: è¦æ±‚æ˜æ˜¾å·®è·
            semantic_result = matcher.match(
                message, 
                threshold=0.65, 
                negative_threshold=0.6,
                confidence_gap=0.15
            )
            
            if semantic_result is None:
                # ğŸ†• è¯­ä¹‰åŒ¹é…ä¸ç¡®å®šï¼Œè¿”å› None è®©åç»­é€»è¾‘å¤„ç†
                logger.info(f"ğŸ§  Semantic match: uncertain, deferring to other methods")
                return None
            
            # å°†è¯­ä¹‰åŒ¹é…ç»“æœè½¬æ¢ä¸º SkillMatch
            skill_id = semantic_result.skill_id
            confidence = semantic_result.confidence
            
            # æŠ€èƒ½ ID æ˜ å°„
            skill_id_mapping = {
                "quiz": "quiz_skill",
                "flashcard": "flashcard_skill",
                "explain": "explain_skill",
                "notes": "notes_skill",
                "mindmap": "mindmap_skill",
                "learning_bundle": "learning_plan_skill",
                "other": "other",
            }
            
            mapped_skill_id = skill_id_mapping.get(skill_id, skill_id)
            
            # ğŸ†• æ›´ä¸¥æ ¼çš„ç½®ä¿¡åº¦æ£€æŸ¥
            # ç”Ÿæˆç±»æŠ€èƒ½éœ€è¦æ›´é«˜ç½®ä¿¡åº¦ï¼ˆ0.80ï¼‰ï¼Œother å¯ä»¥ç¨ä½ï¼ˆ0.70ï¼‰
            generation_skills = {"quiz_skill", "flashcard_skill", "explain_skill", "notes_skill", "mindmap_skill", "learning_plan_skill"}
            
            if mapped_skill_id in generation_skills:
                # ğŸ”¥ ç”Ÿæˆç±»æŠ€èƒ½éœ€è¦ 0.80+ çš„ç½®ä¿¡åº¦
                if confidence >= 0.80:
                    logger.info(f"ğŸ§  Semantic match (high confidence): {mapped_skill_id} ({confidence:.3f})")
                    return SkillMatch(
                        skill_id=mapped_skill_id,
                        confidence=confidence,
                        parameters={"topic": current_topic} if current_topic else {},
                        matched_keywords=["[semantic]"],
                    )
                else:
                    # ç½®ä¿¡åº¦ä¸å¤Ÿï¼Œä¸ä½¿ç”¨è¯­ä¹‰åŒ¹é…ç»“æœ
                    logger.info(f"ğŸ§  Semantic match (insufficient for generation): {mapped_skill_id} ({confidence:.3f}) < 0.80")
                    return None
            elif mapped_skill_id == "other":
                # ğŸ†• other æ„å›¾å¯ä»¥ç¨ä½ç½®ä¿¡åº¦ï¼ˆ0.65ï¼‰- é™ä½é˜ˆå€¼ï¼Œè®©æ›´å¤šå¯¹è¯è¢«æ­£ç¡®è¯†åˆ«
                if confidence >= 0.65:
                    logger.info(f"ğŸ§  Semantic match: {mapped_skill_id} ({confidence:.3f})")
                    return SkillMatch(
                        skill_id=mapped_skill_id,
                        confidence=confidence,
                        parameters={},
                        matched_keywords=["[semantic]"],
                    )
            
            # é»˜è®¤ä¸è¿”å›åŒ¹é…
            logger.info(f"ğŸ§  Semantic match (deferred): {mapped_skill_id} ({confidence:.3f})")
            return None
            
        except ImportError:
            # sentence-transformers æœªå®‰è£…
            logger.debug("âš ï¸ Semantic matching disabled: sentence-transformers not installed")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ Semantic matching failed: {e}")
            return None
    
    def _check_plan_skill_patterns(
        self, 
        message: str, 
        current_topic: Optional[str] = None
    ) -> Optional[SkillMatch]:
        """
        ğŸ†• æ£€æŸ¥ Plan Skill çš„ç‰¹æ®Šæ¨¡å¼
        
        å¤„ç† "å¸®æˆ‘åˆ¶å®šä¸€ä¸ªå­¦ä¹ ç‰©ç†çš„è®¡åˆ’" è¿™ç±»åˆ†æ•£å…³é”®è¯çš„æƒ…å†µ
        """
        message_lower = message.lower()
        
        # Plan skill çš„æ¨¡å¼åˆ—è¡¨
        plan_patterns = [
            # "å¸®æˆ‘åˆ¶å®šä¸€ä¸ª...è®¡åˆ’/æ–¹æ¡ˆ/è§„åˆ’"
            r'(å¸®æˆ‘|ç»™æˆ‘|è¯·)?åˆ¶å®š.{0,15}(è®¡åˆ’|æ–¹æ¡ˆ|è§„åˆ’|è·¯çº¿)',
            # "å¸®æˆ‘è§„åˆ’ä¸€ä¸‹..."
            r'(å¸®æˆ‘|ç»™æˆ‘|è¯·)?è§„åˆ’.{0,10}(å­¦ä¹ |å¤ä¹ |å¤‡è€ƒ)',
            # "åšä¸€ä¸ª...å­¦ä¹ è®¡åˆ’"
            r'åš.{0,10}(å­¦ä¹ |å¤ä¹ |å¤‡è€ƒ).{0,5}(è®¡åˆ’|æ–¹æ¡ˆ|è§„åˆ’)',
            # "å¦‚ä½•è§„åˆ’..."
            r'(å¦‚ä½•|æ€ä¹ˆ|æ€æ ·).{0,5}(è§„åˆ’|å®‰æ’|åˆ¶å®š).{0,10}(å­¦ä¹ |å¤ä¹ |å¤‡è€ƒ)',
            # "å­¦ä¹ è·¯çº¿/è·¯å¾„"
            r'(å­¦ä¹ |å¤ä¹ |å¤‡è€ƒ).{0,5}(è·¯çº¿|è·¯å¾„|è§„åˆ’|å®‰æ’)',
        ]
        
        for pattern in plan_patterns:
            match = re.search(pattern, message_lower)
            if match:
                logger.info(f"ğŸ“‹ Plan skill pattern matched: '{pattern}'")
                
                # æå– topic
                topic = None
                # å°è¯•ä»æ¶ˆæ¯ä¸­æå–å­¦ç§‘/ä¸»é¢˜
                topic_patterns = [
                    r'å­¦ä¹ (.{2,10}?)çš„?(è®¡åˆ’|æ–¹æ¡ˆ|è§„åˆ’)',  # "å­¦ä¹ ç‰©ç†çš„è®¡åˆ’"
                    r'(ç‰©ç†|åŒ–å­¦|æ•°å­¦|ç”Ÿç‰©|å†å²|åœ°ç†|è‹±è¯­|è¯­æ–‡|ç¼–ç¨‹|python)',
                    r'å¤ä¹ (.{2,10}?)çš„?(è®¡åˆ’|æ–¹æ¡ˆ)',  # "å¤ä¹ ç‰›é¡¿å®šå¾‹çš„è®¡åˆ’"
                ]
                for tp in topic_patterns:
                    topic_match = re.search(tp, message_lower)
                    if topic_match:
                        topic = topic_match.group(1) if topic_match.lastindex else topic_match.group(0)
                        break
                
                if not topic:
                    topic = current_topic  # ä½¿ç”¨å½“å‰ä¸»é¢˜
                
                return SkillMatch(
                    skill_id='learning_plan_skill',
                    confidence=0.95,
                    parameters={
                        'topic': topic,
                        'plan_type': 'learning_plan'
                    },
                    matched_keywords=['plan_pattern']
                )
        
        return None
    
    def _check_keywords(self, message: str, keywords: List[str]) -> List[str]:
        """æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯"""
        message_lower = message.lower()
        matched = []
        for keyword in keywords:
            if keyword.lower() in message_lower:
                matched.append(keyword)
        return matched
    
    def _extract_parameters(
        self,
        message: str,
        metadata: Dict[str, Any],
        skill_id: str,
        current_topic: Optional[str] = None,
        session_topics: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ä»æ¶ˆæ¯ä¸­æå–å‚æ•°
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            metadata: æŠ€èƒ½å…ƒæ•°æ®
            skill_id: æŠ€èƒ½ ID
            current_topic: å½“å‰å¯¹è¯ä¸»é¢˜ï¼ˆä» session_contextï¼‰
            session_topics: å†å²topicsåˆ—è¡¨ï¼ˆä» session_contextï¼‰
        
        Returns:
            parameters dict (topic, quantity, use_last_artifact, etc.)
        """
        params = {}
        
        # ğŸ”¥ Step 0: å…ˆæ¸…ç†å¼•ç”¨æ¨¡å¼ï¼Œé¿å… "ç¬¬ä¸€é“é¢˜" ä¸­çš„ "ä¸€é“" è¢«é”™è¯¯æå–ä¸ºæ•°é‡
        # ä¾‹å¦‚ï¼š"æ ¹æ®ç¬¬ä¸€é“é¢˜ï¼Œå¸®æˆ‘å‡ºä¸‰å¼ é—ªå¡" â†’ æ¸…ç†åç”¨äºæ•°é‡æå–çš„æ¶ˆæ¯æ˜¯ "æ ¹æ®ï¼Œå¸®æˆ‘å‡ºä¸‰å¼ é—ªå¡"
        message_for_quantity = self._clean_reference_patterns(message)
        
        # 1. æå–æ•°é‡å‚æ•° - æ”¯æŒé˜¿æ‹‰ä¼¯æ•°å­—å’Œä¸­æ–‡æ•°å­—
        # ä¸­æ–‡æ•°å­—æ˜ å°„
        chinese_numbers = {
            'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
            'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
            'ä¸¤': 2
        }
        
        quantity_value = None
        
        # ä¼˜å…ˆåŒ¹é…é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯ï¼‰
        arabic_match = re.search(r'(\d+)\s*[é“ä¸ªå¼ ä»½é¢˜å¡]', message_for_quantity)
        if arabic_match:
            quantity_value = int(arabic_match.group(1))
        else:
            # åŒ¹é…ä¸­æ–‡æ•°å­—ï¼ˆä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯ï¼‰
            chinese_match = re.search(r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])\s*[é“ä¸ªå¼ ä»½é¢˜å¡]', message_for_quantity)
            if chinese_match:
                chinese_char = chinese_match.group(1)
                quantity_value = chinese_numbers.get(chinese_char)
        
        if quantity_value:
            # æ ¹æ® skill_id è®¾ç½®æ­£ç¡®çš„å‚æ•°å
            if skill_id == 'quiz_skill':
                params['num_questions'] = quantity_value
            elif skill_id == 'flashcard_skill':
                params['num_cards'] = quantity_value
            elif skill_id == 'learning_plan_skill':
                # å­¦ä¹ åŒ…å¯èƒ½åŒ…å«å¤šä¸ªæ•°é‡å‚æ•°
                if 'é—ªå¡' in message or 'å¡ç‰‡' in message:
                    params['flashcard_quantity'] = quantity_value
                elif 'é¢˜' in message:
                    params['quiz_quantity'] = quantity_value
            
            logger.debug(f"ğŸ“Š Extracted quantity: {quantity_value}")
        
        # ğŸ”¥ 2. æ£€æµ‹å¤š topic å¼•ç”¨ï¼ˆå¦‚"åˆšåˆšä¸¤ä¸ªtopicçš„çŸ¥è¯†å¯¼å›¾"ï¼‰
        multi_topic_patterns = [
            r'(ä¸¤ä¸ª|2ä¸ª|ä¸‰ä¸ª|3ä¸ª|å¤šä¸ª)[çš„]?(topic|ä¸»é¢˜)',
            r'(åˆšåˆš|åˆšæ‰|å‰é¢|ä¸Šé¢)[çš„]?(ä¸¤ä¸ª|2ä¸ª|ä¸‰ä¸ª|3ä¸ª|æ‰€æœ‰)[çš„]?(topic|ä¸»é¢˜)',
            r'(æ‰€æœ‰|å…¨éƒ¨)[çš„]?(topic|ä¸»é¢˜)',
        ]
        
        for pattern in multi_topic_patterns:
            if re.search(pattern, message):
                # ç”¨æˆ·è¦æ±‚å¤šä¸ª topics
                if session_topics and len(session_topics) > 1:
                    # æå–æœ€è¿‘çš„2-3ä¸ª topics
                    recent_topics = session_topics[-3:] if len(session_topics) >= 3 else session_topics
                    combined_topic = " + ".join(recent_topics)
                    params['topic'] = combined_topic
                    params['multi_topic'] = True
                    params['topic_list'] = recent_topics
                    logger.info(f"ğŸ”€ Detected multi-topic request: {recent_topics}")
                    return params
                else:
                    # å†å² topics ä¸è¶³ï¼Œéœ€è¦ clarification
                    params['needs_clarification'] = True
                    params['clarification_reason'] = "multi_topic_insufficient"
                    logger.warning(f"âš ï¸  User requested multiple topics but session history insufficient")
                    return params
        
        # 3. æå–ä¸»é¢˜
        # ğŸ†• å…ˆæ¸…ç†å¼•ç”¨æ¨¡å¼ï¼Œé¿å…ä»å¼•ç”¨ä¸­é”™è¯¯æå– topic
        message_for_topic = self._clean_reference_patterns(message)
        
        # ğŸ”§ æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä¸»è¦æ˜¯å¼•ç”¨ï¼ˆæ¸…ç†åå‡ ä¹æ²¡å‰©ä»€ä¹ˆå†…å®¹ï¼‰
        # ä¾‹å¦‚ï¼š"æŠŠç¬¬äºŒé“é¢˜å¸®æˆ‘è¯¦ç»†è§£é‡Šä¸€ä¸‹" â†’ "æŠŠå¸®æˆ‘è¯¦ç»†è§£é‡Šä¸€ä¸‹"
        # è¿™ç§æƒ…å†µä¸‹ä¸åº”è¯¥ä»æ¸…ç†åçš„æ¶ˆæ¯æå– topicï¼Œå› ä¸ºçœŸæ­£çš„ topic åœ¨å¼•ç”¨çš„ artifact ä¸­
        cleaned_has_content = len(message_for_topic.replace('æŠŠ', '').replace('å¸®æˆ‘', '').replace('è¯¦ç»†', '').replace('ä¸€ä¸‹', '').strip()) > 3
        
        if cleaned_has_content:
            topic = self._extract_topic(message_for_topic, metadata)
        else:
            # æ¸…ç†åå‡ ä¹æ²¡æœ‰å†…å®¹ï¼Œè¯´æ˜è¿™æ˜¯ä¸€ä¸ªçº¯å¼•ç”¨è¯·æ±‚ï¼Œä½¿ç”¨ current_topic
            topic = None
            logger.info(f"ğŸ”— Reference-heavy message, skipping topic extraction from cleaned message")
        
        # ğŸ”¥ éªŒè¯æå–çš„ topic æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯å¡«å……è¯ã€ä¸æ˜¯å¤ªçŸ­ï¼‰
        if topic:
            invalid_topic_patterns = [
                'å¥½', 'å—¯', 'æ˜¯', 'è¡Œ', 'é‚£', 'å¯ä»¥', 'ok', 'OK',
                'å¥½çš„', 'æ˜¯çš„', 'è¡Œçš„', 'é‚£å°±', 'é‚£ä¹ˆ', 'å¯ä»¥çš„',
                'æµ‹éªŒ', 'æµ‹è¯•', 'ç»ƒä¹ ', 'é€‰æ‹©', 'åˆ¤æ–­', 'å¡«ç©º', 'ç®€ç­”',  # é¢˜ç›®ç±»å‹
                'å­¦ä¹ ', 'å¤ä¹ ', 'é¢„ä¹ ',  # åŠ¨ä½œè¯
            ]
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ— æ•ˆ topic
            if topic.strip() in invalid_topic_patterns or len(topic.strip()) < 2:
                logger.info(f"âš ï¸ Extracted topic '{topic}' is invalid, falling back to current_topic")
                topic = None
        
        # ğŸ”¥ å¦‚æœæ¶ˆæ¯ä¸­æ²¡æœ‰æ˜ç¡®ä¸»é¢˜ï¼Œä½†æœ‰ current_topicï¼Œä½¿ç”¨å®ƒ
        if not topic and current_topic:
            topic = current_topic
            logger.info(f"ğŸ“š Using current_topic from context: {topic}")
        
        # ğŸ”¥ å¦‚æœä»ç„¶æ²¡æœ‰ topicï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ clarification
        if not topic:
            # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦ topic çš„ skill
            needs_topic_skills = ['explain_skill', 'quiz_skill', 'flashcard_skill', 'notes_skill', 'mindmap_skill']
            if skill_id in needs_topic_skills:
                params['needs_clarification'] = True
                params['clarification_reason'] = "topic_missing"
                logger.warning(f"âš ï¸  Topic required for {skill_id} but not found")
                return params
        
        if topic:
            params['topic'] = topic
            # å¯¹äº explain_skillï¼Œtopic åº”è¯¥è®¾ç½®ä¸º concept_name
            if skill_id == 'explain_skill':
                params['concept_name'] = topic
        
        # 4. æ£€æµ‹ä¸Šä¸‹æ–‡å¼•ç”¨ - ä½¿ç”¨ç®€å•çš„å…³é”®è¯æ£€æµ‹
        context_keywords = ['æ ¹æ®', 'åŸºäº', 'åˆšæ‰', 'è¿™äº›', 'è¿™é“', 'ä¸Šé¢', 'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'ç¬¬', 'å†æ¥', 'å†ç»™']
        if any(kw in message for kw in context_keywords):
            params['use_last_artifact'] = True
            logger.debug(f"ğŸ”— Detected context reference")
        
        return params
    
    def _extract_topic(self, message: str, metadata: Dict[str, Any]) -> Optional[str]:
        """ä»æ¶ˆæ¯ä¸­æå–ä¸»é¢˜ - ä½¿ç”¨ç®€å•ç›´æ¥çš„æ–¹æ³•"""
        
        # ğŸ”¥ Step 0: å¸¸è§çš„ follow-up å“åº”è¯ï¼ˆè¿™äº›æ¶ˆæ¯åº”ä½¿ç”¨ current_topicï¼‰
        # "å¥½çš„ï¼Œç»™æˆ‘ä¸‰å¼ é—ªå¡" / "å—¯ï¼Œå†æ¥å‡ é“é¢˜" / "æ˜¯çš„ï¼Œå¸®æˆ‘åšç¬”è®°"
        followup_starters = [
            'å¥½çš„', 'å¥½', 'å—¯', 'æ˜¯çš„', 'å¯ä»¥', 'è¡Œ', 'é‚£', 'é‚£å°±', 'é‚£ä¹ˆ',
            'æ²¡é—®é¢˜', 'å½“ç„¶', 'ok', 'OK', 'å¥½å•Š', 'å¥½å‘€', 'è¡Œå•Š', 'è¡Œå§',
            'å¯ä»¥çš„', 'æ²¡äº‹', 'å¯¹', 'å¯¹çš„', 'æ˜¯', 'ç¡®å®š', 'ç¡®è®¤'
        ]
        
        # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä»¥ follow-up è¯å¼€å¤´
        message_stripped = message.strip()
        for starter in followup_starters:
            if message_stripped.startswith(starter):
                # ç§»é™¤å¼€å¤´çš„ follow-up è¯å’Œæ ‡ç‚¹
                remaining = message_stripped[len(starter):].lstrip('ï¼Œ,ã€‚.ï¼!ã€ ')
                # å¦‚æœå‰©ä½™éƒ¨åˆ†æ˜¯çº¯åŠ¨ä½œè¯·æ±‚ï¼ˆæ²¡æœ‰æ˜ç¡® topicï¼‰ï¼Œè¿”å› None
                if self._is_pure_action_request(remaining):
                    logger.debug(f"ğŸ”— Follow-up message detected: '{starter}...' â†’ using current_topic")
                    return None
        
        # ğŸ”¥ Step 1: æ£€æµ‹éšå¼ä¸Šä¸‹æ–‡å¼•ç”¨ï¼ˆè¿™äº›æƒ…å†µåº”è¿”å› Noneï¼Œç”± current_topic å¡«å……ï¼‰
        # ğŸ“Œ æ³¨æ„ï¼šåªæœ‰å½“æ¶ˆæ¯ä¸­æ²¡æœ‰æ˜ç¡® topic æ—¶æ‰ç®—éšå¼å¼•ç”¨
        #    "ç»™æˆ‘ä¸‰å¼ é—ªå¡" â†’ éšå¼å¼•ç”¨ï¼ˆæ²¡æœ‰ topicï¼‰
        #    "æˆ‘éœ€è¦å…‰åˆä½œç”¨ä¸‰å¼ é—ªå¡" â†’ ä¸æ˜¯éšå¼å¼•ç”¨ï¼ˆæœ‰ topic "å…‰åˆä½œç”¨"ï¼‰
        implicit_reference_patterns = [
            # åªåŒ¹é…çº¯åŠ¨ä½œè¯·æ±‚ï¼Œä¸åŒ…å«é¢å¤–å†…å®¹ï¼ˆtopicï¼‰
            r'^(éœ€è¦|æƒ³è¦|ç»™æˆ‘|æ¥|è¦|ç”Ÿæˆ|åˆ›å»º)\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])?\s*[é“ä¸ªå¼ ä»½]?(?:çŸ¥è¯†å¯¼å›¾|é—ªå¡|é¢˜ç›®|ç¬”è®°|å¡ç‰‡|é¢˜|å¡)$',  # "ç»™æˆ‘ä¸‰å¼ é—ªå¡"ï¼ˆå®Œå…¨åŒ¹é…ï¼Œç»“å°¾ï¼‰
            r'^(ä¸å¯¹|å†|ç»§ç»­|è¿˜è¦|åˆšåˆš|åˆšæ‰|è¿™ä¸ª|é‚£ä¸ª)',           # "å†æ¥å‡ é“"ã€"åˆšåˆšçš„"
            r'(åˆšåˆš|åˆšæ‰|ä¸Šé¢|å‰é¢|è¿™äº›)[çš„]?(topic|ä¸»é¢˜)',      # "åˆšåˆšä¸¤ä¸ªtopic"
            r'^(å‡º|åš|å†™|ç”»)\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])?\s*[é“ä¸ªå¼ ä»½]?(?:é¢˜|é—ªå¡|å¯¼å›¾|ç¬”è®°|å¡)$',  # "å‡º3é“é¢˜"ï¼ˆå®Œå…¨åŒ¹é…ï¼Œç»“å°¾ï¼‰
        ]
        
        for pattern in implicit_reference_patterns:
            if re.search(pattern, message):
                logger.debug(f"ğŸ”— Detected implicit context reference, will use current_topic")
                return None  # æ˜ç¡®è¿”å› Noneï¼Œè®©è°ƒç”¨è€…ä½¿ç”¨ current_topic
        
        # ğŸ”¥ Step 2: ä¼˜åŒ–çš„ä¸»é¢˜æå–æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        topic_patterns = [
            # ğŸ†• æœ€é«˜ä¼˜å…ˆçº§ï¼šæ˜ç¡®çš„"ä¸»é¢˜æ˜¯XXX"ç»“æ„
            r'ä¸»é¢˜æ˜¯(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',                   # "ä¸»é¢˜æ˜¯å…‰åˆä½œç”¨"
            r'(?:å…³äº|ä¸»é¢˜[ä¸ºæ˜¯]?)(.+?)(?:çš„)?(?:å­¦ä¹ è®¡åˆ’|è®¡åˆ’|è§„åˆ’)',  # "å…³äºå…‰åˆä½œç”¨çš„å­¦ä¹ è®¡åˆ’"
            
            # ğŸ†• æœ€é«˜ä¼˜å…ˆçº§ï¼šæ˜ç¡®çš„"XXXçš„è§£é‡Š/è¯´æ˜"ç»“æ„
            r'(.+?)çš„(?:è§£é‡Š|è®²è§£|è¯´æ˜|ä»‹ç»|å®šä¹‰)',          # "äºŒæˆ˜èµ·å› çš„è§£é‡Š"
            
            # ğŸ”¥ æ–°å¢ï¼štopicåœ¨å‰ï¼ŒåŠ¨ä½œè¯+æ•°é‡è¯åœ¨åï¼ˆå¦‚"äºŒæˆ˜çš„èµ·å› ç»™æˆ‘ä¸‰å¼ é—ªå¡"ï¼‰
            r'^(.+?)(ç»™æˆ‘|æ¥|å¸®æˆ‘|ç”Ÿæˆ|åˆ›å»º)(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])[é“ä¸ªå¼ ä»½é¢˜å¡]',  # "XXXç»™æˆ‘Nå¼ é—ªå¡"
            
            # ğŸ”¥ æ–°å¢ï¼šflashcard ä¸“ç”¨æ¨¡å¼
            r'(?:æ ¹æ®|å…³äº)?(.+?)(?:å‡º|ç”Ÿæˆ|åš|æ¥)(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])[é“ä¸ªå¼ ä»½]?(?:é—ªå¡|å¡ç‰‡|å¡)',  # "äºŒæˆ˜èµ·å› å‡ºä¸‰å¼ é—ªå¡"ã€"æ ¹æ®äºŒæˆ˜èµ·å› å‡ºä¸‰å¼ é—ªå¡"
            r'(?:æˆ‘éœ€è¦|éœ€è¦|æƒ³è¦|è¦)(.+?)(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])[é“ä¸ªå¼ ä»½]?(?:é—ªå¡|å¡ç‰‡|é¢˜|å¡)',  # "æˆ‘éœ€è¦å…‰åˆä½œç”¨ä¸‰å¼ é—ªå¡"
            r'(?:å¸®æˆ‘)?(?:æ ¹æ®|å…³äº)(.+?)(?:å‡º|ç”Ÿæˆ|åš|æ¥)(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])?',  # "å¸®æˆ‘æ ¹æ®äºŒæˆ˜èµ·å› å‡ºä¸‰å¼ é—ªå¡"
            
            # ğŸ”¥ æ–°å¢ï¼šæ— æ•°é‡è¯çš„æ¨¡å¼ï¼ˆå¦‚ "ç”Ÿæˆå…‰åˆä½œç”¨çš„é—ªå¡"ï¼‰
            r'(?:ç”Ÿæˆ|åˆ›å»º|åš|å‡º|æ¥)(.+?)(?:çš„)?(?:é—ªå¡|å¡ç‰‡|é¢˜ç›®|ç»ƒä¹ é¢˜|ç¬”è®°|æ€ç»´å¯¼å›¾|å¯¼å›¾)',  # "ç”Ÿæˆå…‰åˆä½œç”¨çš„é—ªå¡"
            r'(.+?)(?:çš„)?(?:é—ªå¡|å¡ç‰‡|é¢˜ç›®|ç»ƒä¹ é¢˜|ç¬”è®°|æ€ç»´å¯¼å›¾|å¯¼å›¾)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',  # "å…‰åˆä½œç”¨çš„é—ªå¡"
            
            # é«˜ä¼˜å…ˆçº§ï¼šæ˜ç¡®çš„ä¸»é¢˜è¯
            r'ä»€ä¹ˆæ˜¯(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',              # "ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨"
            r'è§£é‡Š(?:ä¸€ä¸‹)?(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',     # "è§£é‡Šå…‰åˆä½œç”¨"ã€"è§£é‡Šä¸€ä¸‹å…‰åˆä½œç”¨"
            r'è®²(?:è¿°(?:ä¸€ä¸‹)?|è§£(?:ä¸€ä¸‹)?|è®²|ä¸€ä¸‹)(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',  # "è®²è¿°å¥½è±åå†å²"ã€"è®²è§£å…‰åˆä½œç”¨"ã€"è®²è§£ä¸€ä¸‹å…‰åˆä½œç”¨"ã€"è®²è®²å…‰åˆä½œç”¨"ã€"è®²ä¸€ä¸‹å…‰åˆä½œç”¨"
            r'ç†è§£(?:ä¸€ä¸‹)?(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',     # "ç†è§£å…‰åˆä½œç”¨"
            r'äº†è§£(?:ä¸€ä¸‹)?(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',     # "äº†è§£å…‰åˆä½œç”¨"
            r'å­¦ä¹ (?:ä¸€ä¸‹)?(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',     # "å­¦ä¹ å…‰åˆä½œç”¨"
            r'å…³äº(.+?)çš„',                             # "å…³äºå…‰åˆä½œç”¨çš„"
            
            # ä¸­ä¼˜å…ˆçº§ï¼šå¸¦æ•°é‡è¯çš„æ¨¡å¼
            r'(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])[é“ä¸ªå¼ ä»½é¢˜å¡](.+?)(?:çš„)?[é¢˜ç¬”é—ªå¯¼å¡å›¾è®°]',  # "3é“å…‰åˆä½œç”¨çš„é¢˜"
        ]
        
        for pattern in topic_patterns:
            match = re.search(pattern, message)
            if match:
                # æå–ç¬¬ä¸€ä¸ªæ•è·ç»„
                topic = match.group(1).strip()
                # æ¸…ç†ä¸»é¢˜
                topic = self._clean_topic(topic)
                
                # ğŸ”¥ Step 3: æ›´ä¸¥æ ¼çš„éªŒè¯ - æ’é™¤åŠ¨ä½œè¯å’Œæ˜æ˜¾æ— æ•ˆçš„ä¸»é¢˜
                invalid_topics = [
                    'æˆ‘éœ€è¦', 'å¸®æˆ‘', 'ç»™æˆ‘', 'æˆ‘è¦', 'å†æ¥', 'å†ç»™', 'å†å‡º', 'å‡º', 'éœ€è¦', 'æƒ³è¦',
                    'é€‰æ‹©', 'åˆ¤æ–­', 'å¡«ç©º', 'ç®€ç­”',  # é¢˜ç›®ç±»å‹ï¼Œä¸æ˜¯ä¸»é¢˜
                    'å­¦ä¹ ', 'å¤ä¹ ', 'ç»ƒä¹ ', 'æµ‹è¯•',  # åŠ¨ä½œè¯ï¼Œä¸æ˜¯ä¸»é¢˜
                    'çŸ¥è¯†', 'topic', 'ä¸»é¢˜', 'å†…å®¹',  # å¤ªæ³›åŒ–
                    'æ–‡ä»¶', 'æ–‡ä»¶å†…å®¹', 'è¿™æ–‡ä»¶', 'é‚£æ–‡ä»¶', 'è¿™ä¸ªæ–‡ä»¶', 'é‚£ä¸ªæ–‡ä»¶',  # ğŸ†• æ–‡ä»¶ç›¸å…³æ— æ•ˆ topic
                    'è¿™ä¸¤ä¸ªæ–‡ä»¶', 'è¿™äº›æ–‡ä»¶', 'é™„ä»¶', 'ä¸Šä¼ çš„æ–‡ä»¶',  # ğŸ†• æ–‡ä»¶ç›¸å…³æ— æ•ˆ topic
                ]
                
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦ä»¥åŠ¨ä½œè¯å¼€å¤´ï¼ˆè¿™äº›ä¸æ˜¯æœ‰æ•ˆä¸»é¢˜ï¼‰
                action_prefixes = ['éœ€è¦', 'æƒ³è¦', 'ç»™æˆ‘', 'å¸®æˆ‘', 'æˆ‘è¦', 'å†æ¥', 'å†ç»™', 'è¿™', 'é‚£', 'æ–‡ä»¶', 'æ ¹æ®']
                starts_with_action = any(topic.startswith(prefix) for prefix in action_prefixes)
                
                # ğŸ†• æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶ç›¸å…³è¯æ±‡ï¼ˆæ•´ä½“æ— æ•ˆï¼‰
                file_related = any(word in topic for word in ['æ–‡ä»¶', 'é™„ä»¶', 'ä¸Šä¼ '])
                
                if topic and len(topic) >= 2 and topic not in invalid_topics and not starts_with_action and not file_related:
                    logger.debug(f"ğŸ“ Extracted topic: {topic} (pattern: {pattern})")
                    return topic
        
        return None
    
    def _clean_reference_patterns(self, message: str) -> str:
        """
        æ¸…ç†æ¶ˆæ¯ä¸­çš„å¼•ç”¨æ¨¡å¼ï¼Œç”¨äºå…³é”®è¯åŒ¹é…
        
        Args:
            message: åŸå§‹æ¶ˆæ¯
        
        Returns:
            æ¸…ç†åçš„æ¶ˆæ¯
        """
        reference_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+é“?é¢˜',       # ç¬¬Xé¢˜ã€ç¬¬Xé“é¢˜
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+å¼ [é—ª]?å¡ç‰‡?', # ç¬¬Xå¼ é—ªå¡ã€ç¬¬Xå¼ å¡ã€ç¬¬Xå¼ å¡ç‰‡
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ä¸ª?ä¾‹[å­]?',  # ç¬¬Xä¸ªä¾‹å­
            r'é‚£é“é¢˜',                                 # é‚£é“é¢˜
            r'è¿™é“é¢˜',                                 # è¿™é“é¢˜
            r'é‚£å¼ [é—ª]?å¡',                            # é‚£å¼ å¡
            r'è¿™å¼ [é—ª]?å¡',                            # è¿™å¼ å¡
        ]
        
        message_cleaned = message
        for pattern in reference_patterns:
            message_cleaned = re.sub(pattern, '', message_cleaned)
        
        if message_cleaned != message:
            logger.info(f"ğŸ“ Reference detected, cleaned message for intent: '{message_cleaned}'")
        
        return message_cleaned
    
    def _is_pure_action_request(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ˜¯çº¯åŠ¨ä½œè¯·æ±‚ï¼ˆæ²¡æœ‰æ˜ç¡®çš„å­¦ä¹ ä¸»é¢˜ï¼‰
        
        Args:
            text: æ¸…ç†åçš„æ–‡æœ¬
            
        Returns:
            True å¦‚æœæ˜¯çº¯åŠ¨ä½œè¯·æ±‚ï¼ŒFalse å¦‚æœåŒ…å«æ˜ç¡®ä¸»é¢˜
        """
        if not text or len(text.strip()) < 2:
            return True
        
        # çº¯åŠ¨ä½œè¯·æ±‚çš„æ¨¡å¼
        pure_action_patterns = [
            r'^(ç»™æˆ‘|å¸®æˆ‘|æ¥|è¦|ç”Ÿæˆ|åˆ›å»º|å‡º|åš|å†™|ç”»)\s*(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])?\s*[é“ä¸ªå¼ ä»½]?[é¢˜é—ªå¡å¯¼å›¾ç¬”è®°å¡ç‰‡æ€ç»´]',
            r'^(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])\s*[é“ä¸ªå¼ ä»½][é¢˜é—ªå¡å¯¼å›¾ç¬”è®°å¡ç‰‡]',  # "3é“é¢˜"ã€"äº”å¼ é—ªå¡"
            r'^[é¢˜é—ªå¡å¯¼å›¾ç¬”è®°å¡ç‰‡æ€ç»´]+$',  # åªæœ‰æŠ€èƒ½è¯
        ]
        
        for pattern in pure_action_patterns:
            if re.search(pattern, text):
                return True
        
        # å¦‚æœå‰©ä½™æ–‡æœ¬åªåŒ…å«åŠ¨ä½œè¯å’Œæ•°é‡è¯ï¼Œä¹Ÿæ˜¯çº¯åŠ¨ä½œè¯·æ±‚
        action_words = ['ç»™æˆ‘', 'å¸®æˆ‘', 'æ¥', 'è¦', 'ç”Ÿæˆ', 'åˆ›å»º', 'å‡º', 'åš', 'å†™', 'ç”»', 
                        'å†æ¥', 'å†ç»™', 'å†å‡º', 'ç»§ç»­', 'è¿˜è¦']
        skill_words = ['é¢˜', 'é—ªå¡', 'å¡ç‰‡', 'å¯¼å›¾', 'æ€ç»´å¯¼å›¾', 'ç¬”è®°', 'è§£é‡Š', 'è®²è§£']
        quantity_pattern = r'(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤])\s*[é“ä¸ªå¼ ä»½]?'
        
        # ç§»é™¤æ‰€æœ‰åŠ¨ä½œè¯ã€æŠ€èƒ½è¯å’Œæ•°é‡è¯åï¼Œçœ‹è¿˜å‰©ä»€ä¹ˆ
        remaining = text
        for word in action_words + skill_words:
            remaining = remaining.replace(word, '')
        remaining = re.sub(quantity_pattern, '', remaining)
        remaining = remaining.strip('ï¼Œ,ã€‚.ï¼!ã€ ')
        
        # å¦‚æœå‰©ä½™éƒ¨åˆ†å¤ªçŸ­ï¼ˆ< 2 å­—ï¼‰ï¼Œè®¤ä¸ºæ˜¯çº¯åŠ¨ä½œè¯·æ±‚
        return len(remaining) < 2
    
    def _clean_topic(self, topic: str) -> str:
        """æ¸…ç†ä¸»é¢˜æ–‡æœ¬ï¼Œç§»é™¤å¡«å……è¯"""
        
        # ğŸ”¥ Step 1: åªä»å¼€å¤´ç§»é™¤çš„è¯ï¼ˆå¯èƒ½æ˜¯ä¸“æœ‰åè¯çš„ä¸€éƒ¨åˆ†ï¼‰
        # "å¥½çš„ï¼Œç»™æˆ‘..." â†’ ç§»é™¤ "å¥½çš„"
        # "å¥½è±åå†å²" â†’ ä¿ç•™ "å¥½"ï¼ˆå› ä¸ºæ˜¯ä¸“æœ‰åè¯çš„ä¸€éƒ¨åˆ†ï¼‰
        prefix_only_words = [
            "å¥½çš„", "å¥½å•Š", "å¥½å‘€", "å—¯", "æ˜¯çš„", "å¯ä»¥", "è¡Œ", "é‚£", "é‚£å°±", "é‚£ä¹ˆ", "ok", "OK",
            "æ²¡é—®é¢˜", "å½“ç„¶", "å¯¹çš„", "å¯¹", "ç¡®å®š", "ç¡®è®¤"
        ]
        for prefix in prefix_only_words:
            if topic.startswith(prefix):
                topic = topic[len(prefix):].lstrip("ï¼Œ,ã€ ")
        
        # ğŸ”¥ Step 2: å¯ä»¥åœ¨ä»»æ„ä½ç½®ç§»é™¤çš„è¯ï¼ˆä¸ä¼šæ˜¯ä¸“æœ‰åè¯çš„ä¸€éƒ¨åˆ†ï¼‰
        filler_words = [
            # è¯­æ°”è¯å’Œæ ‡ç‚¹
            "çš„", "äº†", "å—", "å‘¢", "å•Š", "å§", "ï¼Œ", ",", "ã€‚", ".", "ï¼", "!", "+", "ï¼‹",
            # åŠ¨ä½œè¯
            "ç»™æˆ‘", "å¸®æˆ‘", "æˆ‘è¦", "æˆ‘éœ€è¦", "ç”Ÿæˆ", "åˆ›å»º", "å‡º", "åš", "å†™", "ç”»",
            "æ¥", "è¦", "éœ€è¦", "æƒ³è¦", "å†æ¥", "å†ç»™", "å†å‡º", "ç»§ç»­", "è¿˜è¦",
            # ä¸Šä¸‹æ–‡å¼•ç”¨
            "å…³äº", "æœ‰å…³", "æ ¹æ®", "åˆšåˆš", "åˆšæ‰", "ä¸Šé¢", "è¿™ä¸ª", "é‚£ä¸ª", "è¿™äº›", "é‚£äº›",
            # æŠ€èƒ½ç›¸å…³è¯ï¼ˆä¼šåœ¨ learning_bundle æ¶ˆæ¯ä¸­å‡ºç°ï¼‰
            "æ€ç»´å¯¼å›¾", "å¯¼å›¾", "ç¬”è®°", "é¢˜ç›®", "é—ªå¡", "å¡ç‰‡", "æµ‹éªŒ", "æµ‹è¯•",
            "è§£é‡Š", "è®²è§£", "å­¦ä¹ åŒ…", "å­¦ä¹ èµ„æ–™", "å­¦ä¹ ææ–™", "å­¦ä¹ å¥—è£…",
            # å…¶ä»–æ— æ„ä¹‰è¯
            "ä¸€ä¸‹", "è¯¦ç»†", "ç®€å•", "å®Œæ•´", "å…¨éƒ¨", "æ‰€æœ‰", "åŒ…å«", "åŒ…æ‹¬"
        ]
        for filler in filler_words:
            topic = topic.replace(filler, " ")
        
        # ç§»é™¤æ•°é‡è¯ï¼ˆé˜¿æ‹‰ä¼¯æ•°å­— + ä¸­æ–‡æ•°å­—ï¼‰
        topic = re.sub(r'\d+\s*[ä¸ªé“å¼ ä»½é¢˜å¡]', '', topic)
        topic = re.sub(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤]\s*[ä¸ªé“å¼ ä»½é¢˜å¡]', '', topic)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        topic = ' '.join(topic.split())
        
        # ğŸ”¥ æœ€åæ£€æŸ¥ï¼šå¦‚æœæ¸…ç†åçš„ topic å¤ªçŸ­æˆ–æ˜¯çº¯æ•°å­—/æ ‡ç‚¹/ç¬¦å·ï¼Œè¿”å›ç©º
        cleaned = topic.strip()
        if len(cleaned) < 2 or re.match(r'^[\d\sï¼Œ,ã€‚.ï¼!ã€+ï¼‹\-ï¼]+$', cleaned):
            return ""
        
        return cleaned
    
    def _calculate_confidence(
        self,
        message: str,
        metadata: Dict[str, Any],
        matched_keywords: List[str],
        parameters: Dict[str, Any]
    ) -> float:
        """
        è®¡ç®—åŒ¹é…ç½®ä¿¡åº¦
        
        Returns:
            confidence score (0.0 - 1.0)
        """
        confidence = 0.5  # åŸºç¡€åˆ†
        
        # 1. å…³é”®è¯åŒ¹é…ï¼ˆ+0.3ï¼‰
        if matched_keywords:
            confidence += 0.3
        
        # 2. æœ‰æ˜ç¡®ä¸»é¢˜ï¼ˆ+0.15ï¼‰
        if parameters.get('topic') or parameters.get('concept_name'):
            confidence += 0.15
        
        # 3. æœ‰æ•°é‡å‚æ•°ï¼ˆ+0.05ï¼‰
        if any(k in parameters for k in ['num_questions', 'num_cards', 'flashcard_quantity', 'quiz_quantity']):
            confidence += 0.05
        
        # 4. ç®€çŸ­æ˜ç¡®çš„è¯·æ±‚ï¼ˆ+0.1ï¼‰
        if len(message) < 20 and matched_keywords:
            confidence += 0.05
        
        return min(confidence, 1.0)  # æœ€å¤§ 1.0
    
    def _detect_mixed_intent(
        self, 
        message: str, 
        current_topic: Optional[str] = None
    ) -> Optional[SkillMatch]:
        """
        æ£€æµ‹æ··åˆæ„å›¾ï¼ˆå¤šä¸ªæŠ€èƒ½å…³é”®è¯ï¼‰
        
        å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªæŠ€èƒ½çš„å…³é”®è¯ï¼Œè¿”å› learning_plan_skill
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            current_topic: å½“å‰å¯¹è¯ä¸»é¢˜ï¼ˆä» session_contextï¼‰
        
        Returns:
            SkillMatch for learning_plan_skill or None
        """
        # ğŸ†• Step -1: æ£€æµ‹ç»­é—®æ¨¡å¼ï¼Œè·³è¿‡æ··åˆæ„å›¾æ£€æµ‹
        # ç”¨æˆ·è¯·æ±‚ç®€åŒ–ã€è¯¦ç»†è¯´æ˜ã€å†è¯´ä¸€éç­‰ï¼Œéƒ½æ˜¯ç»­é—®ï¼Œä¸æ˜¯æ–°çš„æ„å›¾
        followup_patterns = [
            # è‹±æ–‡ç»­é—®æ¨¡å¼
            r'\bsimpler\b',           # "explain ... simpler"
            r'\bsimplify\b',          # "simplify that"
            r'\bmore detail\b',       # "more detail"
            r'\bmore details\b',      # "more details"
            r'\bmore specifically\b', # "more specifically"
            r'\bin detail\b',         # "in detail"
            r'\bagain\b',             # "explain again"
            r'\bone more time\b',     # "one more time"
            r'\belaborate\b',         # "elaborate on that"
            r'\bclarify\b',           # "clarify"
            r'\beasier\b',            # "make it easier"
            r'\bshorter\b',           # "shorter version"
            r'\bcontinue\b',          # "continue"
            r'\bgo on\b',             # "go on"
            # ä¸­æ–‡ç»­é—®æ¨¡å¼
            r'ç®€å•[ç‚¹äº›ä¸€]',          # "ç®€å•ç‚¹"
            r'æ›´ç®€å•',                # "æ›´ç®€å•"
            r'è¯¦ç»†[ç‚¹äº›ä¸€]',          # "è¯¦ç»†ç‚¹"
            r'æ›´è¯¦ç»†',                # "æ›´è¯¦ç»†"
            r'å†è¯´[ä¸€]?é',          # "å†è¯´ä¸€é"
            r'é‡æ–°[è¯´è®²è§£é‡Š]',       # "é‡æ–°è¯´"
            r'æ¢[ä¸€ä¸ª]?[ç§]?æ–¹å¼',   # "æ¢ä¸€ç§æ–¹å¼"
            r'ç»§ç»­',                  # "ç»§ç»­"
            r'æ¥ç€è¯´',                # "æ¥ç€è¯´"
        ]
        
        message_lower = message.lower()
        for pattern in followup_patterns:
            if re.search(pattern, message_lower, re.IGNORECASE):
                logger.info(f"â© Followup pattern detected: '{pattern}' in message, skipping mixed intent")
                return None  # è®©åç»­é€»è¾‘å¤„ç†ä¸º other intent
        
        # ğŸ†• Step 0: ç§»é™¤å¼•ç”¨æ¨¡å¼ï¼Œé¿å…æŠŠå¼•ç”¨å½“ä½œ intent
        # "æŠŠç¬¬äºŒé“é¢˜å¸®æˆ‘è§£é‡Š" ä¸­çš„ "ç¬¬äºŒé“é¢˜" æ˜¯å¼•ç”¨ï¼Œä¸æ˜¯è¦å‡ºé¢˜
        reference_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+é“?é¢˜',      # ç¬¬Xé¢˜ã€ç¬¬Xé“é¢˜
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+å¼ [é—ª]?å¡ç‰‡?', # ç¬¬Xå¼ é—ªå¡ã€ç¬¬Xå¼ å¡ã€ç¬¬Xå¼ å¡ç‰‡
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ä¸ª?ä¾‹[å­]?', # ç¬¬Xä¸ªä¾‹å­
            r'é‚£é“é¢˜',                                # é‚£é“é¢˜
            r'è¿™é“é¢˜',                                # è¿™é“é¢˜
            r'é‚£å¼ [é—ª]?å¡',                           # é‚£å¼ å¡
            r'è¿™å¼ [é—ª]?å¡',                           # è¿™å¼ å¡
        ]
        
        # ä»æ¶ˆæ¯ä¸­ç§»é™¤å¼•ç”¨éƒ¨åˆ†åå†æ£€æµ‹
        message_cleaned = message
        for pattern in reference_patterns:
            message_cleaned = re.sub(pattern, '', message_cleaned)
        
        # å¦‚æœæ¸…ç†åæ¶ˆæ¯æœ‰å˜åŒ–ï¼Œè¯´æ˜æœ‰å¼•ç”¨
        has_reference = message_cleaned != message
        if has_reference:
            logger.info(f"ğŸ“ Reference detected, cleaned message for intent: '{message_cleaned}'")
            
            # ğŸ†• è¿›ä¸€æ­¥æ¸…ç†ï¼šç§»é™¤å¼•ç”¨ç›¸å…³çš„ä¿®é¥°è¯
            # "ç»™å‡ºé¢˜ç›®è§£é‡Š" ä¸­çš„ "é¢˜ç›®" æ˜¯æŒ‡è¢«å¼•ç”¨çš„é¢˜ç›®ï¼Œä¸æ˜¯è¦å‡ºæ–°é¢˜
            reference_modifiers = [
                (r'è¿™ä¸ª?é¢˜ç›®?', ''),          # è¿™é¢˜ç›®ã€è¿™ä¸ªé¢˜ç›®
                (r'é‚£ä¸ª?é¢˜ç›®?', ''),          # é‚£é¢˜ç›®ã€é‚£ä¸ªé¢˜ç›®
                (r'ä¸Šé¢çš„?', ''),              # ä¸Šé¢çš„
                (r'åˆšæ‰çš„?', ''),              # åˆšæ‰çš„
                (r'å‰é¢çš„?', ''),              # å‰é¢çš„
            ]
            for pattern, replacement in reference_modifiers:
                message_cleaned = re.sub(pattern, replacement, message_cleaned)
            
            # ğŸ†• å½“æœ‰å¼•ç”¨æ—¶ï¼Œå¦‚æœæ¸…ç†åå‰©ä½™å†…å®¹å¾ˆå°‘ï¼Œè¯´æ˜è¿™æ˜¯çº¯å¼•ç”¨æ“ä½œ
            # ä¾‹å¦‚ï¼š"å¯ä»¥å¸®æˆ‘æ ¹æ®ï¼Œç»™å‡ºè§£é‡Šå—" â†’ åªå‰© "è§£é‡Š" åŠ¨ä½œï¼Œä¸åº”è§¦å‘ quiz
            remaining_content = re.sub(r'[ï¼Œã€‚ï¼Ÿï¼,.\?!]', '', message_cleaned).strip()
            remaining_content = re.sub(r'^(å¯ä»¥|èƒ½ä¸èƒ½|å¸®æˆ‘|è¯·|ç»™æˆ‘|æ ¹æ®)', '', remaining_content).strip()
            if len(remaining_content) < 10:
                logger.info(f"ğŸ“ Reference-heavy message: '{remaining_content}', likely single intent")
        
        # å®šä¹‰å„æŠ€èƒ½çš„å…³é”®è¯é›†åˆ
        # ğŸ”§ æ³¨æ„ï¼šæŸäº›è¯å¦‚ "ç†è§£"ã€"äº†è§£" åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­ä¸åº”è§¦å‘ explain
        #    ä¾‹å¦‚ "åŠ æ·±ç†è§£" ä¸æ˜¯ explain è¯·æ±‚ï¼Œè€Œæ˜¯ä¿®é¥°è¯­
        skill_keywords = {
            'explain': ['è§£é‡Š', 'è®²è§£', 'è®²è¿°', 'è¯´æ˜', 'ä»€ä¹ˆæ˜¯', 'ä»‹ç»', 'å®šä¹‰', 'æ•™æˆ‘', 'å‘Šè¯‰æˆ‘', 'ç§‘æ™®', 'è§£è¯»', 'explain', 'what is', 'understand', 'teach me'],
            'quiz': ['é¢˜', 'é¢˜ç›®', 'ç»ƒä¹ ', 'æµ‹è¯•', 'è€ƒé¢˜', 'æµ‹éªŒ', 'åšé¢˜', 'åˆ·é¢˜', 'é—®é¢˜', 'ä¹ é¢˜', 'è¯•é¢˜', 'å‡ºé¢˜', 'quiz', 'test', 'question', 'exam', 'exercise'],
            'flashcard': ['é—ªå¡', 'å¡ç‰‡', 'è®°å¿†å¡', 'æŠ½è®¤å¡', 'èƒŒè¯µå¡', 'å¤ä¹ å¡', 'ç”Ÿæˆé—ªå¡', 'åšé—ªå¡', 'flashcard', 'card', 'anki'],
            'notes': ['ç¬”è®°', 'æ€»ç»“', 'å½’çº³', 'æ•´ç†', 'æç‚¼', 'æ¢³ç†', 'è¦ç‚¹', 'notes', 'summary', 'outline'],
            'mindmap': ['æ€ç»´å¯¼å›¾', 'å¯¼å›¾', 'è„‘å›¾', 'çŸ¥è¯†å›¾è°±', 'çŸ¥è¯†å›¾', 'æ¦‚å¿µå›¾', 'ç»“æ„å›¾', 'mindmap', 'mind map', 'concept map', 'knowledge graph'],
            'learning_bundle': ['å­¦ä¹ åŒ…', 'å­¦ä¹ èµ„æ–™', 'å­¦ä¹ ææ–™', 'å®Œæ•´', 'å­¦ä¹ å¥—è£…', 'å­¦ä¹ è®¡åˆ’', 'learning bundle', 'study package']
        }
        
        # ğŸ†• éœ€è¦ç‰¹æ®Šå¤„ç†çš„å…³é”®è¯ï¼ˆåªåœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­åŒ¹é… explainï¼‰
        # "ç†è§£" å’Œ "äº†è§£" åªæœ‰åœ¨åŠ¨ä½œè¯­å¢ƒä¸­æ‰åŒ¹é… explain
        explain_contextual_keywords = ['ç†è§£', 'äº†è§£', 'å­¦ä¹ ']
        explain_exclude_prefixes = ['åŠ æ·±', 'æ·±å…¥', 'æ›´å¥½', 'ä¸ºäº†', 'å¸®åŠ©']  # è¿™äº›å‰ç¼€åçš„ "ç†è§£" ä¸æ˜¯ explain è¯·æ±‚
        
        # ğŸ†• quiz å…³é”®è¯çš„æ’é™¤æ¨¡å¼
        # "é—®é¢˜" åœ¨è¿™äº›ä¸Šä¸‹æ–‡ä¸­ä¸åº”è¯¥è§¦å‘ quizï¼ˆæ˜¯åœ¨è®¨è®ºé—®é¢˜ï¼Œè€Œä¸æ˜¯è¯·æ±‚å‡ºé¢˜ï¼‰
        quiz_exclude_patterns = [
            'è¿™ç±»é—®é¢˜',      # "å­¦ç”Ÿåœ¨è¿™ç±»é—®é¢˜ä¸­çŠ¯ä»€ä¹ˆé”™è¯¯" â†’ è®¨è®ºï¼Œä¸æ˜¯å‡ºé¢˜
            'è¿™ä¸ªé—®é¢˜',      # "è¿™ä¸ªé—®é¢˜æ€ä¹ˆç†è§£" â†’ è®¨è®º
            'ä»€ä¹ˆé—®é¢˜',      # "è¿™æœ‰ä»€ä¹ˆé—®é¢˜" â†’ è¯¢é—®é—®é¢˜
            'å¸¸è§é—®é¢˜',      # "æœ‰å“ªäº›å¸¸è§é—®é¢˜" â†’ è®¨è®º
            'é—®é¢˜ä¸­',        # "åœ¨é—®é¢˜ä¸­çŠ¯çš„é”™è¯¯" â†’ è®¨è®º
            'é—®é¢˜é‡Œ',        # "é—®é¢˜é‡Œçš„æ¦‚å¿µ" â†’ è®¨è®º
            'é—®é¢˜ä¸Š',        # "åœ¨è¿™ä¸ªé—®é¢˜ä¸Š" â†’ è®¨è®º
            'çš„é—®é¢˜',        # "å­¦ç”Ÿçš„é—®é¢˜" â†’ è®¨è®º
            'çŠ¯ä»€ä¹ˆé”™',      # "çŠ¯ä»€ä¹ˆé”™è¯¯" â†’ è¯¢é—®é”™è¯¯ï¼Œä¸æ˜¯å‡ºé¢˜
            'è§£ç­”',          # "å¸®æˆ‘è§£ç­”" â†’ è¯·æ±‚è§£ç­”ï¼Œä¸æ˜¯å‡ºé¢˜
            'å¸®æˆ‘åš',        # "å¸®æˆ‘åšè¿™é“é¢˜" â†’ è¯·æ±‚å¸®åšï¼Œä¸æ˜¯å‡ºé¢˜
            'å›ç­”',          # "å›ç­”é—®é¢˜" â†’ è¯·æ±‚å›ç­”ï¼Œä¸æ˜¯å‡ºé¢˜
            'common mistake', # "common mistakes" â†’ è®¨è®ºé”™è¯¯
            'this problem',   # "this problem" â†’ è®¨è®º
            'this question',  # "this question" â†’ è®¨è®º
            'that question',  # "that question" â†’ è®¨è®º/æŒ‡ä»£ç°æœ‰é¢˜ç›®
            'the question',   # "the question" â†’ è®¨è®º/æŒ‡ä»£ç°æœ‰é¢˜ç›®
            'solve',          # "solve that question" â†’ è¯·æ±‚è§£ç­”
            'answer',         # "answer that question" â†’ è¯·æ±‚è§£ç­”
            'help me with',   # "help me with this question" â†’ è¯·æ±‚å¸®åŠ©
            'work out',       # "work out this question" â†’ è¯·æ±‚è®¡ç®—
            'figure out',     # "figure out this question" â†’ è¯·æ±‚è§£ç­”
            'explain',        # "explain this question" â†’ è¯·æ±‚è§£é‡Š
        ]
        
        # ğŸ†• ä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯æ£€æµ‹å…³é”®è¯
        matched_skills = []
        for skill_name, keywords in skill_keywords.items():
            # ğŸ”¥ quiz ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥æ’é™¤æ¨¡å¼
            if skill_name == 'quiz':
                # å…ˆæ£€æŸ¥æ˜¯å¦å‘½ä¸­æ’é™¤æ¨¡å¼
                if any(exc in message_cleaned.lower() for exc in quiz_exclude_patterns):
                    logger.debug(f"âš ï¸ Quiz excluded: message contains exclusion pattern")
                    continue
            
            if any(kw in message_cleaned for kw in keywords):
                matched_skills.append(skill_name)
        
        # ğŸ”¥ ç‰¹æ®Šå¤„ç† explain çš„ä¸Šä¸‹æ–‡å…³é”®è¯
        if 'explain' not in matched_skills:
            for kw in explain_contextual_keywords:
                if kw in message_cleaned:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ’é™¤å‰ç¼€
                    kw_index = message_cleaned.find(kw)
                    if kw_index > 0:
                        prefix = message_cleaned[max(0, kw_index-2):kw_index]
                        if any(exc in prefix for exc in explain_exclude_prefixes):
                            logger.debug(f"âš ï¸ '{kw}' has exclude prefix '{prefix}', not matching explain")
                            continue
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŠ¨ä½œè¯­å¢ƒï¼ˆå¦‚ "å¸®æˆ‘ç†è§£"ã€"æˆ‘è¦ç†è§£"ï¼‰
                    action_patterns = [f'å¸®æˆ‘{kw}', f'æˆ‘è¦{kw}', f'æƒ³{kw}', f'æ¥{kw}', f'å»{kw}']
                    if any(p in message_cleaned for p in action_patterns):
                        matched_skills.append('explain')
                        logger.debug(f"âœ… '{kw}' in action context, matching explain")
                        break
        
        # ğŸ”¥ ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæ˜ç¡®æåˆ° learning_bundle å…³é”®è¯ï¼Œç›´æ¥è¿”å› learning_plan_skill
        if 'learning_bundle' in matched_skills:
            logger.info(f"ğŸ“¦ Detected explicit learning_bundle keywords")
            
            # æå–å‚æ•°
            params = {}
            
            # ğŸ”¥ ä½¿ç”¨å¢å¼ºçš„topicæå–ï¼ˆæ”¯æŒ"å…³äºXçš„å­¦ä¹ åŒ…"ç­‰æ¨¡å¼ï¼‰
            topic = None
            topic_patterns = [
                # ğŸ†• æœ€é«˜ä¼˜å…ˆçº§ï¼šæ˜ç¡®çš„"ä¸»é¢˜æ˜¯XXX"ç»“æ„
                r'ä¸»é¢˜æ˜¯(.+?)(?:[ï¼Œã€‚ï¼ï¼Ÿ]|$)',                      # "ä¸»é¢˜æ˜¯é‡å­åŠ›å­¦"
                r'å…³äº(.+?)(?:çš„|ï¼Œ)',                              # "å…³äºDNAçš„å­¦ä¹ åŒ…"
                r'(.+?)(?:çš„|ï¼Œ)(?:å­¦ä¹ åŒ…|å­¦ä¹ èµ„æ–™|å­¦ä¹ ææ–™|å­¦ä¹ è®¡åˆ’)', # "DNAçš„å­¦ä¹ åŒ…" / "DNAçš„å­¦ä¹ è®¡åˆ’"
                r'ç»™æˆ‘(?:ä¸€?ä¸ª)?(.+?)(?:çš„|ï¼Œ)?(?:å­¦ä¹ åŒ…|å­¦ä¹ èµ„æ–™|å­¦ä¹ è®¡åˆ’)', # "ç»™æˆ‘DNAå­¦ä¹ åŒ…"
            ]
            
            # ğŸ”¥ æŠ€èƒ½ç›¸å…³è¯ï¼Œå¦‚æœ topic åŒ…å«è¿™äº›è¯ï¼Œè¯´æ˜æå–é”™è¯¯
            skill_keywords_in_topic = ['é—ªå¡', 'æµ‹éªŒ', 'ç¬”è®°', 'é¢˜', 'å¯¼å›¾', 'è§£é‡Š', 'è®²è§£', 'å­¦ä¹ åŒ…']
            
            for pattern in topic_patterns:
                match = re.search(pattern, message)
                if match:
                    topic_candidate = match.group(1).strip()
                    topic_candidate = self._clean_topic(topic_candidate)
                    # ğŸ”¥ éªŒè¯ topic ä¸åŒ…å«æŠ€èƒ½å…³é”®è¯ï¼ˆå¦åˆ™è¯´æ˜æå–é”™è¯¯ï¼‰
                    if len(topic_candidate) >= 2 and not any(kw in topic_candidate for kw in skill_keywords_in_topic):
                        topic = topic_candidate
                        break
            
            # ğŸ”¥ å¦‚æœä¸Šé¢çš„patternæ²¡åŒ¹é…åˆ°æˆ–æå–çš„topicæ— æ•ˆï¼Œä¼˜å…ˆä½¿ç”¨ current_topic
            # å› ä¸º learning_bundle æ¶ˆæ¯é€šå¸¸æ˜¯çº¯åŠ¨ä½œè¯·æ±‚ï¼ˆå¦‚"å¸®æˆ‘ç”Ÿæˆé—ªå¡+æµ‹éªŒ+ç¬”è®°çš„å­¦ä¹ åŒ…"ï¼‰
            if not topic and current_topic:
                topic = current_topic
                logger.info(f"ğŸ“¦ Using current_topic for learning_bundle: {current_topic}")
            
            # æœ€åå°è¯•é€šç”¨çš„topicæå–ï¼ˆä¸å¤ªå¯èƒ½æˆåŠŸï¼Œä½†ä½œä¸ºå…œåº•ï¼‰
            if not topic:
                extracted = self._extract_topic(message, {})
                if extracted and len(extracted) >= 2:
                    topic = extracted
            
            if topic:
                params['topic'] = topic
            
            # ğŸ†• æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†å…·ä½“çš„æ­¥éª¤ï¼ˆå¦‚"åŒ…å«è®²è§£ã€3å¼ é—ªå¡å’Œ2é“é¢˜"ï¼‰
            matched_skills_filtered = [s for s in matched_skills if s != 'learning_bundle']
            if matched_skills_filtered:
                step_mapping = {
                    'explain': 'explain',
                    'quiz': 'quiz',
                    'flashcard': 'flashcard',
                    'notes': 'notes',
                    'mindmap': 'mindmap'
                }
                params['required_steps'] = [step_mapping[skill] for skill in matched_skills_filtered if skill in step_mapping]
                logger.info(f"ğŸ“‹ User specified steps in learning bundle: {params['required_steps']}")
            
            # è¿”å› learning_plan_skill åŒ¹é…
            return SkillMatch(
                skill_id='learning_plan_skill',
                confidence=0.95,  # é«˜ç½®ä¿¡åº¦
                parameters=params,
                matched_keywords=['learning_bundle']
            )
        
        # å¦‚æœæ£€æµ‹åˆ° 2 ä¸ªæˆ–ä»¥ä¸Šçš„æŠ€èƒ½å…³é”®è¯ï¼ˆä¸åŒ…æ‹¬ learning_bundleï¼‰ï¼Œåˆ¤å®šä¸ºæ··åˆæ„å›¾
        # è¿‡æ»¤æ‰ learning_bundleï¼Œå› ä¸ºå®ƒå·²ç»åœ¨ä¸Šé¢å¤„ç†äº†
        matched_skills_filtered = [s for s in matched_skills if s != 'learning_bundle']
        
        # ğŸ†• å½“æœ‰å¼•ç”¨æ—¶ï¼Œä¸åº”è½»æ˜“è§¦å‘æ··åˆæ„å›¾
        # ä¾‹å¦‚ï¼š"æ ¹æ®ç¬¬äº”é“é¢˜ï¼Œç»™å‡ºè§£é‡Š" â†’ åªæ˜¯è§£é‡Šé‚£é“é¢˜ï¼Œä¸æ˜¯è¦ç”Ÿæˆæ–°é¢˜+è§£é‡Š
        if has_reference and len(matched_skills_filtered) >= 2:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„ç”ŸæˆåŠ¨ä½œè¯ï¼ˆéœ€è¦æ›´ç²¾ç¡®çš„åŒ¹é…ï¼Œé¿å… "ç»™å‡º" è¢«è¯¯è¯†åˆ«ï¼‰
            generation_patterns = [
                r'(?<!ç»™)å‡º\d*[é“å¼ ä¸ª]',  # "å‡ºä¸‰é“" ä½†ä¸æ˜¯ "ç»™å‡º"
                r'ç”Ÿæˆ',
                r'åˆ›å»º',
                r'åš\d*[é“å¼ ä¸ª]',   # "åšä¸‰é“"
                r'å†™\d*[é“å¼ ä¸ª]',   # "å†™ä¸‰å¼ "
                r'ç”»',
                r'æ¥\d*[é“å¼ ä¸ª]',   # "æ¥ä¸‰é“"
                r'å†æ¥',
                r'å†å‡º',
            ]
            has_generation_action = any(re.search(p, message_cleaned) for p in generation_patterns)
            
            if not has_generation_action:
                logger.info(f"ğŸ“ Reference with no generation action, skipping mixed intent: {matched_skills_filtered}")
                return None  # ä¸è§¦å‘æ··åˆæ„å›¾ï¼Œè®©åç»­çš„å•ä¸€æ„å›¾æ£€æµ‹å¤„ç†
        
        if len(matched_skills_filtered) >= 2:
            logger.info(f"ğŸ”€ Mixed intent detected: {matched_skills_filtered}")
            
            # æå–å‚æ•°
            params = {}
            
            # ğŸ†• Phase 4.2: æ·»åŠ  required_stepsï¼Œè®© Plan Skill çŸ¥é“è¦æ‰§è¡Œå“ªäº›æ­¥éª¤
            step_mapping = {
                'explain': 'explain',
                'quiz': 'quiz',
                'flashcard': 'flashcard',
                'notes': 'notes',
                'mindmap': 'mindmap'
            }
            params['required_steps'] = [step_mapping[skill] for skill in matched_skills_filtered if skill in step_mapping]
            logger.info(f"ğŸ“‹ Required steps: {params['required_steps']}")
            
            # æå–ä¸»é¢˜ - ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•
            # å°è¯•ä»å¸¸è§æ¨¡å¼ä¸­æå–ä¸»é¢˜
            topic = None
            topic_patterns = [
                r'è§£é‡Š(?:ä¸€?ä¸‹?)?(.+?)(?:ï¼Œ|å¹¶|ç„¶å|å†)',       # "è§£é‡Šç‰›é¡¿ç¬¬äºŒå®šå¾‹ï¼Œå¹¶..."
                r'è®²è§£(?:ä¸€?ä¸‹?)?(.+?)(?:ï¼Œ|å¹¶|ç„¶å|å†)',       # "è®²è§£ç‰›é¡¿ç¬¬äºŒå®šå¾‹ï¼Œå¹¶..."
                r'ç†è§£(?:ä¸€?ä¸‹?)?(.+?)(?:ï¼Œ|å¹¶|ç„¶å|å†)',       # "ç†è§£ç‰›é¡¿ç¬¬äºŒå®šå¾‹ï¼Œå¹¶..."
                r'äº†è§£(?:ä¸€?ä¸‹?)?(.+?)(?:ï¼Œ|å¹¶|ç„¶å|å†)',       # "äº†è§£ç‰›é¡¿ç¬¬äºŒå®šå¾‹ï¼Œå¹¶..."
                r'å­¦ä¹ (?:ä¸€?ä¸‹?)?(.+?)(?:ï¼Œ|å¹¶|ç„¶å|å†)',       # "å­¦ä¹ ç‰›é¡¿ç¬¬äºŒå®šå¾‹ï¼Œå¹¶..."
                r'å…³äº(.+?)(?:çš„|ï¼Œ)',                         # "å…³äºç‰›é¡¿ç¬¬äºŒå®šå¾‹çš„..."
                r'(.+?)(?:çš„|ï¼Œ)(?:è®²è§£|è§£é‡Š|ç†è§£|é¢˜ç›®|é—ªå¡)',  # "ç‰›é¡¿ç¬¬äºŒå®šå¾‹çš„è®²è§£..."
            ]
            
            for pattern in topic_patterns:
                match = re.search(pattern, message)
                if match:
                    topic = match.group(1).strip()
                    topic = self._clean_topic(topic)
                    if len(topic) >= 2:
                        params['topic'] = topic
                        break
            
            # ğŸ”¥ å¦‚æœæ²¡æœ‰æå–åˆ°ä¸»é¢˜ï¼Œä½¿ç”¨ current_topic
            if not topic and current_topic:
                topic = current_topic
                params['topic'] = topic
                logger.info(f"ğŸ“š Using current_topic for mixed intent: {topic}")
            
            # æå–æ•°é‡å‚æ•°
            quantity_match = re.search(r'(\d+)\s*[é“ä¸ªå¼ ä»½]', message)
            if quantity_match:
                quantity_value = int(quantity_match.group(1))
                # æ ¹æ®æ¶ˆæ¯ä¸­çš„å…³é”®è¯åˆ¤æ–­æ•°é‡å±äºå“ªä¸ªæŠ€èƒ½
                if 'quiz' in matched_skills_filtered:
                    params['quiz_quantity'] = quantity_value
                if 'flashcard' in matched_skills_filtered:
                    params['flashcard_quantity'] = quantity_value
            
            # è¿”å› learning_plan_skill åŒ¹é…
            return SkillMatch(
                skill_id='learning_plan_skill',
                confidence=0.90,  # é«˜ç½®ä¿¡åº¦
                parameters=params,
                matched_keywords=matched_skills_filtered
            )
        
        return None
    
    def reload(self):
        """é‡æ–°åŠ è½½æ‰€æœ‰ Skillsï¼ˆç”¨äºçƒ­æ›´æ–°ï¼‰"""
        logger.info("ğŸ”„ Reloading skills...")
        self._skills.clear()
        self._intent_map.clear()
        self._skill_metadata.clear()
        self._load_skills()
        self._load_skill_metadata()
        logger.info(f"âœ… Reloaded {len(self._skills)} skills ({len(self._skill_metadata)} with metadata)")


# å…¨å±€å•ä¾‹
_registry_instance: Optional[SkillRegistry] = None


def get_skill_registry() -> SkillRegistry:
    """
    è·å–å…¨å±€ SkillRegistry å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        SkillRegistry å®ä¾‹
    """
    global _registry_instance
    if _registry_instance is None:
        _registry_instance = SkillRegistry()
    return _registry_instance

