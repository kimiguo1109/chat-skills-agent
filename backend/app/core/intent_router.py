"""
Intent Router - æ„å›¾è¯†åˆ«è·¯ç”±å™¨

è´Ÿè´£è§£æç”¨æˆ·è¾“å…¥ï¼Œè¯†åˆ«å­¦ä¹ æ„å›¾å¹¶è¿”å›ç»“æ„åŒ–ç»“æœã€‚

ä¼˜åŒ–ç­–ç•¥ï¼šè§„åˆ™å¼•æ“ + LLM Fallback
- 70% æ˜ç¡®è¯·æ±‚: ä½¿ç”¨è§„åˆ™å¼•æ“ (0 tokens)
- 30% æ¨¡ç³Šè¯·æ±‚: ä½¿ç”¨ LLM (ç²¾ç®€ prompt)
- å¹³å‡ token èŠ‚çœ: ~86%
"""
import logging
import json
from pathlib import Path
from typing import Optional, Dict, List, Any
from datetime import datetime

from ..services.gemini import GeminiClient
from ..models.intent import IntentResult, MemorySummary
from ..config import settings
from .rule_based_classifier import RuleBasedIntentClassifier
from .skill_registry import SkillRegistry, get_skill_registry

logger = logging.getLogger(__name__)


class IntentRouter:
    """æ„å›¾è¯†åˆ«è·¯ç”±å™¨"""
    
    # ç½®ä¿¡åº¦é˜ˆå€¼
    CONFIDENCE_THRESHOLD = 0.6
    
    # Prompt æ¨¡æ¿è·¯å¾„
    PROMPT_TEMPLATE_PATH = Path(__file__).parent.parent / "prompts" / "intent_router.txt"
    
    # Intent Router JSON è¾“å‡ºä¿å­˜è·¯å¾„
    INTENT_OUTPUT_PATH = Path(__file__).parent.parent.parent / "memory_storage" / "intent_router_output.json"
    
    def __init__(
        self,
        gemini_client: Optional[GeminiClient] = None,
        use_rule_engine: bool = True,
        save_output: bool = True
    ):
        """
        åˆå§‹åŒ– Intent Router
        
        Args:
            gemini_client: Gemini API å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸æä¾›åˆ™åˆ›å»ºæ–°å®ä¾‹
            use_rule_engine: æ˜¯å¦å¯ç”¨è§„åˆ™å¼•æ“ä¼˜åŒ–ï¼ˆé»˜è®¤ Trueï¼‰
            save_output: æ˜¯å¦ä¿å­˜ Intent Router çš„ JSON è¾“å‡ºï¼ˆé»˜è®¤ Trueï¼‰
        """
        self.gemini_client = gemini_client or GeminiClient()
        self.use_rule_engine = use_rule_engine
        self.save_output = save_output
        
        # ğŸ†• Phase 4: åˆå§‹åŒ– Skill Registry (0-token matching)
        self.skill_registry = get_skill_registry()
        
        # åˆå§‹åŒ–è§„åˆ™å¼•æ“
        if self.use_rule_engine:
            self.rule_classifier = RuleBasedIntentClassifier()
            logger.info("âœ… IntentRouter initialized with Skill Registry + Rule Engine (Phase 4)")
        else:
            self.rule_classifier = None
            logger.info("âœ… IntentRouter initialized with Skill Registry only (Phase 4)")
        
        self.prompt_template = self._load_prompt_template()
        
        # ç»Ÿè®¡æ•°æ® - ä»æ–‡ä»¶åŠ è½½å†å²ç»Ÿè®¡
        self.stats = self._load_stats_from_file()
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        if self.save_output:
            self.INTENT_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    def _load_prompt_template(self) -> str:
        """
        åŠ è½½ prompt æ¨¡æ¿
        
        Returns:
            str: Prompt æ¨¡æ¿å†…å®¹
        
        Raises:
            FileNotFoundError: å¦‚æœæ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨
        """
        try:
            with open(self.PROMPT_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
                template = f.read()
            logger.debug(f"ğŸ“„ Loaded prompt template: {self.PROMPT_TEMPLATE_PATH}")
            return template
        except FileNotFoundError:
            logger.error(f"âŒ Prompt template not found: {self.PROMPT_TEMPLATE_PATH}")
            raise
    
    def _load_stats_from_file(self) -> Dict[str, int]:
        """ä»æ–‡ä»¶åŠ è½½å†å²ç»Ÿè®¡æ•°æ®"""
        try:
            if self.INTENT_OUTPUT_PATH.exists():
                with open(self.INTENT_OUTPUT_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if "stats" in data:
                        stats = data["stats"]
                        # æå–æ•°å€¼ï¼ˆå»æ‰ç™¾åˆ†å·ç­‰ï¼‰
                        return {
                            "total_requests": stats.get("total_requests", 0),
                            "rule_based_success": stats.get("rule_based_success", 0),
                            "llm_fallback": stats.get("llm_fallback", 0)
                        }
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to load stats from file: {e}")
        
        # é»˜è®¤å€¼
        return {
            "total_requests": 0,
            "rule_based_success": 0,
            "llm_fallback": 0
        }
    
    def _save_intent_output(
        self,
        user_message: str,
        intent_results: list[IntentResult],
        method: str,
        tokens_used: int = 0
    ):
        """
        ä¿å­˜ Intent Router çš„è¾“å‡ºåˆ° JSON æ–‡ä»¶
        
        Args:
            user_message: ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
            intent_results: Intent Router è¯†åˆ«çš„ç»“æœåˆ—è¡¨
            method: è¯†åˆ«æ–¹æ³• ("rule_engine" æˆ– "llm_fallback")
            tokens_used: æ¶ˆè€—çš„ token æ•°é‡
        """
        if not self.save_output:
            return
        
        try:
            # æ„å»ºè¾“å‡ºæ•°æ®
            output_data = {
                "timestamp": datetime.now().isoformat(),
                "user_message": user_message,
                "method": method,
                "tokens_used": tokens_used,
                "results": []
            }
            
            # æ·»åŠ æ¯ä¸ª intent result
            for result in intent_results:
                result_dict = {
                    "intent": result.intent,
                    "topic": result.topic,
                    "confidence": result.confidence,
                    "parameters": result.parameters
                }
                output_data["results"].append(result_dict)
            
            # è¯»å–ç°æœ‰å†å²ï¼ˆä¿ç•™æœ€è¿‘10æ¡ï¼‰
            history = []
            if self.INTENT_OUTPUT_PATH.exists():
                try:
                    with open(self.INTENT_OUTPUT_PATH, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                        history = existing_data.get("history", [])
                except Exception as e:
                    logger.warning(f"âš ï¸  Failed to load existing intent output history: {e}")
            
            # æ·»åŠ å½“å‰ç»“æœåˆ°å†å²
            history.append(output_data)
            
            # åªä¿ç•™æœ€è¿‘10æ¡è®°å½•
            history = history[-10:]
            
            # æ„å»ºå®Œæ•´æ•°æ®ç»“æ„
            full_data = {
                "description": "Intent Router å®æ—¶è¾“å‡ºè®°å½• (Phase 3 æ¶æ„)",
                "latest": output_data,
                "history": history,
                "stats": {
                    "total_requests": self.stats["total_requests"],
                    "rule_based_success": self.stats["rule_based_success"],
                    "llm_fallback": self.stats["llm_fallback"],
                    "rule_success_rate": f"{self.stats['rule_based_success']/self.stats['total_requests']*100:.1f}%" if self.stats['total_requests'] > 0 else "0%"
                }
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(self.INTENT_OUTPUT_PATH, 'w', encoding='utf-8') as f:
                json.dump(full_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ Intent output saved to {self.INTENT_OUTPUT_PATH}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save intent output: {e}")
    
    def _format_prompt(
        self,
        message: str,
        memory_summary: Optional[str] = None,
        last_artifact_summary: Optional[str] = None
    ) -> str:
        """
        æ ¼å¼åŒ– prompt (Phase 3: Minimal Context)
        
        Phase 3 ä¼˜åŒ–ï¼šåªä¼ é€’æœ€å°åŒ–ä¸Šä¸‹æ–‡æ ‡è®°ï¼Œä¸ä¼ é€’å®Œæ•´å†…å®¹
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            memory_summary: è®°å¿†æ‘˜è¦ï¼ˆå­—ç¬¦ä¸²ï¼‰- ä»ä¸­æå– top preference
            last_artifact_summary: ä¸Šä¸€è½® artifact æ‘˜è¦ - åªæ£€æŸ¥æ˜¯å¦å­˜åœ¨
        
        Returns:
            str: æ ¼å¼åŒ–åçš„ prompt
        """
        # Phase 3: æå–æœ€å°åŒ–ä¸Šä¸‹æ–‡æ ‡è®°
        
        # 1. æå–ç”¨æˆ·åå¥½ï¼ˆtop preference only, ~2 tokensï¼‰
        user_preference_top = "null"
        if memory_summary and "prefers" in memory_summary:
            # ä» memory_summary ä¸­æå–åå¥½
            # ç¤ºä¾‹: "[User Preference: prefers flashcards (75%)]"
            try:
                if "flashcard" in memory_summary.lower():
                    user_preference_top = "flashcard"
                elif "quiz" in memory_summary.lower():
                    user_preference_top = "quiz"
                elif "explain" in memory_summary.lower():
                    user_preference_top = "explain"
                elif "mindmap" in memory_summary.lower():
                    user_preference_top = "mindmap"
                elif "notes" in memory_summary.lower():
                    user_preference_top = "notes"
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to extract preference: {e}")
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¸€è½®å†…å®¹ï¼ˆ~1 tokenï¼‰
        has_last_artifact = "false"
        if last_artifact_summary and last_artifact_summary != "No previous interaction.":
            has_last_artifact = "true"
        
        # 3. æ ¼å¼åŒ– promptï¼ˆä½¿ç”¨æœ€å°åŒ–æ ‡è®°ï¼‰
        formatted = self.prompt_template.format(
            message=message,
            user_preference_top=user_preference_top,
            has_last_artifact=has_last_artifact
        )
        
        logger.debug(f"ğŸ“ Context flags: preference={user_preference_top}, has_artifact={has_last_artifact}")
        
        return formatted
    
    async def parse(
        self,
        message: str,
        memory_summary: Optional[str] = None,
        last_artifact_summary: Optional[str] = None,
        current_topic: Optional[str] = None,
        session_topics: Optional[list] = None
    ) -> list[IntentResult]:
        """
        è§£æç”¨æˆ·æ¶ˆæ¯ï¼Œè¯†åˆ«æ„å›¾
        
        ä¼˜åŒ–æµç¨‹ï¼š
        1. å…ˆå°è¯•è§„åˆ™å¼•æ“ (0 tokens)
        2. å¤±è´¥åˆ™å›é€€åˆ° LLM (ç²¾ç®€ prompt)
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            memory_summary: å¯é€‰çš„è®°å¿†æ‘˜è¦ï¼Œç”¨äºå¢å¼ºè¯†åˆ«å‡†ç¡®åº¦
            last_artifact_summary: ä¸Šä¸€è½® artifact æ‘˜è¦ï¼ˆç”¨äºä¸Šä¸‹æ–‡å¼•ç”¨ï¼‰
            current_topic: å½“å‰å¯¹è¯ä¸»é¢˜ï¼ˆä» session_contextï¼‰
            session_topics: å†å²topicsåˆ—è¡¨ï¼ˆä» session_contextï¼‰
        
        Returns:
            list[IntentResult]: æ„å›¾è¯†åˆ«ç»“æœåˆ—è¡¨
        
        Raises:
            Exception: å¦‚æœ API è°ƒç”¨å¤±è´¥
        """
        logger.info(f"ğŸ” Parsing intent for message: {message[:50]}...")
        if current_topic:
            logger.info(f"ğŸ“š Current topic from context: {current_topic}")
        if session_topics:
            logger.info(f"ğŸ“š Session topics: {session_topics}")
        
        # ç»Ÿè®¡
        self.stats["total_requests"] += 1
        
        # ============= ğŸš€ Phase 4: ä¼˜å…ˆä½¿ç”¨ Skill Registry (0 tokens) =============
        skill_match = self.skill_registry.match_message(message, current_topic, session_topics)
        
        # ğŸ”¥ å¤„ç† clarification needed æƒ…å†µ
        if skill_match and skill_match.skill_id == "clarification_needed":
            logger.warning(f"âš ï¸  Clarification needed: {skill_match.parameters.get('clarification_reason')}")
            
            # è¿”å› clarification intent
            clarification_result = IntentResult(
                intent="clarification_needed",
                topic=None,
                target_artifact=None,
                confidence=1.0,
                raw_text=message,
                parameters=skill_match.parameters
            )
            
            return [clarification_result]
        
        if skill_match and skill_match.confidence >= 0.8:
            # Skill Registry æˆåŠŸåŒ¹é…ï¼
            logger.info(
                f"âœ… Skill Registry Match: {skill_match.skill_id} "
                f"(confidence: {skill_match.confidence:.2f}) | "
                f"Keywords: {skill_match.matched_keywords}"
            )
            
            # å°† skill_id è½¬æ¢ä¸º intentï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            intent_mapping = {
                "quiz_skill": "quiz_request",
                "explain_skill": "explain_request",
                "flashcard_skill": "flashcard_request",
                "notes_skill": "notes",
                "mindmap_skill": "mindmap_request",
                "learning_plan_skill": "learning_bundle"
            }
            
            intent = intent_mapping.get(skill_match.skill_id, skill_match.skill_id)
            
            # æ„å»º IntentResult
            intent_result = IntentResult(
                intent=intent,
                topic=skill_match.parameters.get('topic'),
                target_artifact=None,
                confidence=skill_match.confidence,
                raw_text=message,
                parameters=skill_match.parameters
            )
            
            logger.info(
                f"ğŸ“Š Token Usage (Skill Registry) | Input: 0 | Output: 0 | Total: 0 | "
                f"Time: <0.001s | Method: Skill Registry (Phase 4)"
            )
            logger.info(f"ğŸ’° Tokens Saved: ~3,000 | 100% savings")
            
            # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º
            self._save_intent_output(
                user_message=message,
                intent_results=[intent_result],
                method="skill_registry",
                tokens_used=0
            )
            
            return [intent_result]
        else:
            if skill_match:
                logger.debug(
                    f"âš ï¸  Skill Registry low confidence: {skill_match.skill_id} "
                    f"({skill_match.confidence:.2f}), falling back..."
                )
            else:
                logger.debug("âš ï¸  No Skill Registry match, falling back...")
        
        # ============= ğŸš€ Fallback 1: è§„åˆ™å¼•æ“ (0 tokens) =============
        if self.use_rule_engine and self.rule_classifier:
            rule_result = self.rule_classifier.classify(message, memory_summary)
            
            if rule_result:
                # è§„åˆ™å¼•æ“æˆåŠŸè¯†åˆ«ï¼
                self.stats["rule_based_success"] += 1
                
                # è®¡ç®—èŠ‚çœçš„ tokensï¼ˆä¼°ç®—ï¼‰
                estimated_saved_tokens = 3000  # Intent Router å¹³å‡æ¶ˆè€—
                
                logger.info(
                    f"ğŸ“Š Token Usage (Rule-Based) | Input: 0 | Output: 0 | Total: 0 | "
                    f"Time: <0.01s | Method: Rule Engine"
                )
                logger.info(
                    f"ğŸ’° Tokens Saved: ~{estimated_saved_tokens:,} | "
                    f"Success rate: {self.stats['rule_based_success']}/{self.stats['total_requests']} "
                    f"({self.stats['rule_based_success']/self.stats['total_requests']*100:.1f}%)"
                )
                
                # è½¬æ¢ä¸º IntentResult å¯¹è±¡
                intent_result = IntentResult(
                    intent=rule_result["intent"],
                    topic=rule_result["topic"],
                    target_artifact=rule_result["target_artifact"],
                    confidence=rule_result["confidence"],
                    raw_text=rule_result["raw_text"],
                    parameters=rule_result.get("parameters", {})
                )
                
                logger.info(f"âœ… Intent parsed: {intent_result.intent} (confidence: {intent_result.confidence:.2f}, topic: {intent_result.topic})")
                
                # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º
                self._save_intent_output(
                    user_message=message,
                    intent_results=[intent_result],
                    method="rule_engine",
                    tokens_used=0
                )
                
                return [intent_result]
        
        # ============= è§„åˆ™å¼•æ“å¤±è´¥ï¼Œå›é€€åˆ° LLM =============
        self.stats["llm_fallback"] += 1
        logger.info(
            f"âš ï¸  Rule-based classification FAILED, falling back to LLM | "
            f"Fallback rate: {self.stats['llm_fallback']}/{self.stats['total_requests']} "
            f"({self.stats['llm_fallback']/self.stats['total_requests']*100:.1f}%)"
        )
        
        # æ ¼å¼åŒ– prompt
        prompt = self._format_prompt(message, memory_summary, last_artifact_summary)
        
        try:
            # è°ƒç”¨ LLM APIï¼ˆæ”¯æŒ Gemini æˆ– Kimiï¼‰
            response = await self.gemini_client.generate(
                prompt=prompt,
                model=settings.KIMI_MODEL if settings.KIMI_API_KEY else settings.GEMINI_MODEL,
                response_format="json",
                max_tokens=200,  # Intent recognition needs short output
                temperature=0.3,   # Lower temperature for more consistent classification
                thinking_budget=0,  # ğŸ”¥ Intent routing ä¸éœ€è¦ thinkingï¼ˆèŠ‚çœ tokensï¼‰
                return_thinking=False
            )
            
            # ğŸ”¥ å…¼å®¹æ–°ç‰ˆ generate è¿”å›æ ¼å¼ï¼šDict["content", "thinking", "usage"]
            response_text = response.get("content", response) if isinstance(response, dict) else response
            
            # ğŸ› DEBUG: Log the raw LLM response
            logger.debug(f"ğŸ” LLM Response (raw): {response_text[:500]}")  # First 500 chars
            
            # è§£æ JSON å“åº”
            response_data = json.loads(response_text)
            logger.debug(f"ğŸ” LLM Response (parsed): {response_data}")
            
            # æ„å›¾æ˜ å°„ï¼šç»Ÿä¸€åŒ–ä¸åŒçš„è¡¨è¾¾
            intent_mapping = {
                "quiz": "quiz_request",
                "explain": "explain_request",
                "flashcard": "flashcard_request",
                "learning_bundle": "learning_bundle",  # ä¿æŒåŸæ ·ï¼Œä¸skillé…ç½®ä¸€è‡´
                "mindmap": "mindmap_request",  # æ€ç»´å¯¼å›¾
                "other": "other"
            }
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆè¯·æ±‚ï¼ˆæœ‰ "intents" æ•°ç»„ï¼‰
            if "intents" in response_data:
                logger.info(f"ğŸ”€ Detected MIXED REQUEST with {len(response_data['intents'])} intents")
                results = []
                
                for idx, intent_data in enumerate(response_data["intents"]):
                    intent = intent_data.get("intent", "other")
                    topic = intent_data.get("topic")
                    target_artifact = intent_data.get("target_artifact")
                    confidence = float(intent_data.get("confidence", 0.5))
                    parameters = intent_data.get("parameters", {})
                    
                    # æ ‡å‡†åŒ– intent
                    normalized_intent = intent_mapping.get(intent, intent)
                    
                    # å¦‚æœæå–äº† quantity å‚æ•°ï¼Œè®°å½•æ—¥å¿—
                    if parameters.get("quantity"):
                        logger.info(f"ğŸ“Š Intent {idx+1}: Extracted quantity parameter: {parameters['quantity']}")
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    result = IntentResult(
                        intent=normalized_intent,
                        topic=topic,
                        target_artifact=target_artifact,
                        confidence=confidence,
                        raw_text=message,
                        parameters=parameters
                    )
                    
                    results.append(result)
                    logger.info(f"âœ… Intent {idx+1} parsed: {result.intent} (confidence: {result.confidence:.2f}, topic: {result.topic})")
                
                # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º (æ··åˆè¯·æ±‚)
                self._save_intent_output(
                    user_message=message,
                    intent_results=results,
                    method="llm_fallback",
                    tokens_used=1487  # ä¼°ç®—å€¼ï¼ŒPhase 3 ä¼˜åŒ–åçš„ LLM Fallback token æ¶ˆè€—
                )
                
                return results
            else:
                # å•ä¸ªè¯·æ±‚
                intent = response_data.get("intent", "other")
                topic = response_data.get("topic")
                target_artifact = response_data.get("target_artifact")
                confidence = float(response_data.get("confidence", 0.5))
                parameters = response_data.get("parameters", {})
                
                # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼é€»è¾‘
                if confidence < self.CONFIDENCE_THRESHOLD:
                    logger.warning(
                        f"âš ï¸ Low confidence ({confidence:.2f} < {self.CONFIDENCE_THRESHOLD}), "
                        f"falling back to 'other'"
                    )
                    intent = "other"
                    target_artifact = None
                
                # æ ‡å‡†åŒ– intent
                normalized_intent = intent_mapping.get(intent, intent)
                
                # å¦‚æœæå–äº† quantity å‚æ•°ï¼Œè®°å½•æ—¥å¿—
                if parameters.get("quantity"):
                    logger.info(f"ğŸ“Š Extracted quantity parameter: {parameters['quantity']}")
                
                # ğŸ› DEBUG: Log all extracted parameters
                logger.debug(f"ğŸ“Š All extracted parameters: {parameters}")
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                result = IntentResult(
                    intent=normalized_intent,
                    topic=topic,
                    target_artifact=target_artifact,
                    confidence=confidence,
                    raw_text=message,
                    parameters=parameters
                )
                
                logger.info(f"âœ… Intent parsed: {result.intent} (confidence: {result.confidence:.2f}, topic: {result.topic})")
                
                # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º (å•ä¸ªè¯·æ±‚)
                self._save_intent_output(
                    user_message=message,
                    intent_results=[result],
                    method="llm_fallback",
                    tokens_used=1487  # ä¼°ç®—å€¼ï¼ŒPhase 3 ä¼˜åŒ–åçš„ LLM Fallback token æ¶ˆè€—
                )
                
                return [result]  # è¿”å›å•å…ƒç´ åˆ—è¡¨ï¼Œä¿æŒæ¥å£ä¸€è‡´
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse JSON response: {e}")
            # è¿”å›é»˜è®¤çš„ "other" æ„å›¾ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰
            return [IntentResult(
                intent="other",
                topic=None,
                target_artifact=None,
                confidence=0.0,
                raw_text=message
            )]
        
        except Exception as e:
            logger.error(f"âŒ Intent parsing failed: {e}")
            raise
    
    async def parse_batch(
        self,
        messages: list[str],
        memory_summary: Optional[MemorySummary] = None
    ) -> list[IntentResult]:
        """
        æ‰¹é‡è§£æå¤šä¸ªæ¶ˆæ¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            memory_summary: è®°å¿†æ‘˜è¦
        
        Returns:
            list[IntentResult]: æ„å›¾è¯†åˆ«ç»“æœåˆ—è¡¨
        """
        results = []
        for message in messages:
            result = await self.parse(message, memory_summary)
            results.append(result)
        
        return results
    
    def get_optimization_stats(self) -> dict:
        """
        è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = self.stats["total_requests"]
        rule_success = self.stats["rule_based_success"]
        llm_fallback = self.stats["llm_fallback"]
        
        if total == 0:
            return {
                "total_requests": 0,
                "rule_based_success": 0,
                "llm_fallback": 0,
                "rule_success_rate": 0.0,
                "estimated_tokens_saved": 0,
                "average_tokens_per_request": 0
            }
        
        # ä¼°ç®— token èŠ‚çœ
        # è§„åˆ™å¼•æ“: 0 tokens
        # LLM: ~3,000 tokens
        estimated_saved = rule_success * 3000
        average_per_request = llm_fallback * 3000 / total
        
        return {
            "total_requests": total,
            "rule_based_success": rule_success,
            "llm_fallback": llm_fallback,
            "rule_success_rate": rule_success / total * 100 if total > 0 else 0,
            "estimated_tokens_saved": estimated_saved,
            "average_tokens_per_request": int(average_per_request)
        }

