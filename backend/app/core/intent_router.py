"""
Intent Router - æ„å›¾è¯†åˆ«è·¯ç”±å™¨ (Phase 4)

è´Ÿè´£è§£æç”¨æˆ·è¾“å…¥ï¼Œè¯†åˆ«å­¦ä¹ æ„å›¾å¹¶è¿”å›ç»“æ„åŒ–ç»“æœã€‚

Phase 4 æ¶æ„ï¼š100% 0-Token Intent Matching
- ä½¿ç”¨ Skill Registry è¿›è¡ŒåŸºäºå…³é”®è¯çš„æ™ºèƒ½åŒ¹é…
- æ”¯æŒå¤æ‚æ„å›¾ï¼ˆå­¦ä¹ åŒ…ã€æ··åˆè¯·æ±‚ï¼‰
- æ— éœ€ LLMï¼Œå®Œå…¨èŠ‚çœ tokens
- æœªåŒ¹é…è¯·æ±‚è¿”å› 'other' intentï¼ˆé—²èŠ/ä¸æ˜ç¡®è¯·æ±‚ï¼‰

Token èŠ‚çœï¼š100% (ç›¸æ¯” Phase 1/2)
"""
import logging
import json
from pathlib import Path
from typing import Optional, Dict, List, Any
from datetime import datetime

from ..models.intent import IntentResult, MemorySummary
from ..config import settings
from .rule_based_classifier import RuleBasedIntentClassifier
from .skill_registry import SkillRegistry, get_skill_registry
from .reference_resolver import get_reference_resolver

logger = logging.getLogger(__name__)


class IntentRouter:
    """æ„å›¾è¯†åˆ«è·¯ç”±å™¨"""
    
    # ç½®ä¿¡åº¦é˜ˆå€¼
    CONFIDENCE_THRESHOLD = 0.6
    
    # Prompt æ¨¡æ¿è·¯å¾„
    PROMPT_TEMPLATE_PATH = Path(__file__).parent.parent / "prompts" / "intent_router.txt"
    
    # Intent Router JSON è¾“å‡ºä¿å­˜è·¯å¾„
    INTENT_OUTPUT_PATH = Path(__file__).parent.parent.parent / "memory_storage" / "intent_router_output.json"
    
    def __init__(
        self,
        save_output: bool = True
    ):
        """
        åˆå§‹åŒ– Intent Router (Phase 4)
        
        Args:
            save_output: æ˜¯å¦ä¿å­˜ Intent Router çš„ JSON è¾“å‡ºï¼ˆé»˜è®¤ Trueï¼‰
        """
        self.save_output = save_output
        
        # ğŸ†• Phase 4: åˆå§‹åŒ– Skill Registry (0-token matching)
        self.skill_registry = get_skill_registry()
        logger.info("âœ… IntentRouter initialized with Skill Registry (Phase 4, 100% 0-token)")
        
        self.prompt_template = self._load_prompt_template()
        
        # ç»Ÿè®¡æ•°æ® - ä»æ–‡ä»¶åŠ è½½å†å²ç»Ÿè®¡
        self.stats = self._load_stats_from_file()
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        if self.save_output:
            self.INTENT_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    def _load_prompt_template(self) -> str:
        """
        åŠ è½½ prompt æ¨¡æ¿
        
        Returns:
            str: Prompt æ¨¡æ¿å†…å®¹
        
        Raises:
            FileNotFoundError: å¦‚æœæ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨
        """
        try:
            with open(self.PROMPT_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
                template = f.read()
            logger.debug(f"ğŸ“„ Loaded prompt template: {self.PROMPT_TEMPLATE_PATH}")
            return template
        except FileNotFoundError:
            logger.error(f"âŒ Prompt template not found: {self.PROMPT_TEMPLATE_PATH}")
            raise
    
    def _load_stats_from_file(self) -> Dict[str, int]:
        """ä»æ–‡ä»¶åŠ è½½å†å²ç»Ÿè®¡æ•°æ®"""
        try:
            if self.INTENT_OUTPUT_PATH.exists():
                with open(self.INTENT_OUTPUT_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if "stats" in data:
                        stats = data["stats"]
                        # æå–æ•°å€¼ï¼ˆå»æ‰ç™¾åˆ†å·ç­‰ï¼‰
                        return {
                            "total_requests": stats.get("total_requests", 0),
                            "rule_based_success": stats.get("rule_based_success", 0),
                            "llm_fallback": stats.get("llm_fallback", 0)
                        }
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to load stats from file: {e}")
        
        # é»˜è®¤å€¼
        return {
            "total_requests": 0,
            "rule_based_success": 0,
            "llm_fallback": 0
        }
    
    def _save_intent_output(
        self,
        user_message: str,
        intent_results: list[IntentResult],
        method: str,
        tokens_used: int = 0
    ):
        """
        ä¿å­˜ Intent Router çš„è¾“å‡ºåˆ° JSON æ–‡ä»¶
        
        Args:
            user_message: ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
            intent_results: Intent Router è¯†åˆ«çš„ç»“æœåˆ—è¡¨
            method: è¯†åˆ«æ–¹æ³• ("rule_engine" æˆ– "llm_fallback")
            tokens_used: æ¶ˆè€—çš„ token æ•°é‡
        """
        if not self.save_output:
            return
        
        try:
            # æ„å»ºè¾“å‡ºæ•°æ®
            output_data = {
                "timestamp": datetime.now().isoformat(),
                "user_message": user_message,
                "method": method,
                "tokens_used": tokens_used,
                "results": []
            }
            
            # æ·»åŠ æ¯ä¸ª intent result
            for result in intent_results:
                result_dict = {
                    "intent": result.intent,
                    "topic": result.topic,
                    "confidence": result.confidence,
                    "parameters": result.parameters
                }
                output_data["results"].append(result_dict)
            
            # è¯»å–ç°æœ‰å†å²ï¼ˆä¿ç•™æœ€è¿‘10æ¡ï¼‰
            history = []
            if self.INTENT_OUTPUT_PATH.exists():
                try:
                    with open(self.INTENT_OUTPUT_PATH, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                        history = existing_data.get("history", [])
                except Exception as e:
                    logger.warning(f"âš ï¸  Failed to load existing intent output history: {e}")
            
            # æ·»åŠ å½“å‰ç»“æœåˆ°å†å²
            history.append(output_data)
            
            # åªä¿ç•™æœ€è¿‘10æ¡è®°å½•
            history = history[-10:]
            
            # æ„å»ºå®Œæ•´æ•°æ®ç»“æ„
            full_data = {
                "description": "Intent Router å®æ—¶è¾“å‡ºè®°å½• (Phase 3 æ¶æ„)",
                "latest": output_data,
                "history": history,
                "stats": {
                    "total_requests": self.stats["total_requests"],
                    "rule_based_success": self.stats["rule_based_success"],
                    "llm_fallback": self.stats["llm_fallback"],
                    "rule_success_rate": f"{self.stats['rule_based_success']/self.stats['total_requests']*100:.1f}%" if self.stats['total_requests'] > 0 else "0%"
                }
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(self.INTENT_OUTPUT_PATH, 'w', encoding='utf-8') as f:
                json.dump(full_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ Intent output saved to {self.INTENT_OUTPUT_PATH}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save intent output: {e}")
    
    def _format_prompt(
        self,
        message: str,
        memory_summary: Optional[str] = None,
        last_artifact_summary: Optional[str] = None
    ) -> str:
        """
        æ ¼å¼åŒ– prompt (Phase 3: Minimal Context)
        
        Phase 3 ä¼˜åŒ–ï¼šåªä¼ é€’æœ€å°åŒ–ä¸Šä¸‹æ–‡æ ‡è®°ï¼Œä¸ä¼ é€’å®Œæ•´å†…å®¹
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            memory_summary: è®°å¿†æ‘˜è¦ï¼ˆå­—ç¬¦ä¸²ï¼‰- ä»ä¸­æå– top preference
            last_artifact_summary: ä¸Šä¸€è½® artifact æ‘˜è¦ - åªæ£€æŸ¥æ˜¯å¦å­˜åœ¨
        
        Returns:
            str: æ ¼å¼åŒ–åçš„ prompt
        """
        # Phase 3: æå–æœ€å°åŒ–ä¸Šä¸‹æ–‡æ ‡è®°
        
        # 1. æå–ç”¨æˆ·åå¥½ï¼ˆtop preference only, ~2 tokensï¼‰
        user_preference_top = "null"
        if memory_summary and "prefers" in memory_summary:
            # ä» memory_summary ä¸­æå–åå¥½
            # ç¤ºä¾‹: "[User Preference: prefers flashcards (75%)]"
            try:
                if "flashcard" in memory_summary.lower():
                    user_preference_top = "flashcard"
                elif "quiz" in memory_summary.lower():
                    user_preference_top = "quiz"
                elif "explain" in memory_summary.lower():
                    user_preference_top = "explain"
                elif "mindmap" in memory_summary.lower():
                    user_preference_top = "mindmap"
                elif "notes" in memory_summary.lower():
                    user_preference_top = "notes"
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to extract preference: {e}")
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¸€è½®å†…å®¹ï¼ˆ~1 tokenï¼‰
        has_last_artifact = "false"
        if last_artifact_summary and last_artifact_summary != "No previous interaction.":
            has_last_artifact = "true"
        
        # 3. æ ¼å¼åŒ– promptï¼ˆä½¿ç”¨æœ€å°åŒ–æ ‡è®°ï¼‰
        formatted = self.prompt_template.format(
            message=message,
            user_preference_top=user_preference_top,
            has_last_artifact=has_last_artifact
        )
        
        logger.debug(f"ğŸ“ Context flags: preference={user_preference_top}, has_artifact={has_last_artifact}")
        
        return formatted
    
    def _topic_needs_llm_extraction(
        self,
        extracted_topic: Optional[str],
        current_topic: Optional[str],
        message: str
    ) -> bool:
        """
        æ£€æŸ¥æå–çš„ topic æ˜¯å¦éœ€è¦ LLM è¾…åŠ©æå–
        
        è¿”å› True çš„æƒ…å†µï¼š
        1. topic ä¸ºç©ºä¸” current_topic ä¹Ÿä¸ºç©º
        2. topic åŒ…å«åƒåœ¾å­—ç¬¦ï¼ˆå¦‚é¡¿å·ã€é€—å·åˆ†éš”çš„åˆ—è¡¨ï¼‰
        3. topic å¤ªçŸ­ï¼ˆ< 2 å­—ç¬¦ï¼‰
        4. topic åŒ…å«æŠ€èƒ½å…³é”®è¯ï¼ˆè¯´æ˜æå–é”™è¯¯ï¼‰
        """
        # å¦‚æœæœ‰ current_topicï¼Œä¸éœ€è¦ LLMï¼ˆç›´æ¥ fallback åˆ° current_topicï¼‰
        if current_topic:
            return False
        
        # å¦‚æœ topic ä¸ºç©ºï¼Œéœ€è¦ LLM
        if not extracted_topic:
            return True
        
        # æ£€æŸ¥åƒåœ¾å­—ç¬¦
        garbage_indicators = ['ã€', 'ï¼Œ', ',', '  ', 'ã€‚', '.']
        if any(g in extracted_topic for g in garbage_indicators):
            return True
        
        # æ£€æŸ¥ topic å¤ªçŸ­
        if len(extracted_topic.strip()) < 2:
            return True
        
        # æ£€æŸ¥ topic åŒ…å«æŠ€èƒ½å…³é”®è¯
        skill_keywords = ['é—ªå¡', 'æµ‹éªŒ', 'ç¬”è®°', 'é¢˜', 'å¯¼å›¾', 'è§£é‡Š', 'è®²è§£', 'å­¦ä¹ åŒ…', 'æä¾›']
        if any(kw in extracted_topic for kw in skill_keywords):
            return True
        
        return False
    
    async def _llm_extract_topic(
        self,
        message: str,
        current_topic: Optional[str],
        session_topics: Optional[list]
    ) -> Optional[str]:
        """
        ä½¿ç”¨ Gemini LLM è¾…åŠ©æå– topic
        
        è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§è°ƒç”¨ï¼Œåªç”¨äºæå– topic
        """
        try:
            from ..services.gemini import GeminiClient
            import json
            
            gemini = GeminiClient()
            
            # æ„å»ºç®€æ´çš„ prompt
            context_hint = ""
            if current_topic:
                context_hint = f"å½“å‰å¯¹è¯ä¸»é¢˜æ˜¯ï¼š{current_topic}\n"
            if session_topics:
                context_hint += f"å†å²ä¸»é¢˜ï¼š{', '.join(session_topics[:3])}\n"
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ª topic æå–åŠ©æ‰‹ã€‚ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–å­¦ä¹ ä¸»é¢˜ã€‚

{context_hint}
ç”¨æˆ·æ¶ˆæ¯ï¼š{message}

è§„åˆ™ï¼š
1. æå–ç”¨æˆ·æƒ³è¦å­¦ä¹ çš„**æ ¸å¿ƒä¸»é¢˜**ï¼ˆå¦‚"å¥½è±åå†å²"ã€"ç‰›é¡¿ç¬¬äºŒå®šå¾‹"ï¼‰
2. ä¸è¦åŒ…å«åŠ¨ä½œè¯ï¼ˆå¦‚"ç»™æˆ‘"ã€"å¸®æˆ‘"ã€"ç”Ÿæˆ"ï¼‰
3. ä¸è¦åŒ…å«æŠ€èƒ½è¯ï¼ˆå¦‚"é—ªå¡"ã€"æµ‹éªŒ"ã€"ç¬”è®°"ã€"å­¦ä¹ åŒ…"ï¼‰
4. å¦‚æœæ¶ˆæ¯ä¸­æ²¡æœ‰æ˜ç¡®ä¸»é¢˜ï¼Œä½†æœ‰å†å²ä¸»é¢˜ä¸Šä¸‹æ–‡ï¼Œè¿”å›æœ€ç›¸å…³çš„å†å²ä¸»é¢˜
5. å¦‚æœå®Œå…¨æ— æ³•ç¡®å®šä¸»é¢˜ï¼Œè¿”å› null

ä»…è¿”å› JSONï¼š{{"topic": "æå–çš„ä¸»é¢˜" æˆ– null}}"""

            response = await gemini.generate(
                prompt=prompt,
                model="gemini-2.5-flash",
                response_format="json",
                max_tokens=100,
                temperature=0.3
            )
            
            if response and "content" in response:
                content = response["content"]
                if isinstance(content, str):
                    result = json.loads(content)
                elif isinstance(content, dict):
                    result = content
                else:
                    return None
                
                topic = result.get("topic")
                if topic and isinstance(topic, str) and len(topic) >= 2:
                    logger.info(f"ğŸ¤– LLM topic extraction: '{topic}'")
                    return topic
            
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸  LLM topic extraction failed: {e}")
            return None
    
    async def parse(
        self,
        message: str,
        memory_summary: Optional[str] = None,
        last_artifact_summary: Optional[str] = None,
        current_topic: Optional[str] = None,
        session_topics: Optional[list] = None,
        has_files: bool = False
    ) -> list[IntentResult]:
        """
        è§£æç”¨æˆ·æ¶ˆæ¯ï¼Œè¯†åˆ«æ„å›¾ (Phase 4)
        
        Phase 4 æµç¨‹ï¼š
        1. ä½¿ç”¨ Skill Registry è¿›è¡Œ0-tokenå…³é”®è¯åŒ¹é…
        2. å¦‚æœæ— æ³•åŒ¹é…ï¼Œè¿”å› 'other' intentï¼ˆä¸ä½¿ç”¨LLMï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            memory_summary: å¯é€‰çš„è®°å¿†æ‘˜è¦ï¼Œç”¨äºå¢å¼ºè¯†åˆ«å‡†ç¡®åº¦
            last_artifact_summary: ä¸Šä¸€è½® artifact æ‘˜è¦ï¼ˆç”¨äºä¸Šä¸‹æ–‡å¼•ç”¨ï¼‰
            current_topic: å½“å‰å¯¹è¯ä¸»é¢˜ï¼ˆä» session_contextï¼‰
            session_topics: å†å²topicsåˆ—è¡¨ï¼ˆä» session_contextï¼‰
            has_files: æ˜¯å¦æœ‰æ–‡ä»¶é™„ä»¶
        
        Returns:
            list[IntentResult]: æ„å›¾è¯†åˆ«ç»“æœåˆ—è¡¨
        
        Raises:
            Exception: å¦‚æœ API è°ƒç”¨å¤±è´¥
        """
        logger.info(f"ğŸ” Parsing intent for message: {message[:50]}...")
        if current_topic:
            logger.info(f"ğŸ“š Current topic from context: {current_topic}")
        if session_topics:
            logger.info(f"ğŸ“š Session topics: {session_topics}")
        if has_files:
            logger.info(f"ğŸ“ Has file attachments")
        
        # ç»Ÿè®¡
        self.stats["total_requests"] += 1
        
        # ============= ğŸš€ Phase 4: ä¼˜å…ˆä½¿ç”¨ Skill Registry (0 tokens) =============
        skill_match = self.skill_registry.match_message(message, current_topic, session_topics, has_files)
        
        # ğŸ”¥ å¤„ç† clarification needed æƒ…å†µ
        if skill_match and skill_match.skill_id == "clarification_needed":
            logger.warning(f"âš ï¸  Clarification needed: {skill_match.parameters.get('clarification_reason')}")
            
            # è¿”å› clarification intent
            clarification_result = IntentResult(
                intent="clarification_needed",
                topic=None,
                target_artifact=None,
                confidence=1.0,
                raw_text=message,
                parameters=skill_match.parameters
            )
            
            return [clarification_result]
        
        if skill_match and skill_match.confidence >= 0.7:
            # Skill Registry æˆåŠŸåŒ¹é…ï¼
            logger.info(
                f"âœ… Skill Registry Match: {skill_match.skill_id} "
                f"(confidence: {skill_match.confidence:.2f}) | "
                f"Keywords: {skill_match.matched_keywords}"
            )
            
            # å°† skill_id è½¬æ¢ä¸º intentï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            intent_mapping = {
                "quiz_skill": "quiz_request",
                "explain_skill": "explain_request",
                "flashcard_skill": "flashcard_request",
                "notes_skill": "notes",
                "mindmap_skill": "mindmap_request",
                "learning_plan_skill": "learning_bundle"
            }
            
            intent = intent_mapping.get(skill_match.skill_id, skill_match.skill_id)
            
            # ğŸ†• æ£€æµ‹å¼•ç”¨ï¼ˆ0 tokenï¼Œæœ¬åœ°æ­£åˆ™ï¼‰
            reference_resolver = get_reference_resolver()
            has_reference = reference_resolver.has_references(message)
            if has_reference:
                logger.info(f"ğŸ”— Reference detected in message, will resolve in orchestrator")
            
            # ğŸ†• è·å– topic å¹¶æ£€æŸ¥æ˜¯å¦éœ€è¦ LLM è¾…åŠ©æå–
            extracted_topic = skill_match.parameters.get('topic')
            
            # ğŸ”¥ å¯¹äº 'other' intentï¼ˆé—²èŠ/é—®å€™ï¼‰ï¼Œè·³è¿‡ topic æå–ï¼ˆèŠ‚çœ tokenï¼‰
            if intent == "other":
                logger.info(f"ğŸ’¬ Skipping topic extraction for 'other' intent (conversation)")
                extracted_topic = None  # é—²èŠä¸éœ€è¦ topic
            else:
                # æ£€æŸ¥ topic æ˜¯å¦æœ‰æ•ˆï¼ˆå¦‚æœæ— æ•ˆï¼Œéœ€è¦ LLM è¾…åŠ©æå–ï¼‰
                topic_needs_llm = self._topic_needs_llm_extraction(extracted_topic, current_topic, message)
                
                if topic_needs_llm:
                    logger.info(f"âš ï¸  Topic extraction uncertain: '{extracted_topic}', using LLM assist")
                    # ğŸ”¥ è°ƒç”¨ Gemini LLM è¾…åŠ©æå– topic
                    llm_topic = await self._llm_extract_topic(message, current_topic, session_topics)
                    if llm_topic:
                        extracted_topic = llm_topic
                        skill_match.parameters['topic'] = llm_topic
                        logger.info(f"âœ… LLM extracted topic: '{llm_topic}'")
                    elif current_topic:
                        # LLM ä¹Ÿæ— æ³•æå–ï¼Œä½¿ç”¨ current_topic
                        extracted_topic = current_topic
                        skill_match.parameters['topic'] = current_topic
                        logger.info(f"ğŸ“š Fallback to current_topic: '{current_topic}'")
            
            # æ„å»º IntentResult
            intent_result = IntentResult(
                intent=intent,
                topic=extracted_topic,
                target_artifact=None,
                confidence=skill_match.confidence,
                raw_text=message,
                parameters=skill_match.parameters,
                has_reference=has_reference  # ğŸ†• æ ‡è®°æ˜¯å¦åŒ…å«å¼•ç”¨
            )
            
            logger.info(
                f"ğŸ“Š Token Usage (Skill Registry) | Input: 0 | Output: 0 | Total: 0 | "
                f"Time: <0.001s | Method: Skill Registry (Phase 4)"
            )
            logger.info(f"ğŸ’° Tokens Saved: ~3,000 | 100% savings")
            
            # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º
            self._save_intent_output(
                user_message=message,
                intent_results=[intent_result],
                method="skill_registry",
                tokens_used=0
            )
            
            return [intent_result]
        
        # ============= ğŸ†• LLM Fallback: ä½¿ç”¨ä¾¿å®œçš„ Gemini 2.0 Flash Exp =============
        # Skill Registry æœªåŒ¹é…æ—¶ï¼Œä½¿ç”¨ Geminiï¼ˆä¸æ˜¯ç›´æ¥è¿”å› otherï¼‰
        # åœºæ™¯ï¼šç½•è§éœ€æ±‚ã€å¤æ‚è¡¨è¿°ã€æœªæ³¨å†Œçš„æŠ€èƒ½
        
        if skill_match:
            logger.info(
                f"âš ï¸  Skill Registry low confidence: {skill_match.skill_id} "
                f"({skill_match.confidence:.2f} < 0.7), trying LLM fallback"
            )
        else:
            logger.info("âš ï¸  No Skill Registry match, trying LLM fallback (Gemini 2.0 Flash Exp)")
        
        # å°è¯•ä½¿ç”¨ Gemini LLM
        try:
            from ..services.gemini import GeminiClient
            
            gemini = GeminiClient()
            
            # ğŸ†• ä½¿ç”¨æ›´ç®€æ´çš„ promptï¼ˆæé«˜æˆåŠŸç‡ï¼‰
            simple_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ„å›¾åˆ†ç±»å™¨ã€‚åˆ†æç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å› JSONã€‚

ç”¨æˆ·æ¶ˆæ¯ï¼š{message}

è¯·åˆ†æç”¨æˆ·æƒ³è¦åšä»€ä¹ˆï¼Œè¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼š
{{
  "intent": "quiz|flashcard|explain|notes|mindmap|learning_bundle|other",
  "topic": "æå–çš„ä¸»é¢˜æˆ–null",
  "confidence": 0.85
}}

æ„å›¾è¯´æ˜ï¼š
- quiz: ç”¨æˆ·æƒ³è¦ç»ƒä¹ é¢˜/æµ‹éªŒ/åšé¢˜
- flashcard: ç”¨æˆ·æƒ³è¦é—ªå¡/è®°å¿†å¡
- explain: ç”¨æˆ·æƒ³è¦è®²è§£/è§£é‡Š/äº†è§£æŸä¸ªæ¦‚å¿µ
- notes: ç”¨æˆ·æƒ³è¦ç¬”è®°/æ€»ç»“
- mindmap: ç”¨æˆ·æƒ³è¦æ€ç»´å¯¼å›¾
- learning_bundle: ç”¨æˆ·æƒ³è¦å­¦ä¹ è®¡åˆ’/å­¦ä¹ åŒ…
- other: å…¶ä»–å¯¹è¯/é—²èŠ/é—®å€™

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""
            
            # è°ƒç”¨ Geminiï¼ˆä¾¿å®œä¸”å¿«é€Ÿï¼‰
            import time
            start_time = time.time()
            
            response = await gemini.generate(
                prompt=simple_prompt,
                model="gemini-2.5-flash",  # ä¾¿å®œçš„æ¨¡å‹
                response_format="json",
                temperature=0.3,  # ğŸ†• é™ä½æ¸©åº¦ä»¥æé«˜ä¸€è‡´æ€§
                max_tokens=200,  # ğŸ†• é™åˆ¶è¾“å‡ºé•¿åº¦
                thinking_budget=0,  # ğŸ”§ ç¦ç”¨æ€è€ƒä»¥ç¡®ä¿å®Œæ•´è¾“å‡º
                return_thinking=False
            )
            
            elapsed_time = time.time() - start_time
            
            # è§£æ LLM å“åº”
            if response and "content" in response:
                content = response["content"]
                
                # ğŸ†• å¢å¼ºçš„è§£æé€»è¾‘
                llm_result = None
                
                # å¤„ç† content å¯èƒ½æ˜¯ str æˆ– dict
                if isinstance(content, str):
                    import json
                    # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—
                    content_clean = content.strip()
                    if content_clean.startswith("```"):
                        content_clean = content_clean.split("```")[1]
                        if content_clean.startswith("json"):
                            content_clean = content_clean[4:]
                        content_clean = content_clean.strip()
                    
                    # å°è¯•è§£æ JSON
                    try:
                        llm_result = json.loads(content_clean)
                    except json.JSONDecodeError:
                        # ğŸ†• å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå– JSON éƒ¨åˆ†
                        import re
                        json_match = re.search(r'\{[^{}]*\}', content_clean)
                        if json_match:
                            llm_result = json.loads(json_match.group())
                        else:
                            raise ValueError(f"Cannot parse JSON from: {content_clean[:100]}")
                elif isinstance(content, dict):
                    llm_result = content
                else:
                    raise ValueError(f"Unexpected content type: {type(content)}")
                
                # ğŸ†• éªŒè¯å¿…è¦å­—æ®µ
                if not llm_result or "intent" not in llm_result:
                    raise ValueError(f"Missing 'intent' field in response: {llm_result}")
                
                # æå– intent ä¿¡æ¯
                intent = llm_result.get("intent", "other")
                topic = llm_result.get("topic", current_topic)
                confidence = llm_result.get("confidence", 0.7)
                parameters = llm_result.get("parameters", {})
                
                intent_result = IntentResult(
                    intent=intent,
                    topic=topic,
                    target_artifact=None,
                    confidence=confidence,
                    raw_text=message,
                    parameters=parameters
                )
                
                # ç»Ÿè®¡
                self.stats["llm_fallback"] += 1
                
                # ä¼°ç®— token ä½¿ç”¨ï¼ˆGemini Flash æ›´ä¾¿å®œï¼‰
                estimated_tokens = len(simple_prompt) // 4 + len(str(content)) // 4  # ç²—ç•¥ä¼°ç®—
                
                logger.info(
                    f"âœ… LLM Fallback Success (Gemini): intent={intent}, topic={topic}, confidence={confidence:.2f}"
                )
                logger.info(
                    f"ğŸ“Š Token Usage (LLM Fallback) | Estimated: ~{estimated_tokens} | "
                    f"Time: {elapsed_time:.2f}s | Cost: ~1/10 of Kimi"
                )
                
                # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º
                self._save_intent_output(
                    user_message=message,
                    intent_results=[intent_result],
                    method="llm_fallback_gemini",
                    tokens_used=estimated_tokens
                )
                
                return [intent_result]
            
        except Exception as e:
            logger.warning(f"âš ï¸  LLM fallback failed: {e}, returning 'other' intent")
        
        # ============= æœ€ç»ˆ Fallback: è¿”å› other =============
        # å¦‚æœ Gemini ä¹Ÿå¤±è´¥äº†ï¼Œè¿”å› other
        
        logger.info("âš ï¸  All methods failed, returning 'other' intent as final fallback")
        
        other_result = IntentResult(
            intent="other",
            topic=current_topic,  # ä¿æŒå½“å‰topicï¼Œä¾¿äºä¸Šä¸‹æ–‡å¯¹è¯
            target_artifact=None,
            confidence=0.5,
            raw_text=message,
            parameters={}
        )
        
        # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º
        self._save_intent_output(
            user_message=message,
            intent_results=[other_result],
            method="final_fallback",
            tokens_used=0
        )
        
        return [other_result]
        
        # ============= ğŸš€ DEPRECATED: è§„åˆ™å¼•æ“ (Phase 3, å·²åºŸå¼ƒ) =============
        # Phase 4 ä¸å†ä½¿ç”¨è§„åˆ™å¼•æ“ï¼ŒSkill Registry å·²å®Œå…¨æ›¿ä»£
        if False and self.use_rule_engine and self.rule_classifier:
            rule_result = self.rule_classifier.classify(message, memory_summary)
            
            if rule_result:
                # è§„åˆ™å¼•æ“æˆåŠŸè¯†åˆ«ï¼
                self.stats["rule_based_success"] += 1
                
                # è®¡ç®—èŠ‚çœçš„ tokensï¼ˆä¼°ç®—ï¼‰
                estimated_saved_tokens = 3000  # Intent Router å¹³å‡æ¶ˆè€—
                
                logger.info(
                    f"ğŸ“Š Token Usage (Rule-Based) | Input: 0 | Output: 0 | Total: 0 | "
                    f"Time: <0.01s | Method: Rule Engine"
                )
                logger.info(
                    f"ğŸ’° Tokens Saved: ~{estimated_saved_tokens:,} | "
                    f"Success rate: {self.stats['rule_based_success']}/{self.stats['total_requests']} "
                    f"({self.stats['rule_based_success']/self.stats['total_requests']*100:.1f}%)"
                )
                
                # è½¬æ¢ä¸º IntentResult å¯¹è±¡
                intent_result = IntentResult(
                    intent=rule_result["intent"],
                    topic=rule_result["topic"],
                    target_artifact=rule_result["target_artifact"],
                    confidence=rule_result["confidence"],
                    raw_text=rule_result["raw_text"],
                    parameters=rule_result.get("parameters", {})
                )
                
                logger.info(f"âœ… Intent parsed: {intent_result.intent} (confidence: {intent_result.confidence:.2f}, topic: {intent_result.topic})")
                
                # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º
                self._save_intent_output(
                    user_message=message,
                    intent_results=[intent_result],
                    method="rule_engine",
                    tokens_used=0
                )
                
                return [intent_result]
        
        # ============= è§„åˆ™å¼•æ“å¤±è´¥ï¼Œå›é€€åˆ° LLM =============
        self.stats["llm_fallback"] += 1
        logger.info(
            f"âš ï¸  Rule-based classification FAILED, falling back to LLM | "
            f"Fallback rate: {self.stats['llm_fallback']}/{self.stats['total_requests']} "
            f"({self.stats['llm_fallback']/self.stats['total_requests']*100:.1f}%)"
        )
        
        # æ ¼å¼åŒ– prompt
        prompt = self._format_prompt(message, memory_summary, last_artifact_summary)
        
        try:
            # è°ƒç”¨ LLM APIï¼ˆğŸ”§ å…¨éƒ¨ä½¿ç”¨ Geminiï¼‰
            response = await self.gemini_client.generate(
                prompt=prompt,
                model="gemini-2.5-flash",  # ğŸ”§ ç»Ÿä¸€ä½¿ç”¨ Gemini 2.5 Flash
                response_format="json",
                max_tokens=200,  # Intent recognition needs short output
                temperature=0.3,   # Lower temperature for more consistent classification
                thinking_budget=0,  # ğŸ”¥ Intent routing ä¸éœ€è¦ thinkingï¼ˆèŠ‚çœ tokensï¼‰
                return_thinking=False
            )
            
            # ğŸ”¥ å…¼å®¹æ–°ç‰ˆ generate è¿”å›æ ¼å¼ï¼šDict["content", "thinking", "usage"]
            # å¯¹äº Intent Routerï¼ŒJSON å¯èƒ½åœ¨ thinking å­—æ®µä¸­ï¼ˆthinking æ¨¡å‹ç‰¹æ€§ï¼‰
            if isinstance(response, dict):
                thinking_text = response.get("thinking", "")
                content_text = response.get("content", "")
                
                logger.info(f"ğŸ“Š thinking: {len(thinking_text)} chars, preview: {thinking_text[:200]}")
                logger.info(f"ğŸ“Š content: {len(content_text)} chars, preview: {content_text[:200]}")
                
                # åˆ¤æ–­å“ªä¸ªå­—æ®µæ›´å¯èƒ½åŒ…å«JSONï¼ˆé€šè¿‡ç®€å•å¯å‘å¼ï¼‰
                thinking_has_json = thinking_text and (thinking_text.strip().startswith('{') or '{"intent"' in thinking_text)
                content_has_json = content_text and (content_text.strip().startswith('{') or '{"intent"' in content_text)
                
                if thinking_has_json:
                    logger.info(f"âš¡ Using thinking field (detected JSON)")
                    response_text = thinking_text
                elif content_has_json:
                    logger.info(f"âš¡ Using content field (detected JSON)")
                    response_text = content_text
                else:
                    # ä¸¤ä¸ªéƒ½ä¸åƒJSONï¼Œå°è¯•thinkingä¼˜å…ˆï¼ˆthinkingæ¨¡å‹ç‰¹æ€§ï¼‰
                    logger.warning(f"âš ï¸ Neither field looks like JSON, trying thinking first")
                    response_text = thinking_text if thinking_text else content_text
            else:
                response_text = response
            
            # ğŸ› DEBUG: Log the raw LLM response
            response_preview = response_text[:500] if isinstance(response_text, str) else str(response_text)[:500]
            logger.info(f"ğŸ” LLM Response preview: {response_preview}")
            
            # è§£æ JSON å“åº”
            response_data = json.loads(response_text)
            logger.debug(f"ğŸ” LLM Response (parsed): {response_data}")
            
            # æ„å›¾æ˜ å°„ï¼šç»Ÿä¸€åŒ–ä¸åŒçš„è¡¨è¾¾
            intent_mapping = {
                "quiz": "quiz_request",
                "explain": "explain_request",
                "flashcard": "flashcard_request",
                "learning_bundle": "learning_bundle",  # ä¿æŒåŸæ ·ï¼Œä¸skillé…ç½®ä¸€è‡´
                "mindmap": "mindmap_request",  # æ€ç»´å¯¼å›¾
                "other": "other"
            }
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆè¯·æ±‚ï¼ˆæœ‰ "intents" æ•°ç»„ï¼‰
            if "intents" in response_data:
                logger.info(f"ğŸ”€ Detected MIXED REQUEST with {len(response_data['intents'])} intents")
                results = []
                
                for idx, intent_data in enumerate(response_data["intents"]):
                    intent = intent_data.get("intent", "other")
                    topic = intent_data.get("topic")
                    target_artifact = intent_data.get("target_artifact")
                    confidence = float(intent_data.get("confidence", 0.5))
                    parameters = intent_data.get("parameters", {})
                    
                    # æ ‡å‡†åŒ– intent
                    normalized_intent = intent_mapping.get(intent, intent)
                    
                    # å¦‚æœæå–äº† quantity å‚æ•°ï¼Œè®°å½•æ—¥å¿—
                    if parameters.get("quantity"):
                        logger.info(f"ğŸ“Š Intent {idx+1}: Extracted quantity parameter: {parameters['quantity']}")
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    result = IntentResult(
                        intent=normalized_intent,
                        topic=topic,
                        target_artifact=target_artifact,
                        confidence=confidence,
                        raw_text=message,
                        parameters=parameters
                    )
                    
                    results.append(result)
                    logger.info(f"âœ… Intent {idx+1} parsed: {result.intent} (confidence: {result.confidence:.2f}, topic: {result.topic})")
                
                # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º (æ··åˆè¯·æ±‚)
                self._save_intent_output(
                    user_message=message,
                    intent_results=results,
                    method="llm_fallback",
                    tokens_used=1487  # ä¼°ç®—å€¼ï¼ŒPhase 3 ä¼˜åŒ–åçš„ LLM Fallback token æ¶ˆè€—
                )
                
                return results
            else:
                # å•ä¸ªè¯·æ±‚
                intent = response_data.get("intent", "other")
                topic = response_data.get("topic")
                target_artifact = response_data.get("target_artifact")
                confidence = float(response_data.get("confidence", 0.5))
                parameters = response_data.get("parameters", {})
                
                # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼é€»è¾‘
                if confidence < self.CONFIDENCE_THRESHOLD:
                    logger.warning(
                        f"âš ï¸ Low confidence ({confidence:.2f} < {self.CONFIDENCE_THRESHOLD}), "
                        f"falling back to 'other'"
                    )
                    intent = "other"
                    target_artifact = None
                
                # æ ‡å‡†åŒ– intent
                normalized_intent = intent_mapping.get(intent, intent)
                
                # å¦‚æœæå–äº† quantity å‚æ•°ï¼Œè®°å½•æ—¥å¿—
                if parameters.get("quantity"):
                    logger.info(f"ğŸ“Š Extracted quantity parameter: {parameters['quantity']}")
                
                # ğŸ› DEBUG: Log all extracted parameters
                logger.debug(f"ğŸ“Š All extracted parameters: {parameters}")
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                result = IntentResult(
                    intent=normalized_intent,
                    topic=topic,
                    target_artifact=target_artifact,
                    confidence=confidence,
                    raw_text=message,
                    parameters=parameters
                )
                
                logger.info(f"âœ… Intent parsed: {result.intent} (confidence: {result.confidence:.2f}, topic: {result.topic})")
                
                # ğŸ’¾ ä¿å­˜ Intent Router è¾“å‡º (å•ä¸ªè¯·æ±‚)
                self._save_intent_output(
                    user_message=message,
                    intent_results=[result],
                    method="llm_fallback",
                    tokens_used=1487  # ä¼°ç®—å€¼ï¼ŒPhase 3 ä¼˜åŒ–åçš„ LLM Fallback token æ¶ˆè€—
                )
                
                return [result]  # è¿”å›å•å…ƒç´ åˆ—è¡¨ï¼Œä¿æŒæ¥å£ä¸€è‡´
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse JSON response: {e}")
            # è¿”å›é»˜è®¤çš„ "other" æ„å›¾ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰
            return [IntentResult(
                intent="other",
                topic=None,
                target_artifact=None,
                confidence=0.0,
                raw_text=message
            )]
        
        except Exception as e:
            logger.error(f"âŒ Intent parsing failed: {e}")
            raise
    
    async def parse_batch(
        self,
        messages: list[str],
        memory_summary: Optional[MemorySummary] = None
    ) -> list[IntentResult]:
        """
        æ‰¹é‡è§£æå¤šä¸ªæ¶ˆæ¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            memory_summary: è®°å¿†æ‘˜è¦
        
        Returns:
            list[IntentResult]: æ„å›¾è¯†åˆ«ç»“æœåˆ—è¡¨
        """
        results = []
        for message in messages:
            result = await self.parse(message, memory_summary)
            results.append(result)
        
        return results
    
    def get_optimization_stats(self) -> dict:
        """
        è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = self.stats["total_requests"]
        rule_success = self.stats["rule_based_success"]
        llm_fallback = self.stats["llm_fallback"]
        
        if total == 0:
            return {
                "total_requests": 0,
                "rule_based_success": 0,
                "llm_fallback": 0,
                "rule_success_rate": 0.0,
                "estimated_tokens_saved": 0,
                "average_tokens_per_request": 0
            }
        
        # ä¼°ç®— token èŠ‚çœ
        # è§„åˆ™å¼•æ“: 0 tokens
        # LLM: ~3,000 tokens
        estimated_saved = rule_success * 3000
        average_per_request = llm_fallback * 3000 / total
        
        return {
            "total_requests": total,
            "rule_based_success": rule_success,
            "llm_fallback": llm_fallback,
            "rule_success_rate": rule_success / total * 100 if total > 0 else 0,
            "estimated_tokens_saved": estimated_saved,
            "average_tokens_per_request": int(average_per_request)
        }

