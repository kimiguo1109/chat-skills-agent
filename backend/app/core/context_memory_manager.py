"""
Context Engineering - Memory Manager
ä¸Šä¸‹æ–‡ç¼©å‡å’Œä¿®å‰ªç®¡ç†å™¨

æ ¸å¿ƒç†å¿µ:
1. Pruning: å®šæœŸæ¸…ç†å†å²ä¸­çš„å†—ä½™ tool calls å’Œè¾“å‡º
2. Condensation: é€’å½’æ‘˜è¦æ—©æœŸå¯¹è¯è½®æ¬¡
3. Adaptive Loading: åªåŠ è½½ artifact ç´¢å¼•ï¼Œä¸åŠ è½½å®Œæ•´å†…å®¹
"""

import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class ContextMemoryManager:
    """
    ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è®°å¿†ç®¡ç†å™¨
    
    èŒè´£:
    1. ç›‘æ§ context window ä½¿ç”¨ç‡
    2. ä¿®å‰ª(Prune)æ—§çš„ tool calls
    3. é€’å½’æ‘˜è¦(Condense)æ—©æœŸå¯¹è¯
    4. æ„å»ºè½»é‡çº§ contextï¼ˆåªåŠ è½½ç´¢å¼•ï¼‰
    """
    
    # Context ä½¿ç”¨ç‡é˜ˆå€¼
    CONDENSATION_THRESHOLD = 0.7  # 70%
    HARD_LIMIT_THRESHOLD = 0.9    # 90%
    
    # æœ€å¤šä¿ç•™å‡ è½®å®Œæ•´å¯¹è¯
    MAX_FULL_TURNS = 3
    
    # Token é™åˆ¶ï¼ˆKimi k2-thinking çº¦ 128Kï¼‰
    MAX_CONTEXT_TOKENS = 100000
    
    def __init__(
        self,
        artifact_manager: Any,
        llm_client: Optional[Any] = None
    ):
        """
        åˆå§‹åŒ– Memory Manager
        
        Args:
            artifact_manager: ContextArtifactManager å®ä¾‹
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆç”¨äºç”Ÿæˆæ‘˜è¦ï¼‰
        """
        self.artifact_manager = artifact_manager
        self.llm_client = llm_client
        
        logger.info("âœ… ContextMemoryManager initialized")
    
    def condense_history(
        self,
        messages: List[Dict[str, Any]],
        current_tokens: int
    ) -> List[Dict[str, Any]]:
        """
        å‹ç¼©å†å²æ¶ˆæ¯ï¼ˆPruning + Condensationï¼‰
        
        Args:
            messages: å½“å‰çš„æ¶ˆæ¯å†å²
            current_tokens: å½“å‰ context token æ•°
        
        Returns:
            å‹ç¼©åçš„æ¶ˆæ¯å†å²
        """
        utilization = current_tokens / self.MAX_CONTEXT_TOKENS
        
        if utilization < self.CONDENSATION_THRESHOLD:
            # ä½äºé˜ˆå€¼ï¼Œä¸éœ€è¦å‹ç¼©
            logger.info(f"ğŸ“Š Context utilization: {utilization:.1%} (< {self.CONDENSATION_THRESHOLD:.0%}) - No condensation needed")
            return messages
        
        logger.info(f"ğŸ”„ Context utilization: {utilization:.1%} - Starting condensation...")
        
        # 1. ä¿ç•™æœ€è¿‘ N è½®å®Œæ•´å¯¹è¯
        recent_messages = messages[-self.MAX_FULL_TURNS * 2:]  # æ¯è½®çº¦ 2 æ¡æ¶ˆæ¯ (user + assistant)
        
        # 2. å¤„ç†æ—©æœŸæ¶ˆæ¯
        early_messages = messages[:-self.MAX_FULL_TURNS * 2]
        
        if not early_messages:
            logger.info("No early messages to condense")
            return recent_messages
        
        # 3. ä¿®å‰ª tool callsï¼ˆç§»é™¤å†—é•¿çš„ tool outputsï¼‰
        pruned_messages = self._prune_tool_calls(early_messages)
        
        # 4. å¦‚æœä»ç„¶å¤ªå¤§ï¼Œè¿›è¡Œé€’å½’æ‘˜è¦
        if utilization > self.HARD_LIMIT_THRESHOLD:
            logger.info(f"âš ï¸  Hard limit reached ({utilization:.1%}), performing recursive summarization...")
            summary_message = self._create_recursive_summary(pruned_messages)
            condensed_messages = [summary_message] + recent_messages
        else:
            condensed_messages = pruned_messages + recent_messages
        
        logger.info(f"âœ… Condensed {len(messages)} â†’ {len(condensed_messages)} messages")
        return condensed_messages
    
    def _prune_tool_calls(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä¿®å‰ª tool callsï¼ˆä¿ç•™æ„å›¾ï¼Œç§»é™¤å†—é•¿è¾“å‡ºï¼‰
        
        ç­–ç•¥:
        - ä¿ç•™ user/assistant æ¶ˆæ¯
        - å¯¹äº tool_call: ä¿ç•™å‡½æ•°åå’Œå‚æ•°æ¦‚è¦
        - å¯¹äº tool_result: åªä¿ç•™çŠ¶æ€æ‘˜è¦ï¼Œç§»é™¤å®Œæ•´è¾“å‡º
        """
        pruned = []
        
        for msg in messages:
            role = msg.get("role")
            
            if role in ["user", "system"]:
                # ä¿ç•™ç”¨æˆ·å’Œç³»ç»Ÿæ¶ˆæ¯
                pruned.append(msg)
            
            elif role == "assistant":
                # ä¿ç•™ assistant æ¶ˆæ¯ï¼Œä½†ç®€åŒ– tool_calls
                if "tool_calls" in msg:
                    simplified_msg = msg.copy()
                    simplified_msg["tool_calls"] = [
                        {
                            "id": tc.get("id"),
                            "function": {
                                "name": tc.get("function", {}).get("name"),
                                "arguments": "(pruned)"  # ç§»é™¤è¯¦ç»†å‚æ•°
                            }
                        }
                        for tc in msg["tool_calls"]
                    ]
                    pruned.append(simplified_msg)
                else:
                    pruned.append(msg)
            
            elif role == "tool":
                # ç®€åŒ– tool ç»“æœ
                tool_call_id = msg.get("tool_call_id")
                content = msg.get("content", "")
                
                # ä¼°ç®—å¤§å°
                content_len = len(str(content))
                
                if content_len > 500:  # å¦‚æœè¾“å‡ºå¾ˆé•¿
                    simplified_content = f"[Tool output: {content_len} chars] Use read_artifact if needed."
                else:
                    simplified_content = content
                
                pruned.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": simplified_content
                })
        
        logger.info(f"âœ‚ï¸  Pruned tool calls: {len(messages)} â†’ {len(pruned)} messages")
        return pruned
    
    def _create_recursive_summary(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ›å»ºé€’å½’æ‘˜è¦ï¼ˆå°†æ—©æœŸå¯¹è¯å‹ç¼©ä¸ºä¸€æ¡ narrative summaryï¼‰
        
        Args:
            messages: è¦æ‘˜è¦çš„æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            æ‘˜è¦æ¶ˆæ¯ (role=system)
        """
        if not self.llm_client:
            # å¦‚æœæ²¡æœ‰ LLMï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™æ‘˜è¦
            return {
                "role": "system",
                "content": f"[Previous conversation summary: {len(messages)} messages]"
            }
        
        # æ„é€ æ‘˜è¦ prompt
        conversation_text = "\n\n".join([
            f"{msg.get('role')}: {msg.get('content', '')[:500]}"
            for msg in messages
            if msg.get('role') in ['user', 'assistant']
        ])
        
        summary_prompt = f"""Summarize the following conversation into a concise narrative (< 200 tokens):

{conversation_text}

Summary:"""
        
        try:
            response = self.llm_client.generate(
                prompt=summary_prompt,
                temperature=0.3,
                max_tokens=200
            )
            
            summary_content = response.get("content", "") if isinstance(response, dict) else str(response)
            
            logger.info(f"ğŸ“ Generated recursive summary: {len(summary_content)} chars")
            
            return {
                "role": "system",
                "content": f"[Conversation Summary]\n{summary_content}"
            }
        
        except Exception as e:
            logger.error(f"âŒ Failed to generate summary: {e}")
            return {
                "role": "system",
                "content": f"[Previous conversation: {len(messages)} messages]"
            }
    
    def build_lightweight_context(
        self,
        session_id: str,
        user_query: str
    ) -> Dict[str, Any]:
        """
        æ„å»ºè½»é‡çº§ contextï¼ˆåªåŠ è½½ artifact ç´¢å¼•ï¼Œä¸åŠ è½½å®Œæ•´å†…å®¹ï¼‰
        
        Args:
            session_id: ä¼šè¯ ID
            user_query: ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            è½»é‡çº§ context å­—å…¸
        """
        # è·å– artifact ç´¢å¼•
        artifact_index = self.artifact_manager.get_artifact_index(session_id=session_id)
        
        # æ„é€  context
        context = {
            "session_id": session_id,
            "artifacts_available": len(artifact_index),
            "artifact_index": artifact_index,
            "_note": """
You have access to the following artifacts (indexes only, content not loaded):
- To read full content: use read_artifact(artifact_id)
- To search: use search_artifacts(query)
- To list all: use list_artifacts()
            """.strip()
        }
        
        logger.info(f"ğŸ“¦ Built lightweight context: {len(artifact_index)} artifacts indexed")
        return context
    
    def estimate_tokens(self, messages: List[Dict[str, Any]]) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°
        
        ç®€åŒ–ä¼°ç®—: å­—ç¬¦æ•° * 0.8
        """
        total_chars = sum(
            len(str(msg.get("content", ""))) for msg in messages
        )
        return int(total_chars * 0.8)

