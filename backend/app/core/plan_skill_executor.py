"""
Plan Skill Executor - è®¡åˆ’æŠ€èƒ½æ‰§è¡Œå™¨

è´Ÿè´£æ‰§è¡Œ Plan Skill çš„ä¸²è”è°ƒç”¨é€»è¾‘ï¼š
1. è§£ææ‰§è¡Œè®¡åˆ’
2. ä¸²è”è°ƒç”¨å¤šä¸ª skills
3. ç®¡ç†ä¸Šä¸‹æ–‡ä¼ é€’
4. èšåˆæœ€ç»ˆç»“æœ
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import json

from app.core.artifact_storage import ArtifactStorage, generate_session_id

logger = logging.getLogger(__name__)


class PlanSkillExecutor:
    """
    Plan Skill æ‰§è¡Œå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - ä¸²è”æ‰§è¡Œå¤šä¸ª skills
    - ä¸Šä¸‹æ–‡æå–å’Œæ³¨å…¥
    - Token æˆæœ¬æ§åˆ¶
    - é”™è¯¯å¤„ç†å’Œå›é€€
    """
    
    def __init__(self, skill_orchestrator):
        """
        åˆå§‹åŒ– Plan Skill æ‰§è¡Œå™¨
        
        Args:
            skill_orchestrator: SkillOrchestrator å®ä¾‹ï¼ˆç”¨äºè°ƒç”¨å­ skillsï¼‰
        """
        self.skill_orchestrator = skill_orchestrator
        self.execution_log = []
        self.token_usage = {
            "total": 0,
            "per_step": {}
        }
        
        # ğŸ†• Phase 2: Context Offloading æ”¯æŒï¼ˆæ¡ä»¶åˆå§‹åŒ–ï¼‰
        self.artifact_storage = None
        self.offloading_enabled = False
        self.current_session_id = None  # å½“å‰æ‰§è¡Œçš„ session ID
    
    async def execute_plan(
        self,
        plan_config: Dict[str, Any],
        user_input: Dict[str, Any],
        user_profile: Any,
        session_context: Any
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„ Plan
        
        Args:
            plan_config: Plan Skill çš„ YAML é…ç½®
            user_input: ç”¨æˆ·è¾“å…¥å‚æ•°
            user_profile: ç”¨æˆ·å­¦ä¹ ç”»åƒ
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡
        
        Returns:
            èšåˆåçš„å­¦ä¹ åŒ…
        """
        # ğŸšï¸ Phase 2: æ£€æŸ¥æ˜¯å¦å¯ç”¨ Context Offloading
        cost_control = plan_config.get("cost_control", {})
        self.offloading_enabled = cost_control.get("enable_artifact_offloading", False)
        
        if self.offloading_enabled:
            # åˆå§‹åŒ– ArtifactStorage å’Œ session ID
            self.artifact_storage = ArtifactStorage()
            self.current_session_id = generate_session_id()
            
            # ä¿å­˜ Plan metadata
            self.artifact_storage.save_plan_metadata(
                self.current_session_id,
                plan_config,
                user_input
            )
            
            logger.info(f"âœ… [Offloading] Enabled (session: {self.current_session_id})")
        else:
            logger.debug("â„¹ï¸  [Offloading] Disabled (using legacy context pruning)")
        
        execution_plan = plan_config["execution_plan"]
        all_steps = execution_plan["steps"]
        
        # ğŸ†• Phase 4.2: åŠ¨æ€æ­¥éª¤é€‰æ‹© - å¦‚æœ user_input ä¸­æœ‰ required_stepsï¼Œåªæ‰§è¡Œè¿™äº›æ­¥éª¤
        required_steps = user_input.get("required_steps")
        if required_steps:
            logger.info(f"ğŸ¯ User requested specific steps: {required_steps}")
            # è¿‡æ»¤å‡ºéœ€è¦æ‰§è¡Œçš„æ­¥éª¤
            steps = [step for step in all_steps if step.get("step_id") in required_steps]
            logger.info(f"ğŸ“‹ Filtered execution plan: {len(steps)}/{len(all_steps)} steps")
        else:
            steps = all_steps
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œ Plan Skill: {plan_config['display_name']}")
        logger.info(f"ğŸ“‹ æ€»æ­¥éª¤æ•°: {len(steps)}")
        logger.info(f"ğŸ“ ä¸»é¢˜: {user_input.get('topic', 'Unknown')}")
        logger.info(f"{'='*60}\n")
        
        # æ‰§è¡Œç»“æœå­˜å‚¨
        step_results = {}
        step_contexts = {}  # å­˜å‚¨æ¯ä¸ª step æå–çš„ä¸Šä¸‹æ–‡
        
        # ä¸²è”æ‰§è¡Œæ‰€æœ‰ steps
        for actual_index, step in enumerate(steps, 1):  # ğŸ†• ä½¿ç”¨å®é™…ç´¢å¼•
            step_id = step["step_id"]
            step_name = step["display_name"]
            skill_id = step["skill_id"]
            
            logger.info(f"\n{'â”€'*60}")
            logger.info(f"ğŸ“ Step {actual_index}/{len(steps)}: {step_name}")  # ğŸ†• æ˜¾ç¤ºå®é™…è¿›åº¦
            logger.info(f"ğŸ”§ Skill: {skill_id}")
            logger.info(f"ğŸ“¦ ä¾èµ–: {step['depends_on'] or 'æ— '}")
            
            try:
                # 1. æ„å»º step è¾“å…¥
                step_input = self._build_step_input(
                    step=step,
                    user_input=user_input,
                    step_contexts=step_contexts,
                    session_context=session_context  # ğŸ†• ä¼ å…¥ session_context
                )
                logger.info(f"âœ… è¾“å…¥å‚æ•°æ„å»ºå®Œæˆ")
                
                # 2. æ‰§è¡Œ skill
                result = await self._execute_step(
                    skill_id=skill_id,
                    input_params=step_input,
                    user_profile=user_profile,
                    session_context=session_context
                )
                logger.info(f"âœ… Skill æ‰§è¡ŒæˆåŠŸ")
                
                # 3. æå–ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¸‹æ¸¸ stepsï¼‰
                extracted_context = self._extract_context(
                    result=result,
                    extraction_config=step.get("context_extraction", {}),
                    step_id=step_id  # ğŸ†• Phase 2: ä¼ é€’ step_id ç”¨äº offloading
                )
                
                # 4. å­˜å‚¨ç»“æœ
                step_results[step_id] = result
                step_contexts[step_id] = extracted_context
                
                # 5. Token ç»Ÿè®¡
                tokens_used = self._estimate_tokens(result)
                self.token_usage["per_step"][step_id] = tokens_used
                self.token_usage["total"] += tokens_used
                
                logger.info(f"ğŸ’¾ ä¸Šä¸‹æ–‡æå–: {len(str(extracted_context))} å­—ç¬¦")
                logger.info(f"ğŸ’° Token æ¶ˆè€—: ~{tokens_used}")
                logger.info(f"ğŸ“Š ç´¯è®¡ Token: ~{self.token_usage['total']}")
                logger.info(f"âœ… Step {step_id} å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ Step {step_id} å¤±è´¥: {e}")
                logger.exception(e)
                
                # é”™è¯¯å¤„ç†
                error_config = plan_config.get("error_handling", {})
                strategy = error_config.get("on_step_failure", {}).get("strategy", "skip_and_continue")
                
                if strategy == "skip_and_continue":
                    logger.info(f"â­ï¸  è·³è¿‡ Step {step_id}ï¼Œç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥")
                    continue
                elif strategy == "abort":
                    logger.error(f"ğŸš« Plan æ‰§è¡Œä¸­æ­¢")
                    raise
                else:
                    # é»˜è®¤ï¼šè·³è¿‡
                    logger.info(f"â­ï¸  è·³è¿‡ Step {step_id}ï¼Œç»§ç»­æ‰§è¡Œ")
                    continue
        
        logger.info(f"\n{'â”€'*60}")
        logger.info(f"ğŸ“¦ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ")
        logger.info(f"âœ… æˆåŠŸ: {len(step_results)}/{len(steps)} ä¸ªæ­¥éª¤")
        logger.info(f"ğŸ’° æ€» Token æ¶ˆè€—: ~{self.token_usage['total']}")
        
        # æ£€æŸ¥æœ€å°æˆåŠŸæ­¥éª¤æ•°
        min_required = plan_config.get("error_handling", {}).get("min_required_steps", 1)
        if len(step_results) < min_required:
            error_msg = plan_config.get("error_handling", {}).get("fallback", {}).get("on_total_failure", {}).get("message", "å­¦ä¹ åŒ…ç”Ÿæˆå¤±è´¥")
            logger.error(f"âŒ æˆåŠŸæ­¥éª¤ä¸è¶³: {len(step_results)} < {min_required}")
            raise Exception(error_msg)
        
        # èšåˆç»“æœ
        bundle = self._aggregate_results(
            step_results=step_results,
            aggregation_config=plan_config["aggregation"],
            user_input=user_input
        )
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸŠ Plan Skill æ‰§è¡Œå®Œæˆï¼")
        logger.info(f"ğŸ“¦ å­¦ä¹ åŒ… ID: {bundle.get('bundle_id')}")
        logger.info(f"ğŸ“š åŒ…å«ç»„ä»¶: {len(bundle.get('components', []))}")
        logger.info(f"â±ï¸  é¢„è®¡å­¦ä¹ æ—¶é—´: {bundle.get('estimated_time_minutes')} åˆ†é’Ÿ")
        logger.info(f"{'='*60}\n")
        
        return bundle
    
    async def execute_plan_stream(
        self,
        plan_config: Dict[str, Any],
        user_input: Dict[str, Any],
        user_profile: Any,
        session_context: Any
    ):
        """
        ğŸ†• æµå¼æ‰§è¡Œå®Œæ•´çš„ Planï¼ˆå®æ—¶å±•ç¤ºæ¯ä¸ªæ­¥éª¤çš„thinkingå’Œè¿›åº¦ï¼‰
        
        Args:
            plan_config: Plan Skill çš„ YAML é…ç½®
            user_input: ç”¨æˆ·è¾“å…¥å‚æ•°
            user_profile: ç”¨æˆ·å­¦ä¹ ç”»åƒ
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡
        
        Yields:
            Dict: æµå¼äº‹ä»¶ {"type": "plan_progress|thinking|content|step_done|done", ...}
        """
        # ğŸšï¸ Phase 2: æ£€æŸ¥æ˜¯å¦å¯ç”¨ Context Offloading
        cost_control = plan_config.get("cost_control", {})
        self.offloading_enabled = cost_control.get("enable_artifact_offloading", False)
        
        if self.offloading_enabled:
            # åˆå§‹åŒ– ArtifactStorage å’Œ session ID
            self.artifact_storage = ArtifactStorage()
            self.current_session_id = generate_session_id()
            
            # ä¿å­˜ Plan metadata
            self.artifact_storage.save_plan_metadata(
                self.current_session_id,
                plan_config,
                user_input
            )
            
            logger.info(f"âœ… [Offloading] Enabled (session: {self.current_session_id})")
        else:
            logger.debug("â„¹ï¸  [Offloading] Disabled (using legacy context pruning)")
        
        execution_plan = plan_config["execution_plan"]
        all_steps = execution_plan["steps"]
        
        # ğŸ†• Phase 4.2: åŠ¨æ€æ­¥éª¤é€‰æ‹© - å¦‚æœ user_input ä¸­æœ‰ required_stepsï¼Œåªæ‰§è¡Œè¿™äº›æ­¥éª¤
        required_steps = user_input.get("required_steps")
        if required_steps:
            logger.info(f"ğŸ¯ User requested specific steps: {required_steps}")
            # è¿‡æ»¤å‡ºéœ€è¦æ‰§è¡Œçš„æ­¥éª¤
            steps = [step for step in all_steps if step.get("step_id") in required_steps]
            logger.info(f"ğŸ“‹ Filtered execution plan: {len(steps)}/{len(all_steps)} steps (streaming)")
        else:
            steps = all_steps
        
        total_steps = len(steps)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸŒŠ å¼€å§‹æµå¼æ‰§è¡Œ Plan Skill: {plan_config['display_name']}")
        logger.info(f"ğŸ“‹ æ€»æ­¥éª¤æ•°: {total_steps}")
        logger.info(f"ğŸ“ ä¸»é¢˜: {user_input.get('topic', 'Unknown')}")
        logger.info(f"{'='*60}\n")
        
        # ğŸ†• å‡†å¤‡æ­¥éª¤é¢„è§ˆä¿¡æ¯
        steps_preview = []
        for idx, step in enumerate(steps, 1):
            steps_preview.append({
                "step_order": idx,
                "step_name": step.get("name", f"æ­¥éª¤ {idx}"),
                "step_description": step.get("description", ""),
                "skill_id": step["skill_id"]
            })
        
        # å‘é€Planå¼€å§‹çŠ¶æ€ï¼ˆåŒ…å«æ­¥éª¤é¢„è§ˆï¼‰
        yield {
            "type": "plan_start",
            "total_steps": total_steps,
            "topic": user_input.get('topic'),
            "subject": user_input.get('subject'),
            "steps_preview": steps_preview  # ğŸ†• å®Œæ•´æ­¥éª¤åˆ—è¡¨
        }
        
        # æ‰§è¡Œç»“æœå­˜å‚¨
        step_results = {}
        step_contexts = {}
        
        # ä¸²è”æ‰§è¡Œæ‰€æœ‰ stepsï¼ˆæµå¼ï¼‰
        for actual_index, step in enumerate(steps, 1):  # ğŸ†• ä½¿ç”¨å®é™…ç´¢å¼•è€Œä¸æ˜¯åŸå§‹order
            step_id = step["step_id"]
            step_name = step["display_name"]
            skill_id = step["skill_id"]
            step_order = actual_index  # ğŸ†• ä½¿ç”¨åŠ¨æ€ç´¢å¼•ï¼Œè€Œä¸æ˜¯åŸå§‹çš„step["order"]
            
            logger.info(f"\n{'â”€'*60}")
            logger.info(f"ğŸ“ Step {step_order}/{total_steps}: {step_name}")
            logger.info(f"ğŸ”§ Skill: {skill_id}")
            
            # ğŸ†• å‘é€æ­¥éª¤å¼€å§‹çŠ¶æ€
            yield {
                "type": "step_start",
                "step_order": step_order,
                "total_steps": total_steps,
                "step_name": step_name,
                "skill_id": skill_id
            }
            
            try:
                # 1. æ„å»º step è¾“å…¥
                step_input = self._build_step_input(
                    step=step,
                    user_input=user_input,
                    step_contexts=step_contexts,
                    session_context=session_context  # ğŸ†• ä¼ å…¥ session_context
                )
                logger.info(f"âœ… è¾“å…¥å‚æ•°æ„å»ºå®Œæˆ")
                
                # 2. ğŸ†• æµå¼æ‰§è¡Œ skill
                async for chunk in self._execute_step_stream(
                    skill_id=skill_id,
                    input_params=step_input,
                    user_profile=user_profile,
                    session_context=session_context,
                    step_info={
                        "step_order": step_order,
                        "total_steps": total_steps,
                        "step_name": step_name
                    }
                ):
                    # è½¬å‘thinkingå’Œcontent chunks
                    if chunk["type"] in ["thinking", "content"]:
                        yield chunk
                    elif chunk["type"] == "done":
                        # Stepå®Œæˆï¼Œä¿å­˜ç»“æœ
                        result = chunk.get("content", {})  # âœ… ä¿®å¤ï¼šä» content å­—æ®µè·å–ç»“æœ
                        step_results[step_id] = result
                        
                        # 3. æå–ä¸Šä¸‹æ–‡
                        extracted_context = self._extract_context(
                            result=result,
                            extraction_config=step.get("context_extraction", {}),
                            step_id=step_id  # ğŸ†• Phase 2: ä¼ é€’ step_id ç”¨äº offloading
                        )
                        step_contexts[step_id] = extracted_context
                        
                        # 4. ğŸ†• è¯¦ç»†Tokenç»Ÿè®¡
                        tokens_used = self._estimate_tokens(result)
                        
                        # å°è¯•ä»resultè·å–å®é™…çš„usageä¿¡æ¯
                        actual_usage = result.get("_usage", {})
                        
                        # æ„å»ºè¯¦ç»†çš„tokenç»Ÿè®¡
                        step_token_info = {
                            "estimated_tokens": tokens_used,
                            "actual_usage": actual_usage,
                            "step_name": step_name,
                            "skill_id": skill_id
                        }
                        
                        self.token_usage["per_step"][step_id] = step_token_info
                        self.token_usage["total"] += tokens_used
                        
                        # ğŸ†• è¯¦ç»†æ—¥å¿—è¾“å‡º
                        logger.info(f"")
                        logger.info(f"{'â”€'*60}")
                        logger.info(f"âœ… Step {step_order}/{total_steps} å®Œæˆ: {step_name}")
                        logger.info(f"{'â”€'*60}")
                        
                        if actual_usage:
                            logger.info(f"ğŸ’° Tokenæ¶ˆè€—è¯¦æƒ…:")
                            logger.info(f"   â”œâ”€ Prompt Tokens:     {actual_usage.get('prompt_tokens', 'N/A')}")
                            logger.info(f"   â”œâ”€ Completion Tokens: {actual_usage.get('completion_tokens', 'N/A')}")
                            logger.info(f"   â”œâ”€ Total Tokens:      {actual_usage.get('total_tokens', 'N/A')}")
                            if "reasoning_tokens" in actual_usage and actual_usage.get("reasoning_tokens", 0) > 0:
                                logger.info(f"   â””â”€ Reasoning Tokens:  {actual_usage.get('reasoning_tokens', 0)}")
                        else:
                            logger.info(f"ğŸ’° Tokenæ¶ˆè€—ä¼°ç®—: ~{tokens_used} tokens")
                        
                        logger.info(f"ğŸ“Š ç´¯è®¡Tokenæ¶ˆè€—: ~{self.token_usage['total']} tokens")
                        logger.info(f"{'â”€'*60}")
                        
                        # ğŸ†• å‘é€æ­¥éª¤å®ŒæˆçŠ¶æ€ï¼ˆåŒ…å«resultç”¨äºå‰ç«¯å³æ—¶æ˜¾ç¤ºï¼‰
                        yield {
                            "type": "step_done",
                            "step_order": step_order,
                            "total_steps": total_steps,
                            "step_name": step_name,
                            "skill_id": skill_id,
                            "tokens_used": tokens_used,
                            "result": result  # ğŸ†• åŒ…å«å®Œæ•´ç»“æœä¾›å‰ç«¯æ¸²æŸ“
                        }
                    elif chunk["type"] == "error":
                        # Stepå¤±è´¥
                        raise Exception(chunk.get("message", "Step execution failed"))
                
            except Exception as e:
                logger.error(f"âŒ Step {step_id} å¤±è´¥: {e}")
                logger.exception(e)
                
                # é”™è¯¯å¤„ç†
                error_config = plan_config.get("error_handling", {})
                strategy = error_config.get("on_step_failure", {}).get("strategy", "skip_and_continue")
                
                # ğŸ†• å‘é€æ­¥éª¤é”™è¯¯çŠ¶æ€
                yield {
                    "type": "step_error",
                    "step_order": step_order,
                    "step_name": step_name,
                    "error": str(e),
                    "strategy": strategy
                }
                
                if strategy == "skip_and_continue":
                    logger.info(f"â­ï¸  è·³è¿‡ Step {step_id}ï¼Œç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥")
                    continue
                elif strategy == "abort":
                    logger.error(f"ğŸš« Plan æ‰§è¡Œä¸­æ­¢")
                    yield {
                        "type": "error",
                        "message": f"Planæ‰§è¡Œä¸­æ­¢äºStep {step_order}: {str(e)}"
                    }
                    return
        
        # ğŸ†• ç”Ÿæˆè¯¦ç»†çš„Tokenç»Ÿè®¡æŠ¥å‘Š
        logger.info(f"\n{'â”'*60}")
        logger.info(f"ğŸ“¦ Plan Skill æ‰§è¡Œå®Œæˆç»Ÿè®¡")
        logger.info(f"{'â”'*60}")
        logger.info(f"âœ… æˆåŠŸæ­¥éª¤: {len(step_results)}/{total_steps}")
        logger.info(f"")
        logger.info(f"ğŸ’° Tokenæ¶ˆè€—è¯¦æƒ…:")
        logger.info(f"{'â”€'*60}")
        
        total_prompt_tokens = 0
        total_completion_tokens = 0
        total_reasoning_tokens = 0
        
        for step_id, token_info in self.token_usage["per_step"].items():
            step_name = token_info.get("step_name", step_id)
            actual_usage = token_info.get("actual_usage", {})
            
            logger.info(f"")
            logger.info(f"ğŸ“ {step_name} ({token_info.get('skill_id', 'unknown')})")
            
            if actual_usage:
                prompt_t = actual_usage.get('prompt_tokens', 0)
                completion_t = actual_usage.get('completion_tokens', 0)
                reasoning_t = actual_usage.get('reasoning_tokens', 0)
                total_t = actual_usage.get('total_tokens', 0)
                
                total_prompt_tokens += prompt_t
                total_completion_tokens += completion_t
                total_reasoning_tokens += reasoning_t
                
                logger.info(f"   â”œâ”€ Prompt:     {prompt_t:>6} tokens")
                logger.info(f"   â”œâ”€ Completion: {completion_t:>6} tokens")
                if reasoning_t > 0:
                    logger.info(f"   â”œâ”€ Reasoning:  {reasoning_t:>6} tokens")
                logger.info(f"   â””â”€ Total:      {total_t:>6} tokens")
            else:
                estimated = token_info.get("estimated_tokens", 0)
                logger.info(f"   â””â”€ ä¼°ç®—:       ~{estimated:>6} tokens")
        
        logger.info(f"")
        logger.info(f"{'â”€'*60}")
        logger.info(f"ğŸ“Š æ€»è®¡:")
        
        if total_prompt_tokens > 0 or total_completion_tokens > 0:
            logger.info(f"   â”œâ”€ Prompt Tokens:     {total_prompt_tokens:>8}")
            logger.info(f"   â”œâ”€ Completion Tokens: {total_completion_tokens:>8}")
            if total_reasoning_tokens > 0:
                logger.info(f"   â”œâ”€ Reasoning Tokens:  {total_reasoning_tokens:>8}")
            logger.info(f"   â””â”€ Total Tokens:      {total_prompt_tokens + total_completion_tokens:>8}")
        else:
            logger.info(f"   â””â”€ ä¼°ç®—æ€»è®¡:          ~{self.token_usage['total']:>8} tokens")
        
        logger.info(f"{'â”€'*60}")
        
        # æ£€æŸ¥æœ€å°æˆåŠŸæ­¥éª¤æ•°
        min_required = plan_config.get("error_handling", {}).get("min_required_steps", 1)
        if len(step_results) < min_required:
            error_msg = f"å­¦ä¹ åŒ…ç”Ÿæˆå¤±è´¥ï¼šæˆåŠŸæ­¥éª¤ä¸è¶³ ({len(step_results)}/{min_required})"
            logger.error(f"âŒ {error_msg}")
            yield {
                "type": "error",
                "message": error_msg
            }
            return
        
        # èšåˆç»“æœ
        bundle = self._aggregate_results(
            step_results=step_results,
            aggregation_config=plan_config["aggregation"],
            user_input=user_input
        )
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸŠ Plan Skill æ‰§è¡Œå®Œæˆï¼")
        logger.info(f"ğŸ“¦ å­¦ä¹ åŒ… ID: {bundle.get('bundle_id')}")
        logger.info(f"ğŸ“š åŒ…å«ç»„ä»¶: {len(bundle.get('components', []))}")
        logger.info(f"â±ï¸  é¢„è®¡å­¦ä¹ æ—¶é—´: {bundle.get('estimated_time_minutes')} åˆ†é’Ÿ")
        logger.info(f"{'='*60}\n")
        
        # ğŸ†• ç”ŸæˆPlançš„reasoning_summary
        components_summary = []
        for comp in bundle.get('components', []):
            comp_type = comp.get('component_type', 'unknown')  # âœ… ä¿®æ­£å­—æ®µå
            if comp_type == 'explanation':
                components_summary.append('æ¦‚å¿µè®²è§£')
            elif comp_type == 'flashcard_set':
                # å…¼å®¹æ–°æ—§æ ¼å¼ï¼šcardList (æ–°) æˆ– cards (æ—§)
                content = comp.get('content', {})
                cards = content.get('cardList') or content.get('cards', [])
                card_count = len(cards)
                components_summary.append(f'{card_count}å¼ æŠ½è®¤å¡')
            elif comp_type == 'quiz_set':
                quiz_count = len(comp.get('content', {}).get('questions', []))
                components_summary.append(f'{quiz_count}é“ç»ƒä¹ é¢˜')
        
        plan_reasoning_summary = f"å®Œæˆå­¦ä¹ åŒ…ç”Ÿæˆï¼ŒåŒ…å«{len(steps)}ä¸ªæ­¥éª¤ï¼š{' + '.join(components_summary)}"
        
        # ğŸ†• å‘é€Planå®ŒæˆçŠ¶æ€ï¼ˆåŒ…å«reasoning_summaryï¼‰
        yield {
            "type": "done",
            "thinking": "",  # Planæœ¬èº«æ²¡æœ‰thinkingè¿‡ç¨‹
            "content": {
                **bundle,
                "reasoning_summary": plan_reasoning_summary  # ğŸ†• æ·»åŠ reasoning_summary
            },
            "content_type": "learning_bundle"
        }
    
    async def _execute_step_stream(
        self,
        skill_id: str,
        input_params: Dict[str, Any],
        user_profile: Any,
        session_context: Any,
        step_info: Dict[str, Any]
    ):
        """
        ğŸ†• æµå¼æ‰§è¡Œå•ä¸ª skillï¼ˆè½¬å‘thinkingå’Œcontentï¼‰
        
        Args:
            skill_id: Skill ID
            input_params: è¾“å…¥å‚æ•°
            user_profile: ç”¨æˆ·ç”»åƒ
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡
            step_info: æ­¥éª¤ä¿¡æ¯ï¼ˆç”¨äºæ˜¾ç¤ºè¿›åº¦ï¼‰
        
        Yields:
            Dict: æµå¼äº‹ä»¶
        """
        # ğŸ†• ä¼ é€’æ­¥éª¤ç´¢å¼•ï¼Œç”¨äºæ™ºèƒ½é€‰æ‹© thinking æ¨¡å¼
        # - ç¬¬ä¸€æ­¥ (explain) â†’ çœŸæ€è€ƒ (Kimi)ï¼Œæ·±åº¦ç†è§£æ ¸å¿ƒæ¦‚å¿µ
        # - åç»­æ­¥éª¤ (flashcard/quiz/notes/mindmap) â†’ ä¼ªæ€è€ƒ (Gemini)ï¼ŒåŸºäºå·²æœ‰å†…å®¹å¿«é€Ÿç”Ÿæˆ
        step_order = step_info.get("step_order", 1)
        
        async for chunk in self.skill_orchestrator._execute_single_skill_stream(
            skill_id=skill_id,
            input_params=input_params,
            user_profile=user_profile,
            session_context=session_context,
            step_index=step_order  # ğŸ†• ä¼ é€’æ­¥éª¤ç´¢å¼•
        ):
            # è½¬å‘æ‰€æœ‰chunks
            yield chunk
    
    def _build_step_input(
        self,
        step: Dict[str, Any],
        user_input: Dict[str, Any],
        step_contexts: Dict[str, Any],
        session_context: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        æ„å»º step çš„è¾“å…¥å‚æ•°
        
        æ”¯æŒæ¨¡æ¿å˜é‡ï¼š
        - {input.field}: ä»ç”¨æˆ·è¾“å…¥æå–
        - {context.step_id.field}: ä»ä¸Šæ¸¸ step ä¸Šä¸‹æ–‡æå–
        - è‡ªåŠ¨ä» session_context æŸ¥æ‰¾ç¼ºå¤±çš„ä¾èµ–
        
        Args:
            step: Step é…ç½®
            user_input: ç”¨æˆ·è¾“å…¥
            step_contexts: å·²æ‰§è¡Œ steps çš„ä¸Šä¸‹æ–‡
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡ (ç”¨äºæŸ¥æ‰¾ fallback artifacts)
        
        Returns:
            Step è¾“å…¥å‚æ•°å­—å…¸
        """
        logger.debug(f"ğŸ”§ Building input for step: {step.get('step_id')}")
        logger.debug(f"ğŸ“¥ Available user_input keys: {list(user_input.keys())}")
        step_input = {}
        
        for key, value_template in step["input_mapping"].items():
            logger.debug(f"ğŸ” Processing: {key} = {value_template}")
            if isinstance(value_template, str) and "{" in value_template:
                # è§£ææ¨¡æ¿å˜é‡
                if value_template.startswith("{input."):
                    # ä»ç”¨æˆ·è¾“å…¥æå–: {input.topic} æˆ– {input.quantity|default:5}
                    field_expr = value_template[7:-1]
                    
                    # ğŸ†• æ”¯æŒé»˜è®¤å€¼è¿‡æ»¤å™¨: {input.quantity|default:5}
                    if "|default:" in field_expr:
                        field, default_val_str = field_expr.split("|default:", 1)
                        field = field.strip()
                        default_val_str = default_val_str.strip()
                        
                        value = user_input.get(field)
                        if value is None:
                            # å°è¯•è½¬æ¢ç±»å‹
                            if default_val_str.isdigit():
                                value = int(default_val_str)
                            elif default_val_str.lower() == "true":
                                value = True
                            elif default_val_str.lower() == "false":
                                value = False
                            else:
                                value = default_val_str
                            logger.debug(f"ğŸ“ Using default value for {field}: {value}")
                        step_input[key] = value
                    else:
                        # æ— é»˜è®¤å€¼
                        value = user_input.get(field_expr)
                        if value is not None:
                            step_input[key] = value
                        else:
                            logger.warning(f"âš ï¸  Field '{field_expr}' not found in user_input and no default value provided")
                
                elif value_template.startswith("{context."):
                    # ä»ä¸Šæ¸¸ step ä¸Šä¸‹æ–‡æå–
                    # æ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
                    # 1. {context.explain.key_terms} - æ˜ç¡®æŒ‡å®š step
                    # 2. {context.previous} - å‰ä¸€ä¸ª step çš„å®Œæ•´ä¸Šä¸‹æ–‡
                    # 3. {context.previous.summary} - å‰ä¸€ä¸ª step çš„ summary
                    parts = value_template[9:-1].split(".", 1)
                    step_id = parts[0]
                    field_path = parts[1] if len(parts) > 1 else None
                    
                    # ğŸ†• æ”¯æŒ {context.previous} - è‡ªåŠ¨è·å–å‰ä¸€ä¸ª step
                    if step_id == "previous":
                        # è·å–å½“å‰å·²æ‰§è¡Œçš„æœ€åä¸€ä¸ª step
                        if step_contexts:
                            prev_step_id = list(step_contexts.keys())[-1]
                            prev_context = step_contexts[prev_step_id]
                            
                            if field_path:
                                # æå–ç‰¹å®šå­—æ®µï¼ˆå¦‚ summaryï¼‰
                                step_input[key] = self._get_nested_value(prev_context, field_path)
                                logger.info(f"ğŸ“¦ ä¼ é€’ä¸Šä¸‹æ–‡: {key} <- previous({prev_step_id}).{field_path}")
                            else:
                                # ä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå‹ç¼©ç‰ˆsummaryï¼‰
                                # ğŸ¯ è‡ªåŠ¨å‹ç¼©å‰ä¸€æ­¥çš„è¾“å‡ºä¸º summary
                                summary = self._create_context_summary(prev_context, prev_step_id)
                                step_input[key] = summary
                                logger.info(f"ğŸ“¦ ä¼ é€’ä¸Šä¸‹æ–‡: {key} <- previous({prev_step_id}) [summary: {len(str(summary))} chars]")
                        else:
                            logger.warning(f"âš ï¸  No previous step available, passing None")
                            step_input[key] = None
                    
                    # æ˜ç¡®æŒ‡å®š step_id çš„æƒ…å†µ
                    elif step_id in step_contexts:
                        if field_path:
                            step_input[key] = self._get_nested_value(step_contexts[step_id], field_path)
                            logger.debug(f"ğŸ“¦ ä¼ é€’ä¸Šä¸‹æ–‡: {key} <- context.{step_id}.{field_path}")
                        else:
                            # ä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡
                            context_value = step_contexts[step_id]
                            step_input[key] = context_value
                            logger.info(f"ğŸ“¦ ä¼ é€’ä¸Šä¸‹æ–‡: {key} <- context.{step_id} (åŒ…å« {len(context_value)} ä¸ªå­—æ®µ: {list(context_value.keys()) if isinstance(context_value, dict) else 'non-dict'})")
                    else:
                        # ğŸ†• Phase 4.3: ä» session_context æŸ¥æ‰¾ fallback artifact
                        fallback_value = self._find_artifact_from_session(
                            session_context=session_context,
                            artifact_type=step_id,  # ä¾‹å¦‚ "explain"
                            topic=user_input.get("topic")
                        )
                        
                        if fallback_value:
                            # ğŸ‰ æ‰¾åˆ°äº† session ä¸­çš„ç›¸å…³ artifact
                            if field_path:
                                step_input[key] = self._get_nested_value(fallback_value, field_path)
                                logger.info(f"âœ… ä» session fallback: {key} <- {step_id}.{field_path} (artifact from history)")
                            else:
                                step_input[key] = fallback_value
                                logger.info(f"âœ… ä» session fallback: {key} <- {step_id} (artifact from history)")
                        else:
                            # ä»ç„¶æ‰¾ä¸åˆ°ï¼Œè·³è¿‡è¯¥å‚æ•° (ä¸ä¼  Noneï¼Œè®© skill ä½¿ç”¨é»˜è®¤è¡Œä¸º)
                            logger.warning(f"âš ï¸  ä¾èµ–çš„ step {step_id} ä¸å­˜åœ¨ï¼Œä¸” session ä¸­æ— ç›¸å…³ artifactï¼Œè·³è¿‡å‚æ•° '{key}'")
                            # ä¸è®¾ç½® step_input[key]ï¼Œè®©ä¸‹æ¸¸è‡ªå·±å¤„ç†
            else:
                # ç›´æ¥å€¼
                step_input[key] = value_template
        
        return step_input
    
    async def _execute_step(
        self,
        skill_id: str,
        input_params: Dict[str, Any],
        user_profile: Any,
        session_context: Any
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ª skill
        
        Args:
            skill_id: Skill ID
            input_params: è¾“å…¥å‚æ•°
            user_profile: ç”¨æˆ·ç”»åƒ
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡
        
        Returns:
            Skill æ‰§è¡Œç»“æœ
        """
        # è°ƒç”¨ SkillOrchestrator æ‰§è¡Œ skill
        result = await self.skill_orchestrator._execute_single_skill(
            skill_id=skill_id,
            input_params=input_params,
            user_profile=user_profile,
            session_context=session_context
        )
        
        return result
    
    def _extract_context(
        self,
        result: Dict[str, Any],
        extraction_config: Dict[str, Any],
        step_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä» step ç»“æœä¸­æå–ä¸Šä¸‹æ–‡ï¼ˆè·¯ç”±å™¨æ–¹æ³•ï¼‰
        
        Phase 2: æ ¹æ®é…ç½®é€‰æ‹©æå–ç­–ç•¥ï¼š
        - offload + enabled: ä½¿ç”¨æ–‡ä»¶å¸è½½ï¼ˆ_offload_to_fileï¼‰
        - å…¶ä»–: ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼ˆ_extract_context_legacyï¼‰
        
        Args:
            result: Step æ‰§è¡Œç»“æœ
            extraction_config: æå–é…ç½®
            step_id: Step æ ‡è¯†ç¬¦ï¼ˆç”¨äº offloadingï¼‰
        
        Returns:
            æå–çš„ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½æ˜¯å®Œæ•´å†…å®¹æˆ– artifact å¼•ç”¨ï¼‰
        """
        if not extraction_config:
            return {}
        
        strategy = extraction_config.get("strategy", "key_points")
        
        # ğŸšï¸ Phase 2: æ£€æŸ¥æ˜¯å¦å¯ç”¨ offloading
        if strategy == "offload" and self.offloading_enabled:
            # ğŸ†• æ–°è·¯å¾„ï¼šæ–‡ä»¶å¸è½½
            return self._offload_to_file(result, extraction_config, step_id)
        else:
            # âœ… åŸè·¯å¾„ï¼šä¼ ç»Ÿæ–¹å¼ï¼ˆå®Œå…¨ä¸å˜ï¼‰
            return self._extract_context_legacy(result, extraction_config)
    
    def _extract_context_legacy(
        self,
        result: Dict[str, Any],
        extraction_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ä» step ç»“æœä¸­æå–ä¸Šä¸‹æ–‡ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        
        å®ç°ä¸Šä¸‹æ–‡å¸è½½ï¼ˆContext Pruningï¼‰ï¼š
        - åªæå–å…³é”®ä¿¡æ¯
        - å‹ç¼©æ•°æ®é‡
        - å‡å°‘ä¸‹æ¸¸ token æ¶ˆè€—
        
        Args:
            result: Step æ‰§è¡Œç»“æœ
            extraction_config: æå–é…ç½®
        
        Returns:
            æå–çš„ä¸Šä¸‹æ–‡å­—å…¸
        """
        if not extraction_config:
            return {}
        
        strategy = extraction_config.get("strategy", "key_points")
        fields = extraction_config.get("fields", [])
        max_tokens = extraction_config.get("max_tokens", 500)
        
        extracted = {}
        
        if strategy == "key_points":
            # æå–æŒ‡å®šå­—æ®µ
            for field in fields:
                value = self._get_nested_value(result, field)
                if value:
                    extracted[field] = value
        
        elif strategy == "summary":
            # ç”Ÿæˆæ‘˜è¦ï¼ˆæå–å…³é”®ä¿¡æ¯ï¼‰
            for field in fields:
                value = self._get_nested_value(result, field)
                if value:
                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œåªä¿ç•™æ•°é‡ä¿¡æ¯
                    if isinstance(value, list):
                        extracted[field] = f"{len(value)} items"
                    else:
                        extracted[field] = value
        
        elif strategy == "full_content":
            # ğŸ†• ä¼ é€’å®Œæ•´å†…å®¹ï¼ˆç¡®ä¿ä¸‹æ¸¸æ­¥éª¤å†…å®¹è¿è´¯æ€§ï¼‰
            # æå–æ‰€æœ‰æŒ‡å®šå­—æ®µçš„å®Œæ•´å†…å®¹ï¼Œä¸åšä»»ä½•å‹ç¼©
            for field in fields:
                value = self._get_nested_value(result, field)
                if value:
                    extracted[field] = value
            
            logger.info(f"ğŸ“¦ [full_contentç­–ç•¥] æå–äº† {len(extracted)} ä¸ªå­—æ®µçš„å®Œæ•´å†…å®¹")
            logger.info(f"ğŸ“Š æå–å­—æ®µ: {list(extracted.keys())}")
        
        # Token é™åˆ¶æ£€æŸ¥
        extracted_str = json.dumps(extracted, ensure_ascii=False)
        estimated_tokens = len(extracted_str) // 4
        
        if estimated_tokens > max_tokens:
            logger.warning(f"âš ï¸  æå–çš„ä¸Šä¸‹æ–‡è¶…è¿‡é™åˆ¶: {estimated_tokens} > {max_tokens}")
            # è¿›ä¸€æ­¥å‹ç¼©ï¼ˆç®€å•å®ç°ï¼šæˆªæ–­ï¼‰
            extracted = self._compress_context(extracted, max_tokens)
        
        logger.debug(f"ğŸ” ä¸Šä¸‹æ–‡æå–: {strategy} | {len(extracted_str)} chars | ~{estimated_tokens} tokens")
        
        return extracted
    
    def _offload_to_file(
        self,
        result: Dict[str, Any],
        extraction_config: Dict[str, Any],
        step_id: str
    ) -> Dict[str, Any]:
        """
        ğŸ†• Phase 2: å°† step ç»“æœå¸è½½åˆ°æ–‡ä»¶ç³»ç»Ÿï¼ˆContext Offloadingï¼‰
        
        æ ¸å¿ƒä¼˜åŠ¿ï¼š
        - Token èŠ‚çœ > 90%ï¼ˆå¼•ç”¨ < 200 bytes vs å®Œæ•´å†…å®¹ 2000+ bytesï¼‰
        - æ— ä¿¡æ¯æŸå¤±ï¼ˆå®Œæ•´å†…å®¹ä¿å­˜åœ¨æ–‡ä»¶ä¸­ï¼‰
        - æŒ‰éœ€åŠ è½½ï¼ˆ_format_prompt æ—¶æ‰è¯»å–ï¼‰
        
        Args:
            result: Step æ‰§è¡Œç»“æœï¼ˆå®Œæ•´å†…å®¹ï¼‰
            extraction_config: æå–é…ç½®
            step_id: Step æ ‡è¯†ç¬¦
        
        Returns:
            Artifact å¼•ç”¨ï¼ˆtype="artifact_reference"ï¼‰
            
        é™çº§æœºåˆ¶ï¼š
            å¦‚æœæ–‡ä»¶æ“ä½œå¤±è´¥ï¼Œè‡ªåŠ¨å›é€€åˆ° _extract_context_legacy
        """
        try:
            # ä¿å­˜å®Œæ•´ç»“æœåˆ°æ–‡ä»¶
            artifact_path = self.artifact_storage.save_step_result(
                session_id=self.current_session_id,
                step_id=step_id,
                result=result,
                metadata={
                    "skill_id": extraction_config.get("skill_id"),
                    "timestamp": datetime.now().isoformat()
                }
            )
            
            # åˆ›å»ºè½»é‡çº§å¼•ç”¨
            fields = extraction_config.get("fields")
            reference = self.artifact_storage.create_reference(
                session_id=self.current_session_id,
                step_id=step_id,
                fields=fields
            )
            
            # ç»Ÿè®¡æ•ˆæœ
            result_size = len(json.dumps(result, ensure_ascii=False))
            reference_size = len(json.dumps(reference, ensure_ascii=False))
            savings = 1 - (reference_size / result_size)
            
            logger.info(
                f"ğŸ’¾ [Offloading] {step_id}: "
                f"{result_size} bytes â†’ {reference_size} bytes "
                f"(èŠ‚çœ {savings*100:.1f}%)"
            )
            
            return reference
            
        except Exception as e:
            # ğŸ›¡ï¸ é™çº§ï¼šå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            logger.warning(
                f"âš ï¸  Offloading failed for {step_id}, falling back to legacy: {e}"
            )
            return self._extract_context_legacy(result, extraction_config)
    
    def _aggregate_results(
        self,
        step_results: Dict[str, Dict[str, Any]],
        aggregation_config: Dict[str, Any],
        user_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        èšåˆæ‰€æœ‰ step çš„ç»“æœä¸ºå®Œæ•´å­¦ä¹ åŒ…
        
        Args:
            step_results: å„ step çš„æ‰§è¡Œç»“æœ
            aggregation_config: èšåˆé…ç½®
            user_input: ç”¨æˆ·è¾“å…¥
        
        Returns:
            å®Œæ•´çš„å­¦ä¹ åŒ…
        """
        components = []
        total_time = 0
        
        # ç»„è£… components
        for component_config in aggregation_config["assembly"]["components"]:
            step_id = component_config["step_id"]
            
            if step_id in step_results:
                result = step_results[step_id]
                
                components.append({
                    "component_type": component_config["component_type"],
                    "skill_id": result.get("skill_id", "unknown"),
                    "content": result
                })
                
                # ç´¯åŠ æ—¶é—´
                if "estimated_time_minutes" in result:
                    total_time += result.get("estimated_time_minutes", 0)
        
        # ç”Ÿæˆå­¦ä¹ è·¯å¾„
        learning_path = aggregation_config["assembly"]["learning_path_template"]
        
        # ç”Ÿæˆ bundle_id
        timestamp = int(datetime.now().timestamp())
        bundle_id = f"bundle_{user_input.get('subject', 'general')}_{user_input.get('topic', 'topic')}_{timestamp}"
        
        bundle = {
            "bundle_id": bundle_id,
            "subject": user_input.get("subject", "é€šç”¨"),
            "topic": user_input.get("topic"),
            "components": components,
            "estimated_time_minutes": total_time if total_time > 0 else 45,  # é»˜è®¤ 45 åˆ†é’Ÿ
            "learning_path": learning_path,
            "execution_summary": {
                "plan_skill_version": "2.0",
                "total_steps": len(step_results),
                "successful_components": len(components),
                "token_usage": self.token_usage
            }
        }
        
        return bundle
    
    def _create_context_summary(
        self,
        context: Dict[str, Any],
        step_id: str
    ) -> Dict[str, Any]:
        """
        ğŸ†• åˆ›å»ºå‰ä¸€æ­¥ä¸Šä¸‹æ–‡çš„å‹ç¼© summaryï¼ˆä¸Šä¸‹æ–‡å¸è½½ï¼‰
        
        ç­–ç•¥ï¼šä¿ç•™å…³é”®å­—æ®µï¼Œå‹ç¼©å†—ä½™å†…å®¹
        - ä¿ç•™å°å­—æ®µï¼ˆ<100 charsï¼‰
        - å‹ç¼©å¤§å­—æ®µï¼ˆ>100 charsï¼‰ä¸ºæ‘˜è¦
        - æ•°ç»„ï¼šä¿ç•™é•¿åº¦ä¿¡æ¯
        
        Args:
            context: å®Œæ•´çš„å‰ä¸€æ­¥ä¸Šä¸‹æ–‡
            step_id: å‰ä¸€æ­¥çš„ step_id
        
        Returns:
            å‹ç¼©åçš„ summary
        """
        if not isinstance(context, dict):
            return context
        
        summary = {}
        
        for key, value in context.items():
            if isinstance(value, str):
                if len(value) > 100:
                    # é•¿æ–‡æœ¬ï¼šä¿ç•™å‰80å­—ç¬¦ + é•¿åº¦ä¿¡æ¯
                    summary[key] = value[:80] + f"... [total: {len(value)} chars]"
                else:
                    summary[key] = value
            
            elif isinstance(value, list):
                # æ•°ç»„ï¼šä¿ç•™é•¿åº¦ + ç¬¬ä¸€ä¸ªå…ƒç´ ç¤ºä¾‹
                if len(value) > 0:
                    summary[key] = {
                        "_type": "array",
                        "_length": len(value),
                        "_sample": value[0] if len(value) > 0 else None
                    }
                else:
                    summary[key] = []
            
            elif isinstance(value, dict):
                # åµŒå¥—å­—å…¸ï¼šé€’å½’å‹ç¼©
                summary[key] = self._create_context_summary(value, step_id)
            
            else:
                # å…¶ä»–ç±»å‹ï¼ˆint, bool ç­‰ï¼‰ï¼šç›´æ¥ä¿ç•™
                summary[key] = value
        
        # æ·»åŠ å…ƒä¿¡æ¯
        summary["_context_meta"] = {
            "from_step": step_id,
            "original_size_chars": len(str(context)),
            "summary_size_chars": len(str(summary)),
            "compression_ratio": f"{(1 - len(str(summary)) / max(len(str(context)), 1)) * 100:.1f}%"
        }
        
        logger.debug(
            f"ğŸ“‰ Context compressed: {len(str(context))} â†’ {len(str(summary))} chars "
            f"({summary['_context_meta']['compression_ratio']} reduction)"
        )
        
        return summary
    
    def _find_artifact_from_session(
        self,
        session_context: Optional[Any],
        artifact_type: str,
        topic: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """
        ä» session_context.artifact_history ä¸­æŸ¥æ‰¾ç›¸å…³ artifact
        
        å½“ Plan Skill çš„æŸä¸ªæ­¥éª¤è¢«åŠ¨æ€è·³è¿‡æ—¶ï¼Œå°è¯•ä»ä¼šè¯å†å²ä¸­æŸ¥æ‰¾
        è¯¥ç±»å‹çš„ artifact ä½œä¸º fallbackã€‚
        
        Args:
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡
            artifact_type: Artifact ç±»å‹ (e.g., "explain" â†’ "explanation")
            topic: å¯é€‰ä¸»é¢˜ï¼Œç”¨äºè¿‡æ»¤
        
        Returns:
            æ‰¾åˆ°çš„ artifact content (å‹ç¼©çš„ summary)ï¼Œæˆ– None
        """
        if not session_context or not hasattr(session_context, 'artifact_history'):
            return None
        
        # ç±»å‹æ˜ å°„: step_id â†’ artifact_type
        type_mapping = {
            "explain": "explanation",
            "quiz": "quiz_set",
            "flashcard": "flashcard_set",
            "mindmap": "mindmap",
            "notes": "notes"
        }
        
        target_type = type_mapping.get(artifact_type, artifact_type)
        
        # ä»åå¾€å‰æŸ¥æ‰¾ (æœ€æ–°çš„ä¼˜å…ˆ)
        for artifact_record in reversed(session_context.artifact_history):
            # åŒ¹é…ç±»å‹
            if artifact_record.artifact_type == target_type:
                # å¦‚æœæŒ‡å®šäº† topicï¼Œè¿›ä¸€æ­¥åŒ¹é…
                if topic and artifact_record.topic != topic:
                    continue
                
                # è¿”å› artifact çš„ content (å‹ç¼©çš„ summary)
                if artifact_record.content:
                    logger.info(f"ğŸ” Found {target_type} artifact from session history: {artifact_record.artifact_id}")
                    logger.info(f"   Topic: {artifact_record.topic}, Turn: {artifact_record.turn_number}")
                    return artifact_record.content
        
        logger.debug(f"ğŸ” No {target_type} artifact found in session history")
        return None
    
    def _get_nested_value(self, data: Any, path: str) -> Any:
        """
        è·å–åµŒå¥—å­—å…¸/å¯¹è±¡çš„å€¼
        
        æ”¯æŒè·¯å¾„: 'field1.field2.field3'
        """
        if not path:
            return data
        
        keys = path.split(".")
        value = data
        
        for key in keys:
            if isinstance(value, dict):
                value = value.get(key)
            elif hasattr(value, key):
                value = getattr(value, key)
            else:
                return None
            
            if value is None:
                return None
        
        return value
    
    def _compress_context(self, context: Dict[str, Any], max_tokens: int) -> Dict[str, Any]:
        """å‹ç¼©ä¸Šä¸‹æ–‡åˆ°æŒ‡å®š token é™åˆ¶"""
        # ç®€å•å®ç°ï¼šä¿ç•™æœ€é‡è¦çš„å­—æ®µ
        compressed = {}
        current_tokens = 0
        
        for key, value in context.items():
            value_str = json.dumps(value, ensure_ascii=False)
            value_tokens = len(value_str) // 4
            
            if current_tokens + value_tokens <= max_tokens:
                compressed[key] = value
                current_tokens += value_tokens
            else:
                break
        
        return compressed
    
    def _estimate_tokens(self, result: Dict[str, Any]) -> int:
        """ä¼°ç®—ç»“æœçš„ token æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼šå­—ç¬¦æ•° / 4ï¼‰"""
        result_str = json.dumps(result, ensure_ascii=False)
        return len(result_str) // 4

