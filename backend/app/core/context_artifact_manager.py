"""
Context Engineering - Artifact Manager
æ™ºèƒ½å¸è½½å’Œç´¢å¼•ç®¡ç†å™¨

æ ¸å¿ƒç†å¿µ:
1. Offloading: å¤§å‹å†…å®¹è‡ªåŠ¨ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿï¼Œåªè¿”å›å¼•ç”¨
2. Indexing: ç»´æŠ¤è½»é‡çº§ç´¢å¼•ï¼ŒAgent æŒ‰éœ€æ£€ç´¢
3. Token-Aware: åŸºäº token ä¼°ç®—è‡ªåŠ¨å†³å®šæ˜¯å¦å¸è½½
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)


class ContextArtifactManager:
    """
    ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ Artifact ç®¡ç†å™¨
    
    èŒè´£:
    1. æ™ºèƒ½åˆ¤æ–­å†…å®¹æ˜¯å¦éœ€è¦å¸è½½ï¼ˆåŸºäº token ä¼°ç®—ï¼‰
    2. ç»´æŠ¤è½»é‡çº§ç´¢å¼•ï¼ˆartifact catalogï¼‰
    3. æä¾›æŒ‰éœ€æ£€ç´¢æ¥å£
    """
    
    # Token é˜ˆå€¼: è¶…è¿‡æ­¤å€¼è‡ªåŠ¨å¸è½½
    OFFLOAD_THRESHOLD_TOKENS = 500
    
    # ä¼°ç®—å› å­: å¹³å‡æ¯ä¸ªå­—ç¬¦å¯¹åº”å¤šå°‘ token (ä¸­æ–‡çº¦ 1.5, è‹±æ–‡çº¦ 0.25)
    TOKEN_ESTIMATE_FACTOR = 0.8
    
    def __init__(
        self,
        storage_path: Path,
        s3_manager: Optional[Any] = None
    ):
        """
        åˆå§‹åŒ– Artifact Manager
        
        Args:
            storage_path: æœ¬åœ°å­˜å‚¨è·¯å¾„
            s3_manager: S3 ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        self.s3_manager = s3_manager
        
        # ç´¢å¼•æ–‡ä»¶è·¯å¾„
        self.index_path = self.storage_path / "artifact_index.json"
        self._load_index()
        
        logger.info(f"âœ… ContextArtifactManager initialized: {storage_path}")
    
    def _load_index(self):
        """åŠ è½½ artifact ç´¢å¼•"""
        if self.index_path.exists():
            with open(self.index_path, 'r', encoding='utf-8') as f:
                self.index = json.load(f)
        else:
            self.index = {"artifacts": [], "metadata": {"last_updated": None}}
    
    def _save_index(self):
        """ä¿å­˜ artifact ç´¢å¼•"""
        self.index["metadata"]["last_updated"] = datetime.utcnow().isoformat()
        with open(self.index_path, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)
    
    def _estimate_tokens(self, content: Any) -> int:
        """
        ä¼°ç®—å†…å®¹çš„ token æ•°é‡
        
        Args:
            content: å†…å®¹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€å­—å…¸ã€åˆ—è¡¨ç­‰ï¼‰
        
        Returns:
            ä¼°ç®—çš„ token æ•°
        """
        if isinstance(content, str):
            text = content
        elif isinstance(content, (dict, list)):
            text = json.dumps(content, ensure_ascii=False)
        else:
            text = str(content)
        
        return int(len(text) * self.TOKEN_ESTIMATE_FACTOR)
    
    def save_with_offload(
        self,
        artifact_id: str,
        content: Dict[str, Any],
        metadata: Dict[str, Any]
    ) -> Tuple[bool, Optional[str], Dict[str, Any]]:
        """
        æ™ºèƒ½ä¿å­˜: æ ¹æ®å¤§å°å†³å®šæ˜¯å¦å¸è½½
        
        Args:
            artifact_id: Artifact ID
            content: Artifact å†…å®¹
            metadata: å…ƒæ•°æ® (topic, type, user_id, session_id)
        
        Returns:
            (is_offloaded, file_path, lightweight_ref)
            - is_offloaded: æ˜¯å¦è¢«å¸è½½
            - file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœå¸è½½ï¼‰
            - lightweight_ref: è½»é‡çº§å¼•ç”¨ï¼ˆç”¨äºåŠ è½½åˆ° contextï¼‰
        """
        # 1. ä¼°ç®— token
        estimated_tokens = self._estimate_tokens(content)
        
        # 2. å†³å®šæ˜¯å¦å¸è½½
        should_offload = estimated_tokens > self.OFFLOAD_THRESHOLD_TOKENS
        
        if should_offload:
            # 3. å¸è½½åˆ°æ–‡ä»¶
            file_path = self._save_to_file(artifact_id, content, metadata)
            
            # 4. åˆ›å»ºè½»é‡çº§å¼•ç”¨
            lightweight_ref = self._create_lightweight_ref(
                artifact_id, metadata, estimated_tokens, file_path
            )
            
            # 5. æ›´æ–°ç´¢å¼•
            self._update_index(artifact_id, metadata, estimated_tokens, file_path)
            
            logger.info(f"ğŸ“¤ Offloaded artifact {artifact_id}: {estimated_tokens} tokens â†’ {file_path}")
            return True, file_path, lightweight_ref
        
        else:
            # ä¸å¸è½½ï¼Œç›´æ¥è¿”å›å†…å®¹
            logger.info(f"ğŸ“ Kept artifact {artifact_id} in memory: {estimated_tokens} tokens (< threshold)")
            
            # ä»ç„¶æ›´æ–°ç´¢å¼•ï¼ˆä½†æ ‡è®°ä¸º in-memoryï¼‰
            self._update_index(artifact_id, metadata, estimated_tokens, file_path=None)
            
            return False, None, content
    
    def _save_to_file(self, artifact_id: str, content: Dict[str, Any], metadata: Dict[str, Any]) -> str:
        """ä¿å­˜ artifact åˆ°æ–‡ä»¶"""
        # åˆ›å»ºç”¨æˆ·ç›®å½•
        user_id = metadata.get("user_id", "default")
        user_dir = self.storage_path / user_id
        user_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        file_path = user_dir / f"{artifact_id}.json"
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump({
                "artifact_id": artifact_id,
                "metadata": metadata,
                "content": content,
                "created_at": datetime.utcnow().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        # å¯é€‰: ä¸Šä¼ åˆ° S3
        if self.s3_manager and hasattr(self.s3_manager, 'upload_file'):
            try:
                s3_key = f"{user_id}/{artifact_id}.json"
                self.s3_manager.upload_file(str(file_path), s3_key)
                logger.info(f"â˜ï¸  Uploaded to S3: {s3_key}")
            except Exception as e:
                logger.warning(f"âš ï¸  S3 upload failed: {e}")
        
        return str(file_path.relative_to(self.storage_path))
    
    def _create_lightweight_ref(
        self,
        artifact_id: str,
        metadata: Dict[str, Any],
        estimated_tokens: int,
        file_path: str
    ) -> Dict[str, Any]:
        """åˆ›å»ºè½»é‡çº§å¼•ç”¨ï¼ˆç”¨äºåŠ è½½åˆ° LLM contextï¼‰"""
        return {
            "artifact_id": artifact_id,
            "type": metadata.get("type", "unknown"),
            "topic": metadata.get("topic", ""),
            "size_tokens": estimated_tokens,
            "file_path": file_path,
            "summary": metadata.get("summary", ""),
            "_note": "Use read_artifact tool to load full content"
        }
    
    def _update_index(
        self,
        artifact_id: str,
        metadata: Dict[str, Any],
        estimated_tokens: int,
        file_path: Optional[str]
    ):
        """æ›´æ–°ç´¢å¼•"""
        # ç§»é™¤æ—§çš„åŒ ID æ¡ç›®
        self.index["artifacts"] = [
            a for a in self.index["artifacts"] if a["artifact_id"] != artifact_id
        ]
        
        # æ·»åŠ æ–°æ¡ç›®
        self.index["artifacts"].append({
            "artifact_id": artifact_id,
            "type": metadata.get("type", "unknown"),
            "topic": metadata.get("topic", ""),
            "session_id": metadata.get("session_id", ""),
            "size_tokens": estimated_tokens,
            "is_offloaded": file_path is not None,
            "file_path": file_path,
            "created_at": datetime.utcnow().isoformat()
        })
        
        self._save_index()
    
    def get_artifact_index(
        self,
        session_id: Optional[str] = None,
        topic: Optional[str] = None,
        artifact_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å– artifact ç´¢å¼•ï¼ˆè½»é‡çº§ï¼Œç”¨äº Agent contextï¼‰
        
        Args:
            session_id: æŒ‰ä¼šè¯è¿‡æ»¤
            topic: æŒ‰ä¸»é¢˜è¿‡æ»¤
            artifact_type: æŒ‰ç±»å‹è¿‡æ»¤
        
        Returns:
            Artifact ç´¢å¼•åˆ—è¡¨
        """
        artifacts = self.index["artifacts"]
        
        # è¿‡æ»¤
        if session_id:
            artifacts = [a for a in artifacts if a.get("session_id") == session_id]
        if topic:
            artifacts = [a for a in artifacts if topic.lower() in a.get("topic", "").lower()]
        if artifact_type:
            artifacts = [a for a in artifacts if a.get("type") == artifact_type]
        
        # åªè¿”å›è½»é‡çº§å­—æ®µ
        return [
            {
                "artifact_id": a["artifact_id"],
                "type": a["type"],
                "topic": a["topic"],
                "size_tokens": a["size_tokens"],
                "is_offloaded": a["is_offloaded"]
            }
            for a in artifacts
        ]
    
    def read_artifact(
        self,
        artifact_id: str,
        lines: Optional[Tuple[int, int]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        è¯»å– artifact å†…å®¹ï¼ˆæŒ‰éœ€æ£€ç´¢ï¼‰
        
        Args:
            artifact_id: Artifact ID
            lines: å¯é€‰çš„è¡ŒèŒƒå›´ (start, end)
        
        Returns:
            Artifact å†…å®¹
        """
        # ä»ç´¢å¼•æŸ¥æ‰¾
        artifact_entry = next(
            (a for a in self.index["artifacts"] if a["artifact_id"] == artifact_id),
            None
        )
        
        if not artifact_entry:
            logger.warning(f"âš ï¸  Artifact {artifact_id} not found in index")
            return None
        
        # è¯»å–æ–‡ä»¶
        if artifact_entry["is_offloaded"]:
            file_path = self.storage_path / artifact_entry["file_path"]
            if not file_path.exists():
                logger.error(f"âŒ Artifact file not found: {file_path}")
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            content = data["content"]
            
            # å¯é€‰: éƒ¨åˆ†åŠ è½½ï¼ˆå¦‚æœæ”¯æŒï¼‰
            if lines and isinstance(content, str):
                content_lines = content.split('\n')
                start, end = lines
                content = '\n'.join(content_lines[start:end])
            
            logger.info(f"ğŸ“¥ Loaded artifact {artifact_id} from {file_path}")
            return content
        
        else:
            # In-memory artifacts (éœ€è¦ä»å…¶ä»–åœ°æ–¹åŠ è½½)
            logger.warning(f"âš ï¸  In-memory artifact {artifact_id} not implemented")
            return None

